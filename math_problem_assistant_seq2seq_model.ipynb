{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an LSTM. \n",
    "    Converts a sequence of token indices into a hidden representation \n",
    "    that will be used by the decoder for sequence generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder module.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer converts token indices into dense vectors for better representation.\n",
    "        # The embeddings refer to vector representations of words or tokens used as input to the model.\n",
    "        # Instead of feeding raw word indices (which don’t capture meaning), feedingembeddings allows\n",
    "        # the LSTM to learn meaningful semantic relationships between words.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # LSTM layer processes embedded input sequences to generate hidden states\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing token indices for a batch of input sentences.\n",
    "\n",
    "        Returns:\n",
    "            outputs (Tensor): Encoder outputs at each time step.\n",
    "            hidden (Tensor): Final hidden state of the LSTM.\n",
    "            cell (Tensor): Final cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_seq)            # Convert input tokens into embeddings \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)   # Process embeddings through LSTM\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Attention model for the sequence-to-sequence math problem assistant\n",
    "    which implements a 'Bahdanau attention' mechanism. This allows the model\n",
    "    to dynamically compute attention scores based on the decoder's hidden\n",
    "    state and the encoder's outputs, allowing the model to focus on relevant\n",
    "    parts of the input sequence at each decoding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden state of the LSTM.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        # Learnable linear transformation to compute alignment scores\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # Learnable parameter to compute weighted attention scores\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Computes attention weights using Bahdanau's additive attention method.\n",
    "\n",
    "        Args:\n",
    "            hidden (Tensor): Decoder hidden state at the current time step.\n",
    "            encoder_outputs (Tensor): Encoder outputs at all time steps.\n",
    "\n",
    "        Returns:\n",
    "            attention_weights (Tensor): Softmax-normalized attention scores.\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        # Expand the hidden state across sequence length to match encoder outputs\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # Compute energy scores (alignment) using a feed-forward layer\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # Transpose energy tensor for matrix multiplication with attention parameter\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        # Expand attention parameter across batch size\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        # Compute attention weights using learned vector `v`\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        # Apply softmax to normalize scores across sequence length\n",
    "        return torch.softmax(attention_weights, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an\n",
    "    LSTM with Bahdanau attention. The decoder generates output tokens one by one\n",
    "    while dynamically focusing  on relevant parts of the encoder’s outputs using\n",
    "    the attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        # Embedding layer converts token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # LSTM layer processes embeddings and maintains hidden state across timesteps\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Linear layer maps concatenated attention context & LSTM output to vocab space\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        # Attention mechanism for dynamic focus on encoder outputs\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Current token input to the decoder.\n",
    "            hidden (Tensor): Previous hidden state from the LSTM.\n",
    "            cell (Tensor): Previous cell state from the LSTM.\n",
    "            encoder_outputs (Tensor): Encoder outputs from all timesteps.\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): Predicted token probabilities.\n",
    "            hidden (Tensor): Updated hidden state.\n",
    "            cell (Tensor): Updated cell state.\n",
    "            attention_weights (Tensor): Attention scores for each encoder timestep.\n",
    "        \"\"\"\n",
    "        # Expand input dimensions to match expected input shape for embedding\n",
    "        input = input.unsqueeze(1)  \n",
    "        # Convert token indices into dense embeddings\n",
    "        embedded = self.embedding(input)\n",
    "        # Forward pass through LSTM to generate new hidden and cell states\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # Compute attention weights using the current hidden state and encoder outputs\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        # Apply attention: generate weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # Flatten tensors for the fully connected layer\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        # Generate token probabilities using concatenated LSTM output and attention context\n",
    "        output = self.fc(torch.cat((lstm_output, context), dim=1))\n",
    "\n",
    "        return output, hidden, cell, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic Tokenization and Vocabulary Setup\n",
    "First, create a vocabulary and tokenize the input sentence (e.g., \"two plus four\" etc). To keep things extremely simple we'll manually establish a 'word-to-index' vocabulary whihc offers a simple map from word to tokensized integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estalish constants\n",
    "START_OF_SEQUENCE = \"<SOS>\"\n",
    "END_OF_SEQUENCE = \"<EOS>\"\n",
    "PADDING_SEQUENCE = \"<PAD>\"\n",
    "\n",
    "# Tokenization and vocab setup\n",
    "# Create a vocabulary mapping words to indices\n",
    "word_to_index = {START_OF_SEQUENCE: 0, END_OF_SEQUENCE: 1, PADDING_SEQUENCE: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Example input and target sequences\n",
    "input_sentence = \"two plus four\"\n",
    "target_sentence = \"equals six\"\n",
    "\n",
    "# Test Tokenized input and targets\n",
    "input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "target_tokens = [word_to_index[START_OF_SEQUENCE]] + [word_to_index[word] for word in target_sentence.split()] + [word_to_index[END_OF_SEQUENCE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMathWordProblemDataset(Dataset):\n",
    "    def __init__(self, input_sentences, target_sentences, word_to_index):\n",
    "        self.input_data = [[word_to_index[word] for word in sentence.split()] for sentence in input_sentences]\n",
    "        self.target_data = [[word_to_index[START_OF_SEQUENCE]] +\n",
    "                            [word_to_index[word] for word in sentence.split()] +\n",
    "                            [word_to_index[END_OF_SEQUENCE]] for sentence in target_sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.input_data[idx], dtype=torch.long), torch.tensor(self.target_data[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "input_sentences = [\"two plus four\"]\n",
    "target_sentences = [\"equals six\"]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BasicMathWordProblemDataset(input_sentences, target_sentences, word_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "input_size = len(word_to_index)  # Total vocabulary size\n",
    "output_size = len(word_to_index)  # Vocabulary size\n",
    "hidden_size = 128\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE])  # Ignore padding tokens\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "attention_matrix = []  # Store attention weights for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the training regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, encoder_save_name, decoder_save_name):\n",
    "    \"\"\"\n",
    "    Trains the sequence-to-sequence model using an encoder-decoder architecture.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        encoder_save_name (str): File name for saving the trained encoder model.\n",
    "        decoder_save_name (str): File name for saving the trained decoder model.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            # Move data to GPU if available, else keep on CPU\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            # Reset gradients before each batch\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # **ENCODER FORWARD PASS**\n",
    "            # Processes the input sequence and generates context for the decoder\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "            # **DECODER INITIALIZATION**\n",
    "            # The first input to the decoder is always the <SOS> token\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "            # Compute actual target sequence lengths (excluding padding)\n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()  # Maximum sequence length in batch\n",
    "\n",
    "            loss = 0  # Track loss per batch\n",
    "            \n",
    "            # **ITERATE OVER TARGET SEQUENCE (Adaptive length)**\n",
    "            for t in range(max_target_length):\n",
    "                # Check if sequences are still active (haven't reached <EOS>)\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():  # If all sequences are finished, stop decoding\n",
    "                    break\n",
    "\n",
    "                # **DECODER FORWARD PASS**\n",
    "                output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "\n",
    "                # Compute masked loss (only valid tokens contribute to loss)\n",
    "                loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "\n",
    "                # Apply teacher forcing: Use actual target token as next input\n",
    "                decoder_input = target_seq[:, t]  \n",
    "\n",
    "            # **BACKPROPAGATION & OPTIMIZATION**\n",
    "            loss.backward()  # Compute gradients\n",
    "            encoder_optimizer.step()  # Update encoder weights\n",
    "            decoder_optimizer.step()  # Update decoder weights\n",
    "\n",
    "        # Print epoch loss for tracking progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "        attention_matrix.append(attention_weights.cpu().detach().numpy())  # Convert to NumPy for plotting\n",
    "    \n",
    "    # **SAVE TRAINED MODELS**\n",
    "    torch.save(encoder.state_dict(), f\"{encoder_save_name}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"{decoder_save_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"basic_math_problem_encoder\", \"basic_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention(input_tokens, output_tokens, attention_matrix):\n",
    "    \"\"\"\n",
    "    Plot the attention weights as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        input_tokens (list of str): Tokens in the input sequence.\n",
    "        output_tokens (list of str): Tokens in the output sequence.\n",
    "        attention_matrix (np.array): Attention weights matrix (output_tokens x input_tokens).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attention_matrix, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=np.arange(len(input_tokens)), labels=input_tokens, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks=np.arange(len(output_tokens)), labels=output_tokens)\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.xlabel(\"Input Tokens\")\n",
    "    plt.ylabel(\"Output Tokens\")\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=100, with_attention_plot=False):\n",
    "    # Set the models to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Forward pass through the encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "    # Initialize the decoder\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]], dtype=torch.long)  # Start-of-Sequence token\n",
    "    decoder_hidden = hidden\n",
    "    decoder_cell = cell\n",
    "\n",
    "    # Generate output sequence and collect attention weights\n",
    "    output_sequence = []\n",
    "    attention_matrices = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get the token with the highest probability\n",
    "\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE]:  # Stop at End-of-Sequence token\n",
    "            break\n",
    "\n",
    "        output_sequence.append(predicted_token)\n",
    "        attention_matrices.append(attention_weights.cpu().detach().numpy())  # Save attention weights\n",
    "\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)  # Next decoder input\n",
    "\n",
    "    # Convert input and output tokens to words\n",
    "    input_sentence_tokens = [index_to_word[token] for token in input_tokens]\n",
    "    output_sentence_tokens = [index_to_word[token] for token in output_sequence]\n",
    "\n",
    "    # Stack attention matrices into a 2D array (output_tokens x input_tokens)\n",
    "    attention_matrix = np.vstack(attention_matrices)\n",
    "\n",
    "    # Print the input and output sentences\n",
    "    print(\"Input Sentence:\", input_sentence_tokens)\n",
    "    print(\"Generated Sentence:\", output_sentence_tokens)\n",
    "    print(\"Attention Weights Shape:\", attention_matrix)\n",
    "\n",
    "    # Visualize attention\n",
    "    if with_attention_plot:\n",
    "        # Plot the attention weights\n",
    "        plot_attention(input_sentence_tokens, output_sentence_tokens, attention_matrix)\n",
    "\n",
    "    # Convert token indices to words\n",
    "    return [index_to_word[token] for token in output_sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 128  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"basic_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"basic_math_problem_decoder.pth\"))\n",
    "\n",
    "input_sentence = \"one plus two\"\n",
    "\n",
    "word_to_index = {START_OF_SEQUENCE: 0, END_OF_SEQUENCE: 1, PADDING_SEQUENCE: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Test and visualize\n",
    "test(encoder, decoder, input_sentence, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a more complex dataset and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline vocabulary and tokenization capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "word_to_index = {START_OF_SEQUENCE: 0, END_OF_SEQUENCE: 1, PADDING_SEQUENCE: 2}  # establish only the baseline tokens this time\n",
    " \n",
    "# Function to tokenize a sentence and update mapping dynamically\n",
    "def tokenize(sentence, word_to_index):\n",
    "    tokens = []\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)  # Assign index to new words\n",
    "        tokens.append(word_to_index[word])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "def load_sequences_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df[\"Problem\"].tolist(), df[\"Solution\"].tolist()\n",
    "\n",
    "csv_file = \"simple_math_problems_addition_only.csv\"\n",
    "input_sentences, target_sentences = load_sequences_from_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize input and target sentences and convert into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and target sentences\n",
    "input_data = [tokenize(sentence, word_to_index) for sentence in input_sentences]\n",
    "target_data = [[word_to_index[START_OF_SEQUENCE]] + tokenize(sentence, word_to_index) + [word_to_index[END_OF_SEQUENCE]]\n",
    "               for sentence in target_sentences]\n",
    "\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "\n",
    "# Convert tokenized sentences into tensors\n",
    "input_tensors = [torch.tensor(seq) for seq in input_data]\n",
    "target_tensors = [torch.tensor(seq) for seq in target_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data, including padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic padding\n",
    "input_padded = pad_sequence(input_tensors, batch_first=True, padding_value=word_to_index[PADDING_SEQUENCE])\n",
    "target_padded = pad_sequence(target_tensors, batch_first=True, padding_value=word_to_index[PADDING_SEQUENCE])\n",
    "\n",
    "class DynamicMathWordProblemDataset(Dataset):\n",
    "    def __init__(self, input_padded, target_padded):\n",
    "        self.input_data = input_padded\n",
    "        self.target_data = target_padded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "dataset = DynamicMathWordProblemDataset(input_padded, target_padded)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = len(word_to_index)  # Vocabulary size\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE]) # Ignore padding tokens\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0005)#0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0005)#0.001)\n",
    "\n",
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"addition_only_math_problem_encoder\", \"addition_only_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the revised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"addition_only_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"addition_only_math_problem_decoder.pth\"))\n",
    "\n",
    "#thirty eight plus twenty seven,sixty five\n",
    "\n",
    "input_sentence = \"thirty eight plus twenty six\" #sixty four\n",
    "\n",
    "# Test and visualize\n",
    "test_token_indices_to_words = test(encoder, decoder, input_sentence, word_to_index, index_to_word, with_attention_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the test predictions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "    # Initialize decoder with <SOS> token\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]], dtype=torch.long)\n",
    "    decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "    # Generate output sequence\n",
    "    output_sequence = []\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get token with highest probability\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE]:  # Stop at <EOS> token\n",
    "            break\n",
    "        output_sequence.append(predicted_token)\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert token indices to words\n",
    "    return [index_to_word[token] for token in output_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edit_distance\n",
    "\n",
    "def evaluate_with_edit_distance(test_data, encoder, decoder, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Evaluate the sequence-to-sequence model using edit distance.\n",
    "\n",
    "    Args:\n",
    "        test_data (list): List of (input_sentence, ground_truth_sentence) pairs.\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        word_to_index (dict): Word-to-index mapping.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "\n",
    "    Returns:\n",
    "        float: Average edit distance across the test dataset.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    for input_sentence, ground_truth_sentence in test_data:\n",
    "        # Generate predicted sequence\n",
    "        predicted_sentence = generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word)\n",
    "\n",
    "        # Compute edit distance\n",
    "        distance = edit_distance.SequenceMatcher(a=ground_truth_sentence, b=predicted_sentence).distance()\n",
    "        total_distance += distance\n",
    "\n",
    "        # Debugging: Print example results\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Ground Truth: {ground_truth_sentence}\")\n",
    "        print(f\"Predicted: {predicted_sentence}\")\n",
    "        print(f\"Edit Distance: {distance}\\n\")\n",
    "\n",
    "    # Calculate average edit distance\n",
    "    average_distance = total_distance / len(test_data)\n",
    "    print(f\"Average Edit Distance: {average_distance}\")\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset: [(input_sentence, ground_truth_sentence)]\n",
    "test_data = [\n",
    "    (\"thirty eight plus twenty six\", [START_OF_SEQUENCE, \"sixty\", \"four\"]),\n",
    "]\n",
    "\n",
    "# Assume you have trained encoder and decoder models\n",
    "average_distance = evaluate_with_edit_distance(\n",
    "    test_data, encoder, decoder, word_to_index, index_to_word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to be tuned\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 128, 512)  # Hidden layer size\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)  # LR (log-scaled search)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate\n",
    "    \n",
    "    # Define encoder-decoder with suggested hyperparameters\n",
    "    encoder = Encoder(input_size, hidden_size).to(device)\n",
    "    decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "    # Apply dropout in LSTM layers\n",
    "    encoder.lstm.dropout = dropout_rate\n",
    "    decoder.lstm.dropout = dropout_rate\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE])\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Run one epoch to evaluate hyperparameter performance\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "        decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]] * input_seq.size(0), device=device)\n",
    "        decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(target_seq.size(1)):\n",
    "            output, decoder_hidden, decoder_cell, _ = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "            loss += criterion(output, target_seq[:, t])\n",
    "            decoder_input = target_seq[:, t]  # Teacher forcing\n",
    "        \n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)  # Optuna minimizes this loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform optimum hyperparameter search\n",
    "\n",
    "\n",
    "Best hyperparameters: {'hidden_size': 283, 'learning_rate': 0.004869044905756817, 'dropout_rate': 0.28901830138217327}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize loss\n",
    "study.optimize(objective, n_trials=30)  # Run 30 optimization trials\n",
    "\n",
    "# Print best hyperparameter combination\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement hyperparameter changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
