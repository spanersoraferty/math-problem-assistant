{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an LSTM. \n",
    "    Converts a sequence of token indices into a hidden representation \n",
    "    that will be used by the decoder for sequence generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder module.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer converts token indices into dense vectors for better representation.\n",
    "        # The embeddings refer to vector representations of words or tokens used as input to the model.\n",
    "        # Instead of feeding raw word indices (which don’t capture meaning), feedingembeddings allows\n",
    "        # the LSTM to learn meaningful semantic relationships between words.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # LSTM layer processes embedded input sequences to generate hidden states\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing token indices for a batch of input sentences.\n",
    "\n",
    "        Returns:\n",
    "            outputs (Tensor): Encoder outputs at each time step.\n",
    "            hidden (Tensor): Final hidden state of the LSTM.\n",
    "            cell (Tensor): Final cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_seq)            # Convert input tokens into embeddings \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)   # Process embeddings through LSTM\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Attention model for the sequence-to-sequence math problem assistant\n",
    "    which implements a 'Bahdanau attention' mechanism. This allows the model\n",
    "    to dynamically compute attention scores based on the decoder's hidden\n",
    "    state and the encoder's outputs, allowing the model to focus on relevant\n",
    "    parts of the input sequence at each decoding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"\n",
    "        Initializes the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden state of the LSTM.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        # Learnable linear transformation to compute alignment scores\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # Learnable parameter to compute weighted attention scores\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Computes attention weights using Bahdanau's additive attention method.\n",
    "\n",
    "        Args:\n",
    "            hidden (Tensor): Decoder hidden state at the current time step.\n",
    "            encoder_outputs (Tensor): Encoder outputs at all time steps.\n",
    "\n",
    "        Returns:\n",
    "            attention_weights (Tensor): Softmax-normalized attention scores.\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        # Expand the hidden state across sequence length to match encoder outputs\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # Compute energy scores (alignment) using a feed-forward layer\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # Transpose energy tensor for matrix multiplication with attention parameter\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        # Expand attention parameter across batch size\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        # Compute attention weights using learned vector `v`\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        # Apply softmax to normalize scores across sequence length\n",
    "        return torch.softmax(attention_weights, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an\n",
    "    LSTM with Bahdanau attention. The decoder generates output tokens one by one\n",
    "    while dynamically focusing  on relevant parts of the encoder’s outputs using\n",
    "    the attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        # Embedding layer converts token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # LSTM layer processes embeddings and maintains hidden state across timesteps\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Linear layer maps concatenated attention context & LSTM output to vocab space\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        # Attention mechanism for dynamic focus on encoder outputs\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Current token input to the decoder.\n",
    "            hidden (Tensor): Previous hidden state from the LSTM.\n",
    "            cell (Tensor): Previous cell state from the LSTM.\n",
    "            encoder_outputs (Tensor): Encoder outputs from all timesteps.\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): Predicted token probabilities.\n",
    "            hidden (Tensor): Updated hidden state.\n",
    "            cell (Tensor): Updated cell state.\n",
    "            attention_weights (Tensor): Attention scores for each encoder timestep.\n",
    "        \"\"\"\n",
    "        # Expand input dimensions to match expected input shape for embedding\n",
    "        input = input.unsqueeze(1)  \n",
    "        # Convert token indices into dense embeddings\n",
    "        embedded = self.embedding(input)\n",
    "        # Forward pass through LSTM to generate new hidden and cell states\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # Compute attention weights using the current hidden state and encoder outputs\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        # Apply attention: generate weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # Flatten tensors for the fully connected layer\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        # Generate token probabilities using concatenated LSTM output and attention context\n",
    "        output = self.fc(torch.cat((lstm_output, context), dim=1))\n",
    "\n",
    "        return output, hidden, cell, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic Tokenization and Vocabulary Setup\n",
    "First, create a vocabulary and tokenize the input sentence (e.g., \"two plus four\" etc). To keep things extremely simple we'll manually establish a 'word-to-index' vocabulary whihc offers a simple map from word to tokensized integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and vocab setup\n",
    "# Create a vocabulary mapping words to indices\n",
    "word_to_index = {\"<SOS>\": 0, \"<EOS>\": 1, \"<PAD>\": 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Example input and target sequences\n",
    "input_sentence = \"two plus four\"\n",
    "target_sentence = \"equals six\"\n",
    "\n",
    "# Test Tokenized input and targets\n",
    "input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "target_tokens = [word_to_index[\"<SOS>\"]] + [word_to_index[word] for word in target_sentence.split()] + [word_to_index[\"<EOS>\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMathWordProblemDataset(Dataset):\n",
    "    def __init__(self, input_sentences, target_sentences, word_to_index):\n",
    "        self.input_data = [[word_to_index[word] for word in sentence.split()] for sentence in input_sentences]\n",
    "        self.target_data = [[word_to_index[\"<SOS>\"]] +\n",
    "                            [word_to_index[word] for word in sentence.split()] +\n",
    "                            [word_to_index[\"<EOS>\"]] for sentence in target_sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.input_data[idx], dtype=torch.long), torch.tensor(self.target_data[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "input_sentences = [\"two plus four\"]\n",
    "target_sentences = [\"equals six\"]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BasicMathWordProblemDataset(input_sentences, target_sentences, word_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "input_size = len(word_to_index)  # Total vocabulary size\n",
    "output_size = len(word_to_index)  # Vocabulary size\n",
    "hidden_size = 128\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<PAD>\"])  # Ignore padding tokens\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "attention_matrix = []  # Store attention weights for visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the training regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, encoder_save_name, decoder_save_name):\n",
    "    \"\"\"\n",
    "    Trains the sequence-to-sequence model using an encoder-decoder architecture.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        encoder_save_name (str): File name for saving the trained encoder model.\n",
    "        decoder_save_name (str): File name for saving the trained decoder model.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            # Move data to GPU if available, else keep on CPU\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            # Reset gradients before each batch\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # **ENCODER FORWARD PASS**\n",
    "            # Processes the input sequence and generates context for the decoder\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "            # **DECODER INITIALIZATION**\n",
    "            # The first input to the decoder is always the <SOS> token\n",
    "            decoder_input = torch.tensor([word_to_index[\"<SOS>\"]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "            # Compute actual target sequence lengths (excluding padding)\n",
    "            target_lengths = (target_seq != word_to_index[\"<PAD>\"]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()  # Maximum sequence length in batch\n",
    "\n",
    "            loss = 0  # Track loss per batch\n",
    "            \n",
    "            # **ITERATE OVER TARGET SEQUENCE (Adaptive length)**\n",
    "            for t in range(max_target_length):\n",
    "                # Check if sequences are still active (haven't reached <EOS>)\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():  # If all sequences are finished, stop decoding\n",
    "                    break\n",
    "\n",
    "                # **DECODER FORWARD PASS**\n",
    "                output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "\n",
    "                attention_matrix.append(attention_weights.cpu().detach().numpy())  # Convert to NumPy for plotting\n",
    "\n",
    "                # Compute masked loss (only valid tokens contribute to loss)\n",
    "                loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "\n",
    "                # Apply teacher forcing: Use actual target token as next input\n",
    "                decoder_input = target_seq[:, t]  \n",
    "\n",
    "            # **BACKPROPAGATION & OPTIMIZATION**\n",
    "            loss.backward()  # Compute gradients\n",
    "            encoder_optimizer.step()  # Update encoder weights\n",
    "            decoder_optimizer.step()  # Update decoder weights\n",
    "\n",
    "        # Print epoch loss for tracking progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    # **SAVE TRAINED MODELS**\n",
    "    torch.save(encoder.state_dict(), f\"{encoder_save_name}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"{decoder_save_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 9.424482345581055\n",
      "Epoch 2/100, Loss: 8.697931289672852\n",
      "Epoch 3/100, Loss: 7.979613304138184\n",
      "Epoch 4/100, Loss: 7.2646484375\n",
      "Epoch 5/100, Loss: 6.566460132598877\n",
      "Epoch 6/100, Loss: 5.9007415771484375\n",
      "Epoch 7/100, Loss: 5.281635284423828\n",
      "Epoch 8/100, Loss: 4.720970153808594\n",
      "Epoch 9/100, Loss: 4.2247233390808105\n",
      "Epoch 10/100, Loss: 3.7915377616882324\n",
      "Epoch 11/100, Loss: 3.4147372245788574\n",
      "Epoch 12/100, Loss: 3.0856740474700928\n",
      "Epoch 13/100, Loss: 2.796184539794922\n",
      "Epoch 14/100, Loss: 2.539581537246704\n",
      "Epoch 15/100, Loss: 2.310718297958374\n",
      "Epoch 16/100, Loss: 2.1057820320129395\n",
      "Epoch 17/100, Loss: 1.922117829322815\n",
      "Epoch 18/100, Loss: 1.7577781677246094\n",
      "Epoch 19/100, Loss: 1.610344648361206\n",
      "Epoch 20/100, Loss: 1.4770331382751465\n",
      "Epoch 21/100, Loss: 1.35555100440979\n",
      "Epoch 22/100, Loss: 1.2440009117126465\n",
      "Epoch 23/100, Loss: 1.141202449798584\n",
      "Epoch 24/100, Loss: 1.0470316410064697\n",
      "Epoch 25/100, Loss: 0.9616942405700684\n",
      "Epoch 26/100, Loss: 0.8844259977340698\n",
      "Epoch 27/100, Loss: 0.8133978843688965\n",
      "Epoch 28/100, Loss: 0.7470656633377075\n",
      "Epoch 29/100, Loss: 0.6850978136062622\n",
      "Epoch 30/100, Loss: 0.6279094219207764\n",
      "Epoch 31/100, Loss: 0.5757177472114563\n",
      "Epoch 32/100, Loss: 0.5281506776809692\n",
      "Epoch 33/100, Loss: 0.4845822751522064\n",
      "Epoch 34/100, Loss: 0.4445706009864807\n",
      "Epoch 35/100, Loss: 0.4079032242298126\n",
      "Epoch 36/100, Loss: 0.3743988573551178\n",
      "Epoch 37/100, Loss: 0.3438332676887512\n",
      "Epoch 38/100, Loss: 0.31603553891181946\n",
      "Epoch 39/100, Loss: 0.290915846824646\n",
      "Epoch 40/100, Loss: 0.2683744430541992\n",
      "Epoch 41/100, Loss: 0.24821050465106964\n",
      "Epoch 42/100, Loss: 0.2301384061574936\n",
      "Epoch 43/100, Loss: 0.21386750042438507\n",
      "Epoch 44/100, Loss: 0.19916215538978577\n",
      "Epoch 45/100, Loss: 0.18585063517093658\n",
      "Epoch 46/100, Loss: 0.1737973392009735\n",
      "Epoch 47/100, Loss: 0.16287453472614288\n",
      "Epoch 48/100, Loss: 0.15295805037021637\n",
      "Epoch 49/100, Loss: 0.1439325213432312\n",
      "Epoch 50/100, Loss: 0.1356981247663498\n",
      "Epoch 51/100, Loss: 0.12817694246768951\n",
      "Epoch 52/100, Loss: 0.12130068242549896\n",
      "Epoch 53/100, Loss: 0.11500979959964752\n",
      "Epoch 54/100, Loss: 0.10924658924341202\n",
      "Epoch 55/100, Loss: 0.1039566844701767\n",
      "Epoch 56/100, Loss: 0.0990903228521347\n",
      "Epoch 57/100, Loss: 0.09460420906543732\n",
      "Epoch 58/100, Loss: 0.09046227484941483\n",
      "Epoch 59/100, Loss: 0.08663231879472733\n",
      "Epoch 60/100, Loss: 0.0830860435962677\n",
      "Epoch 61/100, Loss: 0.07979835569858551\n",
      "Epoch 62/100, Loss: 0.07674457132816315\n",
      "Epoch 63/100, Loss: 0.07390221208333969\n",
      "Epoch 64/100, Loss: 0.07125099003314972\n",
      "Epoch 65/100, Loss: 0.06877157092094421\n",
      "Epoch 66/100, Loss: 0.06644894182682037\n",
      "Epoch 67/100, Loss: 0.06426875293254852\n",
      "Epoch 68/100, Loss: 0.06221969425678253\n",
      "Epoch 69/100, Loss: 0.06029035151004791\n",
      "Epoch 70/100, Loss: 0.05847294256091118\n",
      "Epoch 71/100, Loss: 0.05675801262259483\n",
      "Epoch 72/100, Loss: 0.055138424038887024\n",
      "Epoch 73/100, Loss: 0.05360657721757889\n",
      "Epoch 74/100, Loss: 0.05215635523200035\n",
      "Epoch 75/100, Loss: 0.050782132893800735\n",
      "Epoch 76/100, Loss: 0.04947718232870102\n",
      "Epoch 77/100, Loss: 0.04823703318834305\n",
      "Epoch 78/100, Loss: 0.047057636082172394\n",
      "Epoch 79/100, Loss: 0.04593389481306076\n",
      "Epoch 80/100, Loss: 0.044863052666187286\n",
      "Epoch 81/100, Loss: 0.04384057968854904\n",
      "Epoch 82/100, Loss: 0.04286370798945427\n",
      "Epoch 83/100, Loss: 0.04192929342389107\n",
      "Epoch 84/100, Loss: 0.041034672409296036\n",
      "Epoch 85/100, Loss: 0.040177762508392334\n",
      "Epoch 86/100, Loss: 0.039354708045721054\n",
      "Epoch 87/100, Loss: 0.038564350455999374\n",
      "Epoch 88/100, Loss: 0.03780483081936836\n",
      "Epoch 89/100, Loss: 0.03707404434680939\n",
      "Epoch 90/100, Loss: 0.03637024387717247\n",
      "Epoch 91/100, Loss: 0.035691674798727036\n",
      "Epoch 92/100, Loss: 0.03503740578889847\n",
      "Epoch 93/100, Loss: 0.03440544754266739\n",
      "Epoch 94/100, Loss: 0.03379509225487709\n",
      "Epoch 95/100, Loss: 0.0332048237323761\n",
      "Epoch 96/100, Loss: 0.0326339453458786\n",
      "Epoch 97/100, Loss: 0.03208151459693909\n",
      "Epoch 98/100, Loss: 0.03154612332582474\n",
      "Epoch 99/100, Loss: 0.031027548015117645\n",
      "Epoch 100/100, Loss: 0.03052390180528164\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"basic_math_problem_encoder\", \"basic_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention(attention_matrix, input_tokens, target_tokens):\n",
    "    \"\"\"\n",
    "    Plots attention weights as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        attention_matrix (list): Attention weights captured during decoding.\n",
    "        input_tokens (list): List of input words.\n",
    "        target_tokens (list): List of target words.\n",
    "    \"\"\"\n",
    "    attention_matrix = np.array(attention_matrix).squeeze().T  # Convert to NumPy array\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    # Generate heatmap using Seaborn\n",
    "    sns.heatmap(attention_matrix, xticklabels=input_tokens, yticklabels=target_tokens, \n",
    "                cmap=\"Blues\", linewidths=0.5, annot=True, fmt=\".2f\")\n",
    "\n",
    "    plt.xlabel(\"Input Sequence\")\n",
    "    plt.ylabel(\"Generated Output\")\n",
    "    plt.title(\"Attention Weights\")\n",
    "    plt.show()\n",
    "    \n",
    "# Call the attention visualization function after generating output\n",
    "plot_attention(attention_matrix, input_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, input_sentence):\n",
    "    # Set the models to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # generate the output sequence\n",
    "    # Forward pass through the encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "    # Initialize the decoder\n",
    "    decoder_input = torch.tensor([word_to_index[\"<SOS>\"]], dtype=torch.long)  # Start-of-Sequence token\n",
    "    decoder_hidden = hidden\n",
    "    decoder_cell = cell\n",
    "\n",
    "    # Generate output sequence\n",
    "    output_sequence = []\n",
    "    target_length = 10  # Maximum output sequence length\n",
    "\n",
    "    for _ in range(target_length):\n",
    "        output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get token with the highest probability\n",
    "        if predicted_token == word_to_index[\"<EOS>\"]:  # Stop at End-of-Sequence token\n",
    "            break\n",
    "        output_sequence.append(predicted_token)\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    print(\"Word to index: \", word_to_index)\n",
    "    print(\"Input tokens: \", input_tokens)\n",
    "    print(\"Tokenized output sentence: \", output_sequence)\n",
    "\n",
    "    # Convert tokens back to words\n",
    "    output_sentence = \" \".join([index_to_word[token] for token in output_sequence])\n",
    "    print(\"Output Sentence:\", output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word to index:  {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, 'two': 3, 'plus': 4, 'four': 5, 'equals': 6, 'six': 7, 'three': 8, 'minus': 9, 'one': 10}\n",
      "Input tokens:  [3, 4, 5]\n",
      "Tokenized output sentence:  [0, 6, 7]\n",
      "Output Sentence: <SOS> equals six\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 128  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"basic_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"basic_math_problem_decoder.pth\"))\n",
    "\n",
    "input_sentence = \"two plus four\"\n",
    "\n",
    "test(encoder, decoder, input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a more complex dataset and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline vocabulary and tokenization capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "word_to_index = {\"<SOS>\": 0, \"<EOS>\": 1, \"<PAD>\": 2}  \n",
    " \n",
    "# Function to tokenize a sentence and update mapping dynamically\n",
    "def tokenize(sentence, word_to_index):\n",
    "    tokens = []\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)  # Assign index to new words\n",
    "        tokens.append(word_to_index[word])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "def load_sequences_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df[\"Problem\"].tolist(), df[\"Solution\"].tolist()\n",
    "\n",
    "csv_file = \"simple_math_problems_addition_only.csv\"\n",
    "input_sentences, target_sentences = load_sequences_from_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize input and target sentences and convert into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and target sentences\n",
    "input_data = [tokenize(sentence, word_to_index) for sentence in input_sentences]\n",
    "target_data = [[word_to_index[\"<SOS>\"]] + tokenize(sentence, word_to_index) + [word_to_index[\"<EOS>\"]]\n",
    "               for sentence in target_sentences]\n",
    "\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "\n",
    "# Convert tokenized sentences into tensors\n",
    "input_tensors = [torch.tensor(seq) for seq in input_data]\n",
    "target_tensors = [torch.tensor(seq) for seq in target_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data, including padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic padding\n",
    "input_padded = pad_sequence(input_tensors, batch_first=True, padding_value=word_to_index[\"<PAD>\"])\n",
    "target_padded = pad_sequence(target_tensors, batch_first=True, padding_value=word_to_index[\"<PAD>\"])\n",
    "\n",
    "class DynamicMathWordProblemDataset(Dataset):\n",
    "    def __init__(self, input_padded, target_padded):\n",
    "        self.input_data = input_padded\n",
    "        self.target_data = target_padded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "dataset = DynamicMathWordProblemDataset(input_padded, target_padded)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 6.720978260040283\n",
      "Epoch 2/100, Loss: 4.95189905166626\n",
      "Epoch 3/100, Loss: 4.143381118774414\n",
      "Epoch 4/100, Loss: 3.6392662525177\n",
      "Epoch 5/100, Loss: 2.93449330329895\n",
      "Epoch 6/100, Loss: 2.2459475994110107\n",
      "Epoch 7/100, Loss: 1.5081427097320557\n",
      "Epoch 8/100, Loss: 0.9893795847892761\n",
      "Epoch 9/100, Loss: 0.6708764433860779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[222], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m decoder_optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(decoder\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\u001b[38;5;66;03m#0.001)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Define the number of epochs\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m train(num_epochs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddition_only_math_problem_encoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maddition_only_math_problem_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[213], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(num_epochs, encoder_save_name, decoder_save_name)\u001b[0m\n\u001b[0;32m     52\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m target_seq[:, t]  \n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# **BACKPROPAGATION & OPTIMIZATION**\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Compute gradients\u001b[39;00m\n\u001b[0;32m     56\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update encoder weights\u001b[39;00m\n\u001b[0;32m     57\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update decoder weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sohegarty\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    527\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sohegarty\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[0;32m    268\u001b[0m     tensors,\n\u001b[0;32m    269\u001b[0m     grad_tensors_,\n\u001b[0;32m    270\u001b[0m     retain_graph,\n\u001b[0;32m    271\u001b[0m     create_graph,\n\u001b[0;32m    272\u001b[0m     inputs,\n\u001b[0;32m    273\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sohegarty\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    746\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = len(word_to_index)  # Vocabulary size\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<PAD>\"])\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0005)#0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0005)#0.001)\n",
    "\n",
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"addition_only_math_problem_encoder\", \"addition_only_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the revised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"addition_only_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"addition_only_math_problem_decoder.pth\"))\n",
    "\n",
    "input_sentence = \"twenty plus fifty nine\" #zero point eight six\n",
    "\n",
    "test(encoder, decoder, input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the trained model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to be tuned\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 128, 512)  # Hidden layer size\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)  # LR (log-scaled search)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate\n",
    "    \n",
    "    # Define encoder-decoder with suggested hyperparameters\n",
    "    encoder = Encoder(input_size, hidden_size).to(device)\n",
    "    decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "    # Apply dropout in LSTM layers\n",
    "    encoder.lstm.dropout = dropout_rate\n",
    "    decoder.lstm.dropout = dropout_rate\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<PAD>\"])\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Run one epoch to evaluate hyperparameter performance\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "        decoder_input = torch.tensor([word_to_index[\"<SOS>\"]] * input_seq.size(0), device=device)\n",
    "        decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(target_seq.size(1)):\n",
    "            output, decoder_hidden, decoder_cell, _ = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "            loss += criterion(output, target_seq[:, t])\n",
    "            decoder_input = target_seq[:, t]  # Teacher forcing\n",
    "        \n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)  # Optuna minimizes this loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform optimum hyperparameter search\n",
    "\n",
    "\n",
    "Best hyperparameters: {'hidden_size': 283, 'learning_rate': 0.004869044905756817, 'dropout_rate': 0.28901830138217327}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize loss\n",
    "study.optimize(objective, n_trials=30)  # Run 30 optimization trials\n",
    "\n",
    "# Print best hyperparameter combination\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
