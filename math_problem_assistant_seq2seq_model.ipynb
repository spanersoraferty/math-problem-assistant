{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Add our overview narrative here!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This Encoder class is a part of the larger sequence-to-sequence model designed for processing mathematical word problems using an LSTM. It takes input sequences (math question) represented as token indices and converts them into dense vector embeddings using an embedding layer, allowing the model to capture meaningful semantic relationships between words. The embedded representations then pass through an LSTM layer, which extracts temporal dependencies and generates both hidden and cell states that encode contextual information about the input sequence. A dropout layer is applied to the embeddings to reduce overfitting and improve generalization. Ultimately, the encoder produces a set of outputs from each time step and a final hidden representation, which the decoder later uses to generate responses or solutions (the expected answer to our math question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an LSTM. \n",
    "    Converts a sequence of token indices into a hidden representation \n",
    "    that will be used by the decoder for sequence generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder module.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer converts token indices into dense vector representations.\n",
    "        # Instead of using raw word indices (which lack meaning), embeddings allow\n",
    "        # the LSTM to learn meaningful semantic relationships between words.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # LSTM layer processes embedded input sequences to generate hidden states.\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Dropout layer applied to embeddings to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing token indices for a batch of input sentences.\n",
    "\n",
    "        Returns:\n",
    "            outputs (Tensor): Encoder outputs at each time step.\n",
    "            hidden (Tensor): Final hidden state of the LSTM.\n",
    "            cell (Tensor): Final cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_seq)           # Convert input tokens into embeddings\n",
    "        embedded = self.dropout(embedded)             # Apply dropout before LSTM processing\n",
    "        outputs, (hidden, cell) = self.lstm(embedded) # Process embeddings through LSTM\n",
    "\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The Attention class implements 'Bahdanau attention', which dynamically calculates attention scores based on the decoder’s hidden state and the encoder’s outputs. Instead of relying on a fixed context vector, this method allows the decoder to focus on specific parts of the input sequence at each decoding step. The attention mechanism works by concatenating the decoder's hidden state with the encoder’s outputs, passing them through a linear transformation, and applying the tanh activation function. A learnable vector (v) helps compute alignment scores, which are then normalized using softmax to generate attention weights—these determine the importance of each encoder output when predicting the next token. A dropout layer is also applied to prevent overfitting. The output is a set of attention weights that guides the decoder in generating more context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Implements Bahdanau attention mechanism for a sequence-to-sequence model. \n",
    "    Dynamically computes attention scores based on the decoder’s hidden state \n",
    "    and the encoder’s outputs, allowing the decoder to focus on relevant \n",
    "    parts of the input sequence at each decoding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden state of the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        # Linear layer to compute alignment scores\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # Learnable vector for attention computation\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        # Dropout layer to regularize attention mechanism and prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Computes attention weights using Bahdanau's additive attention method.\n",
    "\n",
    "        Args:\n",
    "            hidden (Tensor): Decoder hidden state at the current time step.\n",
    "            encoder_outputs (Tensor): Encoder outputs at all time steps.\n",
    "\n",
    "        Returns:\n",
    "            attention_weights (Tensor): Softmax-normalized attention scores.\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        # Expand the decoder hidden state across the sequence length\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # Concatenate hidden state with encoder outputs to compute alignment scores\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # Apply dropout to energy scores before computing attention weights\n",
    "        energy = self.dropout(energy)\n",
    "        # Transpose energy tensor for compatibility with the attention vector\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        # Expand learned vector `v` across batch size\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        # Compute attention weights via matrix multiplication\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        # Apply softmax to normalize scores across sequence length\n",
    "        return torch.softmax(attention_weights, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The Decoder class plays a crucial role in our sequence-to-sequence model designed to solve math word problems. When given a problem as an input sequence, the Encoder first processes it into a hidden representation, which the Decoder then uses to generate the solution step by step. At each step, the attention mechanism dynamically selects the most relevant parts of the encoded problem statement, allowing the Decoder to focus on specific numerical relationships and mathematical operations. The LSTM maintains contextual understanding across time steps, helping track dependencies between numbers and mathematical operators. As the model generates each token in the solution, it refines its prediction using previously computed values, making the process similar to how humans break down word problems into logical steps. This structure ensures that the model interprets and solves math problems contextually, rather than simply memorizing formulas, enabling it to generalize across different problem types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Decoder for the sequence-to-sequence math problem assistant model using an \n",
    "    LSTM with Bahdanau attention. The decoder generates output tokens one by one \n",
    "    while dynamically focusing on relevant parts of the encoder’s outputs using \n",
    "    the attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        # Embedding layer converts token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # LSTM layer processes embeddings and maintains hidden state across timesteps\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Linear layer maps concatenated attention context & LSTM output to vocab space\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        # Dropout layer to regularize embeddings before passing them to LSTM\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        # Attention mechanism for dynamic focus on encoder outputs\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Current token input to the decoder.\n",
    "            hidden (Tensor): Previous hidden state from the LSTM.\n",
    "            cell (Tensor): Previous cell state from the LSTM.\n",
    "            encoder_outputs (Tensor): Encoder outputs from all timesteps.\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): Predicted token probabilities.\n",
    "            hidden (Tensor): Updated hidden state.\n",
    "            cell (Tensor): Updated cell state.\n",
    "            attention_weights (Tensor): Attention scores for each encoder timestep.\n",
    "        \"\"\"\n",
    "        # Expand input dimensions to match expected input shape for embedding\n",
    "        input = input.unsqueeze(1)  \n",
    "        # Convert token indices into dense embeddings & apply dropout for regularization\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # Forward pass through LSTM to generate new hidden and cell states\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # Compute attention weights using the current hidden state and encoder outputs\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        # Apply attention: generate weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # Flatten tensors for the fully connected layer\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        # Generate token probabilities using concatenated LSTM output and attention context\n",
    "        output = self.fc(torch.cat((lstm_output, context), dim=1))\n",
    "\n",
    "        return output, hidden, cell, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic tokenisation and vocabulary Setup\n",
    "\n",
    "Here we set up the constants and tokenisation process for a sequence-to-sequence math problem assistant. It first defines special tokens <SOS>, <EOS>, and <PAD> to mark the start, end, and padding of sequences. Then, it creates a vocabulary mapping that converts words into numerical indices, allowing the model to process text as numbers. The reverse mapping (index_to_word) ensures that predictions can be decoded back into words. Finally, the script tokenizes example input and target sequences, transforming for example, \"two plus four\" into a list of indices [3, 4, 5] and the target \"equals six\" into [0, 6, 7, 1], ensuring that the decoder starts with <SOS> and ends with <EOS>. This setup enables the neural network to work with structured input-output pairs for training our expected math-solving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estalish constants\n",
    "START_OF_SEQUENCE_TOKEN = \"<SOS>\"\n",
    "END_OF_SEQUENCE_TOKEN = \"<EOS>\"\n",
    "PADDING_SEQUENCE_TOKEN = \"<PAD>\"\n",
    "\n",
    "# Tokenization and vocab setup\n",
    "# Create a vocabulary mapping words to indices\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Example input and target sequences\n",
    "input_sentence = \"two plus four\"\n",
    "target_sentence = \"equals six\"\n",
    "\n",
    "# Test Tokenized input and targets\n",
    "input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "target_tokens = [word_to_index[START_OF_SEQUENCE_TOKEN]] + [word_to_index[word] for word in target_sentence.split()] + [word_to_index[END_OF_SEQUENCE_TOKEN]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data\n",
    "\n",
    "This BasicMathWordProblemDataset class creates a custom PyTorch Dataset for training our sequence-to-sequence model that solves basic math word problems. It converts input and target sentences (our math questions) into lists of token indices using a predefined vocabulary (word_to_index). The target sequence is prepended with <SOS> and appended with <EOS> tokens to indicate the start and end of the output. The class implements essential dataset methods, notablu __len__() which returns the number of samples, while __getItem__() retrieves tokenized tensors for input and target sequences, ensuring compatibility with PyTorch models. Finally, the dataset is wrapped in a DataLoader, allowing efficient batch processing with shuffling to improve learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMathWordProblemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for tokenized math word problems.\n",
    "    This class converts input sentences into tokenized sequences using \n",
    "    a predefined vocabulary (`word_to_index`) and structures target \n",
    "    sequences with <SOS> (start-of-sequence) and <EOS> (end-of-sequence) \n",
    "    tokens for proper handling by the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_sentences, target_sentences, word_to_index):\n",
    "        \"\"\"\n",
    "        Initializes the dataset by converting words into token indices.\n",
    "        Args:\n",
    "            input_sentences (list of str): List of math word problems (input).\n",
    "            target_sentences (list of str): Corresponding solutions (output).\n",
    "            word_to_index (dict): Mapping of words to numerical token indices.\n",
    "        \"\"\"\n",
    "        # Tokenize input sentences into numerical sequences\n",
    "        self.input_data = [[word_to_index[word] for word in sentence.split()] for sentence in input_sentences]\n",
    "        # Tokenize target sentences while adding special tokens <SOS> and <EOS>\n",
    "        self.target_data = [[word_to_index[START_OF_SEQUENCE_TOKEN]] + \n",
    "                            [word_to_index[word] for word in sentence.split()] + \n",
    "                            [word_to_index[END_OF_SEQUENCE_TOKEN]] for sentence in target_sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Returns:\n",
    "            int: Total number of input-target pairs.\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the tokenized input and target sequences as PyTorch tensors.\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: Tokenized input and target sequences.\n",
    "        \"\"\"\n",
    "        return torch.tensor(self.input_data[idx], dtype=torch.long), torch.tensor(self.target_data[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "input_sentences = [\"two plus four\"]\n",
    "target_sentences = [\"equals six\"]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BasicMathWordProblemDataset(input_sentences, target_sentences, word_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters\n",
    "\n",
    "Here we initialise the key components for training our sequence-to-sequence math word problem solving model. The encoder and decoder are instantiated with the vocabulary size (input_size and output_size) and a hidden state of 128, allowing the model to process and generate our math solutions. The CrossEntropy loss function is used with padding tokens ignored to prevent unnecessary calculations from affecting training. Adam optimizers are applied to both the encoder and decoder with a learning rate of 0.001, enabling stable gradient updates. Finally, an attention matrix is initialized to store attention weights, which will later be used for visualization, thus helping to analyze how the model focuses on different parts of the input when generating solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "input_size = len(word_to_index)  # Total vocabulary size\n",
    "output_size = len(word_to_index)  # Vocabulary size\n",
    "hidden_size = 128\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "# Ignore padding tokens\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN])  \n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# Store attention weights for visualization\n",
    "attention_matrix = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss over epochs\n",
    "\n",
    "This function visualizes the training loss over epochs using Matplotlib. It takes a list of loss values (epoch_losses), representing the loss at each epoch, and plots them on a graph to track performance trends over time. This is useful for monitoring model convergence and diagnosing potential issues such as overfitting or slow learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(epoch_losses):\n",
    "    \"\"\"\n",
    "    Plots the training loss over epochs.\n",
    "    Args:\n",
    "        epoch_losses (list): A list containing the loss values for each epoch.\n",
    "    \"\"\"\n",
    "    # Create a figure with a predefined size for better readability\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Plot the loss values over epochs with markers for each epoch\n",
    "    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', label='Training Loss')\n",
    "    # Label the x-axis to indicate the epoch number\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    # Label the y-axis to indicate the loss value\n",
    "    plt.ylabel(\"Loss\")\n",
    "    # Add a title to the plot for better context\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    # Display a legend to clarify the plotted line\n",
    "    plt.legend()\n",
    "    # Add a grid to improve readability of the trend\n",
    "    plt.grid()\n",
    "    # Show the final plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the training regime\n",
    "\n",
    "This train() function is responsible for training our sequence-to-sequence model with an encoder-decoder architecture with 'Bahdanau attention'. It iterates through a dataset of tokenized math problems for multiple epochs, processing each batch by passing inputs through the encoder, which generates a context vector. The decoder then predicts output tokens step by step, using teacher forcing to guide learning while dynamically focusing on relevant parts of the input via attention mechanisms. The function tracks masked loss, ensuring that padding tokens don't affect optimization, and updates model weights using backpropagation with Adam optimizers. After training, it stores attention weights for visualization, logs epoch losses, plots them for monitoring, and saves the trained models to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, encoder_save_name, decoder_save_name):\n",
    "    \"\"\"\n",
    "    Trains the sequence-to-sequence model using an encoder-decoder architecture.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        encoder_save_name (str): File name for saving the trained encoder model.\n",
    "        decoder_save_name (str): File name for saving the trained decoder model.\n",
    "    \"\"\"\n",
    "    epoch_losses = []  # Stores loss values for tracking progress\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0  # Accumulate total loss for the epoch\n",
    "\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            # Move data to GPU if available\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            # Reset gradients before each batch\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # Encoder forward pass - processes input sequences to generate hidden states\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "            # Decoder initialization - starts with the encoder's final hidden state\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "            # Get actual sequence lengths (excluding padding)\n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()\n",
    "\n",
    "            loss = 0  # Tracks batch loss\n",
    "            \n",
    "            # Iterate through target sequence timesteps\n",
    "            for t in range(max_target_length):\n",
    "                # Determine active sequences\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():  # If all sequences finished, break loop\n",
    "                    break\n",
    "\n",
    "                # Decoder forward pass - generates output token probabilities\n",
    "                output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "\n",
    "                # Masked loss calculation - only consider active sequences\n",
    "                loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "\n",
    "                # Teacher forcing: Use actual target token as next input\n",
    "                decoder_input = target_seq[:, t]  \n",
    "\n",
    "            # Backpropagation - compute gradients and update model parameters\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Store normalized loss\n",
    "        epoch_losses.append(epoch_loss / len(dataloader))\n",
    "\n",
    "        # Print training progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_losses[-1]:.4f}\")\n",
    "\n",
    "        # Store attention weights for visualization\n",
    "        attention_matrix.append(attention_weights.cpu().detach().numpy())\n",
    "\n",
    "    # Plot training loss\n",
    "    plot_loss(epoch_losses)\n",
    "\n",
    "    # Save the trained models\n",
    "    torch.save(encoder.state_dict(), f\"{encoder_save_name}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"{decoder_save_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We'll start training for 100 epochs to refine our sequence-to-sequence model. This will help our encoder-decoder system learn better representations and improve predictions for solving math word problems. We'll expect to see loss decreasing over time, but if it stagnates or spikes, we can adjust learning rate, batch size, and/or other hyperparameters to stabilise training. Also, our ability to monitor attention weights will help us verify whether the model is correctly focusing on key parts of the input during decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"basic_math_problem_encoder\", \"basic_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate heatmap\n",
    "\n",
    "Function to allow us to visualise attention weights as a heatmap, helping us analyse which input tokens the decoder focuses on while generating each output token in a sequence-to-sequence model. Using Matplotlib, it plots the attention_matrix as a color-coded intensity map, where darker shades indicate stronger attention at specific positions. This visualization is helpful for allowing us to understand how our model prioritises different parts of the input, making it inciteful when attempting to optimise our attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention(input_tokens, output_tokens, attention_matrix):\n",
    "    \"\"\"\n",
    "    Plot the attention weights as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        input_tokens (list of str): Tokens in the input sequence.\n",
    "        output_tokens (list of str): Tokens in the output sequence.\n",
    "        attention_matrix (np.array): Attention weights matrix (output_tokens x input_tokens).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attention_matrix, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=np.arange(len(input_tokens)), labels=input_tokens, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks=np.arange(len(output_tokens)), labels=output_tokens)\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.xlabel(\"Input Tokens\")\n",
    "    plt.ylabel(\"Output Tokens\")\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup\n",
    "\n",
    "Function to allow us to test our trained sequence-to-sequence model by encoding our math question input sentence into hidden states and decoding it step-by-step to generate our expected answer. It tokenizes the input using a predefined vocabulary and passes it through the encoder, which extracts a contextual representation. The decoder then predicts each output token using an iterative approach, incorporating attention weights to dynamically focus on relevant parts of the input. The function tracks attention matrices, enabling visualization of how the model distributes focus across tokens. It prints the input sentence, generated output, and attention shape, and optionally plots a heatmap for deeper analysis. The final output is a list of predicted words, representing the answer to our math problem, which can then be evaluated for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=100, with_attention_plot=False):\n",
    "    # Set the models to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Forward pass through the encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "    # Initialize the decoder\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]], dtype=torch.long)  # Start-of-Sequence token\n",
    "    decoder_hidden = hidden\n",
    "    decoder_cell = cell\n",
    "\n",
    "    # Generate output sequence and collect attention weights\n",
    "    output_sequence = []\n",
    "    attention_matrices = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get the token with the highest probability\n",
    "\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE_TOKEN]:  # Stop at End-of-Sequence token\n",
    "            break\n",
    "\n",
    "        output_sequence.append(predicted_token)\n",
    "        attention_matrices.append(attention_weights.cpu().detach().numpy())  # Save attention weights\n",
    "\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)  # Next decoder input\n",
    "\n",
    "    # Convert input and output tokens to words\n",
    "    input_sentence_tokens = [index_to_word[token] for token in input_tokens]\n",
    "    output_sentence_tokens = [index_to_word[token] for token in output_sequence]\n",
    "\n",
    "    # Stack attention matrices into a 2D array (output_tokens x input_tokens)\n",
    "    attention_matrix = np.vstack(attention_matrices)\n",
    "\n",
    "    # Print the input and output sentences\n",
    "    print(\"Input Sentence:\", input_sentence_tokens)\n",
    "    print(\"Generated Sentence:\", output_sentence_tokens)\n",
    "    print(\"Attention Weights Shape:\", attention_matrix)\n",
    "\n",
    "    # Visualize attention\n",
    "    if with_attention_plot:\n",
    "        # Plot the attention weights\n",
    "        plot_attention(input_sentence_tokens, output_sentence_tokens, attention_matrix)\n",
    "\n",
    "    # Convert token indices to words\n",
    "    return [index_to_word[token] for token in output_sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model - basic approach\n",
    "\n",
    "Here we load our pre-trained sequence-to-sequence model for solving basic math word problems. We first initializes the encoder and decoder architectures with the same parameters used during training, ensuring consistency. Then, we load the pretrained model weights from the associated **.pth** files, restoring previously learned knowledge. After defining a sample input sentence, **\"two plus four\"**, we set up **word-to-index** and **index-to-word** mappings, allowing tokenisation and conversion between words and numerical indices. Finally, the test function is called, running the encoder-decoder model on the input question to generate a predicted answer while optionally visualizing attention weights so we can attempt analyse how the model focuses on different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 128  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"basic_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"basic_math_problem_decoder.pth\"))\n",
    "\n",
    "input_sentence = \"two plus four\"\n",
    "\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Test and visualize\n",
    "test(encoder, decoder, input_sentence, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a more complex dataset and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline vocabulary and tokenisation capability\n",
    "\n",
    "Here we set up a dynamic tokenisation function for our sequence-to-sequence model by defining a baseline vocabulary containing special tokens (<SOS>, <EOS>, <PAD>). The function processes an input sentence by splitting words, checking if they already exist in the **word_to_index** mapping, and assigning a new index to any unseen words. This ensures that the vocabulary expands dynamically as new words are encountered, allowing the model to handle previously unknown input without predefined restrictions, with the function ultimately generating a list of token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens for sequence processing - initially, the vocabulary only contains special tokens (<SOS>, <EOS>, <PAD>)\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2}  \n",
    "\n",
    "def tokenize(sentence, word_to_index):\n",
    "    \"\"\"\n",
    "    Tokenizes a sentence and dynamically updates the vocabulary mapping.\n",
    "    Args:\n",
    "        sentence (str): The input sentence to be tokenised.\n",
    "        word_to_index (dict): Dictionary mapping words to unique indices.\n",
    "    Returns:\n",
    "        list: A list of token indices representing the sentence.\n",
    "    \"\"\"\n",
    "    tokens = []  # Stores tokenized word indices\n",
    "\n",
    "    # Process each word in the sentence\n",
    "    for word in sentence.lower().split():\n",
    "        # Add unseen words to the vocabulary and assign a unique index\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)  \n",
    "        \n",
    "        # Append the tokenized index to the token list\n",
    "        tokens.append(word_to_index[word])\n",
    "\n",
    "    return tokens  # Return tokenized sentence as a list of indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV dataset from file\n",
    "\n",
    "Here we load a dataset of math questions and their associated answers from our previously generated CSV file, using Pandas. The function reads the CSV file into a DataFrame, extracts the \"Question\" column (math word question) and \"Answer\" column (corresponding answers), and returns them as lists. The script then specifies **\"simple_math_problems_addition_only.csv\"** as the dataset and calls the function to populate **input_sentences** and **target_sentences**, which can be tokenised and fed into the model for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_sequences_from_csv(csv_file):\n",
    "    \"\"\"\n",
    "    Loads math problem sequences from a CSV file.\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing math problems and solutions.\n",
    "    Returns:\n",
    "        tuple: Two lists containing problem statements and their corresponding solutions.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Extract the \"Problem\" column as a list of math problems\n",
    "    input_sentences = df[\"Problem\"].tolist()\n",
    "    # Extract the \"Solution\" column as a list of corresponding answers\n",
    "    target_sentences = df[\"Solution\"].tolist()\n",
    "    return input_sentences, target_sentences  # Return both lists for processing\n",
    "\n",
    "# Specify the dataset file name\n",
    "csv_file = \"simple_math_problems_addition_only.csv\"\n",
    "# Load problem and solution sequences from the CSV file\n",
    "input_sentences, target_sentences = load_sequences_from_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise input and target sentences and convert into tensors\n",
    "\n",
    "We now tokenise the question and answer sentences and convert them into PyTorch tensors, preparing them for processing in a sequence-to-sequence model. It first transforms input sentences into numerical token lists using a predefined vocabulary (**word_to_index**). For target sentences, thus ensuring a structured decoding by appending **<SOS>** at the beginning and **<SOS>** at the end, guiding the model in output generation. A reverse mapping (**index_to_word**) is also created, allowing numerical tokens to be converted back into words for interpretation. Finally, both input and target tokenised sequences are converted into PyTorch tensors, ensuring they are formatted correctly for our models training and inference requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input sentences by converting words to numerical indices\n",
    "input_data = [tokenize(sentence, word_to_index) for sentence in input_sentences]\n",
    "\n",
    "# Tokenize target sentences while adding <SOS> (start token) and <EOS> (end token)\n",
    "target_data = [[word_to_index[START_OF_SEQUENCE_TOKEN]] + \n",
    "               tokenize(sentence, word_to_index) + \n",
    "               [word_to_index[END_OF_SEQUENCE_TOKEN]] \n",
    "               for sentence in target_sentences]\n",
    "\n",
    "# Create a reverse mapping dictionary to convert token indices back into words\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "\n",
    "# Convert tokenized sentences into PyTorch tensors for model compatibility\n",
    "input_tensors = [torch.tensor(seq) for seq in input_data]  # Convert input sequences to tensors\n",
    "target_tensors = [torch.tensor(seq) for seq in target_data]  # Convert target sequences to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokensied data, including padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic padding to ensure uniform sequence length\n",
    "input_padded = pad_sequence(input_tensors, padding_value=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "target_padded = pad_sequence(target_tensors, padding_value=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "\n",
    "class DynamicMathWordProblemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for padded math word problems.\n",
    "    This class stores input and target sequences that have been padded \n",
    "    to a fixed length, ensuring consistency in batch processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_padded, target_padded):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with padded sequences.\n",
    "        Args:\n",
    "            input_padded (Tensor): Padded input sequences.\n",
    "            target_padded (Tensor): Padded target sequences.\n",
    "        \"\"\"\n",
    "        self.input_data = input_padded  # Store padded input sequences\n",
    "        self.target_data = target_padded  # Store padded target sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Returns:\n",
    "            int: Total number of input-target pairs.\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a padded input-target pair as tensors.\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: Padded input and target sequences.\n",
    "        \"\"\"\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "# Create a dataset instance using the padded sequences\n",
    "dataset = DynamicMathWordProblemDataset(input_padded, target_padded)\n",
    "# Initialize a DataLoader for batch processing\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)  # Batch size of 64, shuffle disabled for sequential processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters\n",
    "\n",
    "We now set-up the hyperparameters for trainining model. We first initialise the encoder and decoder models with a hidden size of **256**, then a cross-entropy loss function is defined with **ignore_index**, ensuring that our padding tokens do not affect training. Both models use the Adam optimizer with a learning rate of **0.0005** to adjust weights and improve performance. Finally, the we define training for 100 epochs, allowing saving of the learned model parameters under specified filenames for later inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model parameters\n",
    "input_size = len(word_to_index)  # Vocabulary size for the encoder\n",
    "output_size = len(word_to_index)  # Vocabulary size for the decoder (same as input)\n",
    "hidden_size = 256  # Number of hidden units in encoder and decoder\n",
    "\n",
    "# Initialize the encoder and move it to the selected device\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "\n",
    "# Initialize the decoder and move it to the selected device\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "# Define the loss function (CrossEntropyLoss), ignoring padding tokens during training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN]) \n",
    "\n",
    "# Set up Adam optimizers for both encoder and decoder with a learning rate of 0.0005\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0005)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0005)\n",
    "\n",
    "# Define the number of epochs for training\n",
    "num_epochs = 100  \n",
    "\n",
    "# Start the training process with specified epoch count and model save names\n",
    "train(num_epochs, \"addition_only_math_problem_encoder\", \"addition_only_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the revised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"addition_only_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"addition_only_math_problem_decoder.pth\"))\n",
    "\n",
    "#thirty eight plus twenty seven,sixty five\n",
    "\n",
    "input_sentence = \"thirty eight plus twenty six\" #sixty four\n",
    "\n",
    "# Test and visualize\n",
    "test_token_indices_to_words = test(encoder, decoder, input_sentence, word_to_index, index_to_word, with_attention_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the test predictions for evaluation\n",
    "\n",
    "We look to generate predictions using our trained sequence-to-sequence model and evaluate its performance using 'edit distance'. Edit distance is a measure of how different our sequences are from each other, calculated by the minimum number of operations required to transform one sequence into another. A lower edit distance indicates that the two sequences are highly similar, while a higher edit distance suggests greater differences. The **generate_predictions** function processes an input question, tokenizes it into indices, and runs it through the encoder to obtain hidden states, which are then passed to the decoder. The decoder iteratively predicts the answer tokens, stopping when the **<EOS>** token is encountered. The predicted answer tokens are converted back into words for interpretation. The **evaluate_with_edit_distance** function allows us to compare generated outputs with ground truth sentences by computing the edit distance, which quantifies the differences between predictions and expected answers. The final output is the average edit distance, helping us to assess model accuracy when solving math word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edit_distance\n",
    "\n",
    "def generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=10):\n",
    "    \"\"\"\n",
    "    Generates a predicted output sequence for a given input sentence using a trained encoder-decoder model.\n",
    "    Args:\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        input_sentence (str): The input sentence to be processed.\n",
    "        word_to_index (dict): Mapping of words to numerical token indices.\n",
    "        index_to_word (dict): Reverse mapping of indices to words.\n",
    "        max_target_length (int): Maximum length of the output sequence.\n",
    "    Returns:\n",
    "        list: A list of words representing the predicted output sequence.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "\n",
    "    # Tokenize the input sentence by converting words to numerical token indices\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "    # Forward pass through the encoder to obtain hidden and cell states\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "    # Initialize decoder with <SOS> token as the first input\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]], dtype=torch.long)\n",
    "    decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs to initialize decoder states\n",
    "    \n",
    "    # Generate output sequence iteratively\n",
    "    output_sequence = []\n",
    "    for _ in range(max_target_length):\n",
    "        # Pass current decoder input through the decoder\n",
    "        output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        # Get the token with the highest probability from the decoder output\n",
    "        predicted_token = output.argmax(1).item()\n",
    "        # Stop generation if <EOS> token is predicted\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE_TOKEN]:\n",
    "            break\n",
    "        # Append the predicted token to the output sequence\n",
    "        output_sequence.append(predicted_token)\n",
    "        # Use predicted token as the next decoder input\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert token indices back into words for interpretation\n",
    "    return [index_to_word[token] for token in output_sequence]\n",
    "\n",
    "\n",
    "def evaluate_with_edit_distance(test_data, encoder, decoder, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Evaluates the sequence-to-sequence model using edit distance.\n",
    "    Args:\n",
    "        test_data (list): List of (input_sentence, ground_truth_sentence) pairs.\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        word_to_index (dict): Word-to-index mapping.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "    Returns:\n",
    "        float: Average edit distance across the test dataset.\n",
    "    \"\"\"\n",
    "    total_distance = 0  # Initialize total edit distance accumulator\n",
    "    for input_sentence, ground_truth_sentence in test_data:\n",
    "        # Generate predicted sequence for the input sentence\n",
    "        predicted_sentence = generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word)\n",
    "        # Compute edit distance between ground truth and predicted sentence\n",
    "        distance = edit_distance.SequenceMatcher(a=ground_truth_sentence, b=predicted_sentence).distance()\n",
    "        total_distance += distance  # Accumulate total distance\n",
    "        # Debugging: Print example results for verification\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Ground Truth: {ground_truth_sentence}\")\n",
    "        print(f\"Predicted: {predicted_sentence}\")\n",
    "        print(f\"Edit Distance: {distance}\\n\")\n",
    "\n",
    "    # Compute average edit distance across the dataset\n",
    "    average_distance = total_distance / len(test_data)\n",
    "    print(f\"Average Edit Distance: {average_distance}\")\n",
    "\n",
    "    return average_distance  # Return evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using edit distance\n",
    "\n",
    "We'll setup a small test dataset consisting of math word problems and their expected answers, formatted as tokenized sequences with a start token (**<SOS**). We'll evaluate the trained model using 'edit distance' (as described above). Each test case follows a structured pattern where a simple math question is presented in text, and associated with the corresponding answer, again in text format. The **evaluate_with_edit_distance** is called to generate answer predictions and compare them against the ground truth answers. Finally, we calculate the average edit distance across all test cases, with the aim of assessing model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an example test dataset with math addition problems and expected solutions\n",
    "test_data = [\n",
    "    (\"thirty eight plus twenty six\", [START_OF_SEQUENCE_TOKEN, \"sixty\", \"four\"]),   # Expected output: \"sixty four\"\n",
    "    (\"thirty eight plus twenty five\", [START_OF_SEQUENCE_TOKEN, \"sixty\", \"three\"]), # Expected output: \"sixty three\"\n",
    "    (\"thirty eight plus twenty four\", [START_OF_SEQUENCE_TOKEN, \"sixty\", \"two\"]),   # Expected output: \"sixty two\"\n",
    "    (\"thirty eight plus twenty three\", [START_OF_SEQUENCE_TOKEN, \"sixty\", \"one\"]),  # Expected output: \"sixty one\"\n",
    "    (\"thirty eight plus twenty two\", [START_OF_SEQUENCE_TOKEN, \"sixty\"]),           # Expected output: \"sixty\"\n",
    "    (\"thirty eight plus twenty one\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"nine\"]),   # Expected output: \"fifty nine\"\n",
    "    (\"thirty eight plus twenty\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"eight\"]),      # Expected output: \"fifty eight\"\n",
    "    (\"thirty eight plus nineteen\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"seven\"]),    # Expected output: \"fifty seven\"\n",
    "    (\"thirty eight plus eighteen\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"six\"]),      # Expected output: \"fifty six\"\n",
    "    (\"thirty eight plus seventeen\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"five\"]),    # Expected output: \"fifty five\"\n",
    "]\n",
    "\n",
    "# Evaluate the trained sequence-to-sequence model using edit distance\n",
    "average_distance = evaluate_with_edit_distance(\n",
    "    test_data, encoder, decoder, word_to_index, index_to_word\n",
    ")\n",
    "\n",
    "# Print the final average edit distance to assess model performance\n",
    "print(f\"Final Average Edit Distance: {average_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add positional encodings to our encode model\n",
    "\n",
    "We look to further extend our original encoder model with the addition of a positional encoding layer to inject sequential information. Positional encodings allow us to enforce built-in order awareness, ensuring that questions and answers maintain their relative positioning within a given sequence.\n",
    "Thes encodings are implemented using learned embeddings, which generate continuous patterns that help the model differentiate token positions. The intention here is to ensure the model can effectively understand and retain the order of words, improving context understanding, translation accuracy, and sentence structure in an attempt to improve training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM-based encoder with positional encodings for sequence-to-sequence tasks.\n",
    "    This model encodes input sequences using embeddings, positional encodings, \n",
    "    and a stacked LSTM, returning hidden and cell states for use by a decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1, max_seq_len=100):\n",
    "        \"\"\"\n",
    "        Initializes the encoder model.\n",
    "        Args:\n",
    "            input_size (int): Vocabulary size (number of unique tokens).\n",
    "            hidden_size (int): Dimensionality of hidden states in the LSTM.\n",
    "            dropout (float): Dropout rate for regularization.\n",
    "            max_seq_len (int): Maximum sequence length for positional encodings.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer to convert token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Positional encodings to retain word order information\n",
    "        self.positional_encodings = nn.Embedding(max_seq_len, hidden_size)\n",
    "        # LSTM layer for sequential processing with batch support\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Dropout for regularization to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing input token indices with shape (batch_size, seq_length).\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tuple[Tensor, Tensor]]: LSTM outputs and final hidden & cell states.\n",
    "        \"\"\"\n",
    "        # Generate positional indices for the input sequence\n",
    "        positions = torch.arange(0, input_seq.size(1), device=input_seq.device).unsqueeze(0)\n",
    "        # Retrieve positional encodings for each token position\n",
    "        positional_enc = self.positional_encodings(positions)\n",
    "        # Compute embeddings, add positional encodings, and apply dropout\n",
    "        embedded = self.dropout(self.embedding(input_seq) + positional_enc)\n",
    "        # Process embedded input sequence through the LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return outputs, (hidden, cell)  # Return LSTM outputs and final hidden/cell states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters using Opuna\n",
    "\n",
    "This code utilizes Optuna, a powerful hyperparameter optimization framework, to fine-tune an encoder-decoder model for sequence-to-sequence tasks. The **objective** function defines the hyperparameters to be optimized, including hidden size, learning rate, and dropout rate, which are sampled across a predefined range. It then initializes the encoder and decoder models with these parameters, applies dropout regularization, and sets up cross-entropy loss while ignoring padding tokens. The models are trained for one epoch with teacher forcing, where the actual target sequence is used as input to the decoder, helping accelerate learning. The script runs 30 trials, minimizing loss to find the best-performing hyperparameter combination, which is then printed for further experimentation. We aslo provide visualisations of the hyperparameter tuning process, which should help to identify trends in model performance based on different parameter values, with plots including an optimization history graph that tracks how loss decreases across trials, a hyperparameter importance chart highlighting which parameters most affect performance, and a loss vs. hyperparameter relationships plot to visualize individual parameter effects. The final visualization offers insights into how different hidden sizes, learning rates, and dropout rates impact model accuracy, enabling informed decisions on optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "# Objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines the optimization objective for hyperparameter tuning using Optuna.\n",
    "    Args:\n",
    "        trial (optuna.Trial): A single optimization trial where hyperparameters are sampled.\n",
    "    Returns:\n",
    "        float: The averaged loss computed over one epoch, which Optuna minimizes.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters for tuning\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 128, 512)  # Hidden layer size\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)  # Log-scaled learning rate\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate for regularization\n",
    "    # Initialize encoder-decoder models with suggested hyperparameters\n",
    "    encoder = Encoder(input_size, hidden_size).to(device)\n",
    "    decoder = Decoder(output_size, hidden_size).to(device)\n",
    "    # Apply dropout in LSTM layers for both models\n",
    "    encoder.lstm.dropout = dropout_rate\n",
    "    decoder.lstm.dropout = dropout_rate\n",
    "    # Define loss function, ignoring padding tokens in label sequences\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "    # Set up Adam optimizers with the suggested learning rate\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    # Initialize total loss for tracking performance\n",
    "    total_loss = 0\n",
    "\n",
    "    # Run one epoch of training to evaluate hyperparameter performance\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        # Move data to the selected device (CPU/GPU)\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        # Reset gradients before forward pass\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        # Forward pass through encoder to obtain initial hidden states\n",
    "        encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "        # Initialize decoder input with <SOS> token for each sequence in batch\n",
    "        decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "        decoder_hidden, decoder_cell = hidden, cell  # Set decoder states from encoder outputs\n",
    "\n",
    "        loss = 0  # Track cumulative loss across target sequence\n",
    "\n",
    "        for t in range(target_seq.size(1)):  # Iterate over target sequence length\n",
    "            # Forward pass through decoder\n",
    "            output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "            )\n",
    "\n",
    "            # Compute loss for current timestep\n",
    "            loss += criterion(output, target_seq[:, t])\n",
    "            # Apply teacher forcing: use ground-truth target as next input\n",
    "            decoder_input = target_seq[:, t]\n",
    "\n",
    "        # Backpropagate loss and update optimizer\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "    return total_loss / len(dataloader)  # Return average loss per batch for Optuna optimization\n",
    "\n",
    "# Create Optuna study for hyperparameter tuning (minimizing loss)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)  # Run 30 trials to find the best parameters\n",
    "# Extract best-performing hyperparameters from optimization results\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "# Store best hyperparameter values for model training\n",
    "best_hidden_size = best_params[\"hidden_size\"]\n",
    "best_learning_rate = best_params[\"learning_rate\"]\n",
    "best_dropout_rate = best_params[\"dropout_rate\"]\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot optimization history (loss over trials)\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "# Plot relationship between hyperparameters\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "# Additional plots (Loss vs. specific hyperparameters)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "optuna.visualization.matplotlib.plot_slice(study).set_title(\"Loss vs. Hyperparameters\")\n",
    "plt.show()  # Display plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with k-fold\n",
    "\n",
    "We look to further improvements by implementing **k-fold cross-validation** for training our model.\n",
    "\n",
    "K-fold cross-validation is a technique used to evaluate a model’s generalization ability by splitting the dataset into multiple equal-sized folds (typically 5, as in our case). The model is trained on (k-1) folds while the remaining 1 fold is used for validation, ensuring every data point is tested at least once. This process repeats (k) times, each time using a different fold as the validation set. The final performance metric is the average over all iterations, making this method highly effective in reducing bias and variance compared to simple train-test splits. It helps us assess how well our model performs on unseen data, ensuring reliable real-world predictions.\n",
    "\n",
    "To achieve this, we split our dataset into multiple folds (default: 5), ensuring each portion is used for both training and validation, which helps to assess model generalization. During each fold, the encoder and decoder weights are reset, trained for multiple epochs using teacher forcing, and optimized with Adam optimizers. After training, the validation step computes loss while ignoring padded tokens, ensuring evaluation accuracy. The trained models are then saved per fold for further analysis. Finally, the **evaluate_model** function calculates validation loss, ensuring we have some means of performance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def train_with_k_fold(num_epochs, encoder_save_name, decoder_save_name, batch_size, weight_init, learning_rate, k_folds=5):\n",
    "    \"\"\"\n",
    "    Implements k-fold cross-validation for training a sequence-to-sequence model.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of epochs for training each fold.\n",
    "        encoder_save_name (str): Base name for saving the trained encoder model.\n",
    "        decoder_save_name (str): Base name for saving the trained decoder model.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        weight_init (function): Function to initialize model weights.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        k_folds (int): Number of folds for cross-validation.\n",
    "    \"\"\"\n",
    "    # Create a KFold object with shuffled data for better generalization\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    fold_index = 1  # Track the current fold number\n",
    "    \n",
    "    # Loop over each fold, splitting dataset into training and validation sets\n",
    "    for train_indices, val_indices in kfold.split(dataset):  # Ensure `dataset` is preloaded\n",
    "        # Create subsets for training and validation based on fold indices\n",
    "        train_data = Subset(dataset, train_indices)\n",
    "        val_data = Subset(dataset, val_indices)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print(f\"Training fold {fold_index}/{k_folds}...\")\n",
    "        # Reset encoder and decoder model weights for each fold\n",
    "        encoder.apply(weight_init)  # Reset encoder weights\n",
    "        decoder.apply(weight_init)  # Reset decoder weights\n",
    "        # Initialize Adam optimizers with the given learning rate\n",
    "        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model for the specified number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0  # Track loss for the epoch\n",
    "        \n",
    "            for input_seq, target_seq in train_loader:\n",
    "                # Move data to the selected device (GPU or CPU)\n",
    "                input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "                # Reset gradients before forward pass\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "                # Forward pass through the encoder to generate hidden states\n",
    "                encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "                # Initialize decoder input with <SOS> token for batch processing\n",
    "                decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "                decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs to initialize decoder states\n",
    "                # Compute target sequence lengths (excluding padding tokens)\n",
    "                target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "                max_target_length = target_lengths.max().item()\n",
    "\n",
    "                loss = 0  # Track cumulative loss per batch\n",
    "                \n",
    "                # Iterate through the target sequence length\n",
    "                for t in range(max_target_length):\n",
    "                    still_active = t < target_lengths  # Ensure valid sequence length\n",
    "                    if not still_active.any():\n",
    "                        break\n",
    "                    # Forward pass through decoder\n",
    "                    output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                        decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                    )\n",
    "                    # Compute loss while ignoring padded sequences\n",
    "                    loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "                    # Apply teacher forcing: use actual target sequence as next input\n",
    "                    decoder_input = target_seq[:, t]\n",
    "\n",
    "                # Backpropagate and update model parameters\n",
    "                loss.backward()\n",
    "                encoder_optimizer.step()\n",
    "                decoder_optimizer.step()\n",
    "                epoch_loss += loss.item()  # Accumulate loss over the epoch\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Evaluate model performance on validation set\n",
    "        val_loss = evaluate_model(val_loader)  # Call validation function\n",
    "        print(f\"Validation Loss for fold {fold_index}: {val_loss:.4f}\")\n",
    "\n",
    "        # Save trained model for the current fold\n",
    "        torch.save(encoder.state_dict(), f\"{encoder_save_name}_fold{fold_index}.pth\")\n",
    "        torch.save(decoder.state_dict(), f\"{decoder_save_name}_fold{fold_index}.pth\")\n",
    "\n",
    "        fold_index += 1  # Move to the next fold\n",
    "\n",
    "    print(\"K-fold cross-validation complete!\")\n",
    "\n",
    "def evaluate_model(dataloader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset and computes loss.\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for validation samples.\n",
    "    Returns:\n",
    "        float: Average validation loss over all batches.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "\n",
    "    val_loss = 0  # Track validation loss\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            # Generate encoder outputs\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "            # Initialize decoder input with <SOS> token\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs for decoder initialization\n",
    "            # Compute target sequence lengths\n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()\n",
    "\n",
    "            for t in range(max_target_length):\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():\n",
    "                    break\n",
    "\n",
    "                # Forward pass through decoder\n",
    "                output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "                # Compute validation loss while ignoring padded sequences\n",
    "                val_loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum().item() / still_active.sum().item()\n",
    "\n",
    "                decoder_input = target_seq[:, t]  # Teacher forcing\n",
    "\n",
    "    return val_loss / len(dataloader)  # Return average validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement hyperparameter changes and re-test\n",
    "\n",
    "Finally, we initialise and train our revised model using hyperparameter-tuned values and k-fold cross-validation to enhance generalization. It first sets-up the encoder and decoder models using the best hyperparameters obtained from tuning (**hidden_size**, **dropout_rate**, and **learning_rate**), and applies cross-entropy loss with ignored padding tokens to prevent irrelevant data from influencing training. The Adam optimizer refines both models, ensuring efficient weight adjustments. We then invoke **k-fold cross-validation** (5 folds), systematically splitting the dataset into training and validation subsets, enabling performance tracking across different data partitions. Each fold resets model weights, trains for 100 epochs, evaluates validation performance, and saves the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model parameters using best hyperparameters from tuning\n",
    "input_size = len(word_to_index)  # Vocabulary size (total unique tokens)\n",
    "output_size = len(word_to_index)  # Decoder vocabulary size (same as input)\n",
    "hidden_size = best_hidden_size  # Optimized hidden layer size from tuning\n",
    "dropout_rate = best_dropout_rate  # Optimized dropout rate from tuning\n",
    "\n",
    "# Initialize encoder and decoder with tuned parameters, moving them to the selected device\n",
    "encoder = Encoder(input_size, hidden_size, dropout_rate).to(device)\n",
    "decoder = Decoder(output_size, hidden_size, dropout_rate).to(device)\n",
    "\n",
    "# Define loss function, ignoring padding tokens to avoid unnecessary error influence\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN]) \n",
    "\n",
    "# Set up Adam optimizers using the best learning rate obtained from hyperparameter tuning\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=best_learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Define the number of training epochs\n",
    "num_epochs = 100  \n",
    "\n",
    "# Train using k-fold cross-validation (ensures better generalization)\n",
    "train_with_k_fold(\n",
    "    num_epochs, \n",
    "    \"addition_only_math_problem_encoder\",  # Base filename for saving encoder model\n",
    "    \"addition_only_math_problem_decoder\",  # Base filename for saving decoder model\n",
    "    batch_size=32,  # Mini-batch size for training\n",
    "    weight_init=0.01,  # Weight initialization parameter\n",
    "    learning_rate=best_learning_rate,  # Optimized learning rate from tuning\n",
    "    k_folds=5  # Number of folds for cross-validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, max_target_len):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a trained sequence-to-sequence model using BLEU score.\n",
    "    Args:\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        dataloader: DataLoader containing test data.\n",
    "        max_target_len (int): Maximum length of the output sequence.\n",
    "    Returns:\n",
    "        float: BLEU score representing translation accuracy.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode (disable dropout)\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "\n",
    "    references = []  # List to store ground truth sequences\n",
    "    candidates = []  # List to store predicted sequences\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq = input_seq.to(device)  # Move input sequence to the appropriate device\n",
    "            # Forward pass through the encoder to obtain hidden states\n",
    "            _, (hidden, cell) = encoder(input_seq)\n",
    "            # Initialize decoder input with <SOS> token for each sequence in the batch\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs to initialize decoder states\n",
    "\n",
    "            predictions = []  # List to store predicted tokens for each timestep\n",
    "            for t in range(max_target_len):\n",
    "                # Forward pass through the decoder\n",
    "                output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                # Select the token with the highest probability\n",
    "                top_token = output.argmax(1)  # Get index of the most probable token\n",
    "                predictions.append(top_token)  # Append predicted token to sequence\n",
    "                # Stop decoding if <EOS> token is generated\n",
    "                if top_token.item() == word_to_index[END_OF_SEQUENCE_TOKEN]:\n",
    "                    break\n",
    "                # Use predicted token as next input to the decoder (auto-regressive decoding)\n",
    "                decoder_input = top_token\n",
    "\n",
    "            # Convert predicted and target sequences from indices to words\n",
    "            predicted_seq = [index_to_word[token.item()] for token in predictions]\n",
    "            target_seq_words = [[index_to_word[token.item()] for token in target_seq[0]]]  # Wrap target in a nested list for BLEU\n",
    "            # Append sequences to BLEU evaluation lists\n",
    "            references.append(target_seq_words)  # Store ground-truth sequence\n",
    "            candidates.append(predicted_seq)  # Store model-predicted sequence\n",
    "\n",
    "    # Apply smoothing and compute corpus BLEU score\n",
    "    smooth = SmoothingFunction().method1  # Helps with short sequences\n",
    "    bleu_score = corpus_bleu(references, candidates, smoothing_function=smooth)\n",
    "    # Print BLEU score to evaluate translation quality\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "    return bleu_score  # Return computed BLEU score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## TODO: we need to implement this function to evaluate the model using BLEU score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
