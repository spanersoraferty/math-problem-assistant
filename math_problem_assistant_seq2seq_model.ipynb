{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an LSTM. \n",
    "    Converts a sequence of token indices into a hidden representation \n",
    "    that will be used by the decoder for sequence generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder module.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer converts token indices into dense vectors for better representation.\n",
    "        # The embeddings refer to vector representations of words or tokens used as input to the model.\n",
    "        # Instead of feeding raw word indices (which don’t capture meaning), feedingembeddings allows\n",
    "        # the LSTM to learn meaningful semantic relationships between words.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # LSTM layer processes embedded input sequences to generate hidden states\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing token indices for a batch of input sentences.\n",
    "\n",
    "        Returns:\n",
    "            outputs (Tensor): Encoder outputs at each time step.\n",
    "            hidden (Tensor): Final hidden state of the LSTM.\n",
    "            cell (Tensor): Final cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_seq)            # Convert input tokens into embeddings \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)   # Process embeddings through LSTM\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Attention model for the sequence-to-sequence math problem assistant\n",
    "    which implements a 'Bahdanau attention' mechanism. This allows the model\n",
    "    to dynamically compute attention scores based on the decoder's hidden\n",
    "    state and the encoder's outputs, allowing the model to focus on relevant\n",
    "    parts of the input sequence at each decoding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size):\n",
    "        \"\"\"\n",
    "        Initializes the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden state of the LSTM.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        # Learnable linear transformation to compute alignment scores\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # Learnable parameter to compute weighted attention scores\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Computes attention weights using Bahdanau's additive attention method.\n",
    "\n",
    "        Args:\n",
    "            hidden (Tensor): Decoder hidden state at the current time step.\n",
    "            encoder_outputs (Tensor): Encoder outputs at all time steps.\n",
    "\n",
    "        Returns:\n",
    "            attention_weights (Tensor): Softmax-normalized attention scores.\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        # Expand the hidden state across sequence length to match encoder outputs\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # Compute energy scores (alignment) using a feed-forward layer\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # Transpose energy tensor for matrix multiplication with attention parameter\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        # Expand attention parameter across batch size\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        # Compute attention weights using learned vector `v`\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        # Apply softmax to normalize scores across sequence length\n",
    "        return torch.softmax(attention_weights, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an\n",
    "    LSTM with Bahdanau attention. The decoder generates output tokens one by one\n",
    "    while dynamically focusing  on relevant parts of the encoder’s outputs using\n",
    "    the attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        # Embedding layer converts token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # LSTM layer processes embeddings and maintains hidden state across timesteps\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Linear layer maps concatenated attention context & LSTM output to vocab space\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        # Attention mechanism for dynamic focus on encoder outputs\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Current token input to the decoder.\n",
    "            hidden (Tensor): Previous hidden state from the LSTM.\n",
    "            cell (Tensor): Previous cell state from the LSTM.\n",
    "            encoder_outputs (Tensor): Encoder outputs from all timesteps.\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): Predicted token probabilities.\n",
    "            hidden (Tensor): Updated hidden state.\n",
    "            cell (Tensor): Updated cell state.\n",
    "            attention_weights (Tensor): Attention scores for each encoder timestep.\n",
    "        \"\"\"\n",
    "        # Expand input dimensions to match expected input shape for embedding\n",
    "        input = input.unsqueeze(1)  \n",
    "        # Convert token indices into dense embeddings\n",
    "        embedded = self.embedding(input)\n",
    "        # Forward pass through LSTM to generate new hidden and cell states\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # Compute attention weights using the current hidden state and encoder outputs\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        # Apply attention: generate weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # Flatten tensors for the fully connected layer\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        # Generate token probabilities using concatenated LSTM output and attention context\n",
    "        output = self.fc(torch.cat((lstm_output, context), dim=1))\n",
    "\n",
    "        return output, hidden, cell, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic Tokenization and Vocabulary Setup\n",
    "First, create a vocabulary and tokenize the input sentence (e.g., \"two plus four\" etc). To keep things extremely simple we'll manually establish a 'word-to-index' vocabulary whihc offers a simple map from word to tokensized integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and vocab setup\n",
    "# Create a vocabulary mapping words to indices\n",
    "word_to_index = {\"<SOS>\": 0, \"<EOS>\": 1, \"<PAD>\": 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Example input and target sequences\n",
    "input_sentence = \"two plus four\"\n",
    "target_sentence = \"equals six\"\n",
    "\n",
    "# Test Tokenized input and targets\n",
    "input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "target_tokens = [word_to_index[\"<SOS>\"]] + [word_to_index[word] for word in target_sentence.split()] + [word_to_index[\"<EOS>\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMathWordProblemDataset(Dataset):\n",
    "    def __init__(self, input_sentences, target_sentences, word_to_index):\n",
    "        self.input_data = [[word_to_index[word] for word in sentence.split()] for sentence in input_sentences]\n",
    "        self.target_data = [[word_to_index[\"<SOS>\"]] +\n",
    "                            [word_to_index[word] for word in sentence.split()] +\n",
    "                            [word_to_index[\"<EOS>\"]] for sentence in target_sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.input_data[idx], dtype=torch.long), torch.tensor(self.target_data[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "input_sentences = [\"two plus four\"]\n",
    "target_sentences = [\"equals six\"]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BasicMathWordProblemDataset(input_sentences, target_sentences, word_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "input_size = len(word_to_index)  # Total vocabulary size\n",
    "output_size = len(word_to_index)  # Vocabulary size\n",
    "hidden_size = 128\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<PAD>\"])  # Ignore padding tokens\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "attention_matrix = []  # Store attention weights for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the training regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, encoder_save_name, decoder_save_name):\n",
    "    \"\"\"\n",
    "    Trains the sequence-to-sequence model using an encoder-decoder architecture.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        encoder_save_name (str): File name for saving the trained encoder model.\n",
    "        decoder_save_name (str): File name for saving the trained decoder model.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            # Move data to GPU if available, else keep on CPU\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            # Reset gradients before each batch\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # **ENCODER FORWARD PASS**\n",
    "            # Processes the input sequence and generates context for the decoder\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "            # **DECODER INITIALIZATION**\n",
    "            # The first input to the decoder is always the <SOS> token\n",
    "            decoder_input = torch.tensor([word_to_index[\"<SOS>\"]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "            # Compute actual target sequence lengths (excluding padding)\n",
    "            target_lengths = (target_seq != word_to_index[\"<PAD>\"]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()  # Maximum sequence length in batch\n",
    "\n",
    "            loss = 0  # Track loss per batch\n",
    "            \n",
    "            # **ITERATE OVER TARGET SEQUENCE (Adaptive length)**\n",
    "            for t in range(max_target_length):\n",
    "                # Check if sequences are still active (haven't reached <EOS>)\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():  # If all sequences are finished, stop decoding\n",
    "                    break\n",
    "\n",
    "                # **DECODER FORWARD PASS**\n",
    "                output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "\n",
    "                # Compute masked loss (only valid tokens contribute to loss)\n",
    "                loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "\n",
    "                # Apply teacher forcing: Use actual target token as next input\n",
    "                decoder_input = target_seq[:, t]  \n",
    "\n",
    "            # **BACKPROPAGATION & OPTIMIZATION**\n",
    "            loss.backward()  # Compute gradients\n",
    "            encoder_optimizer.step()  # Update encoder weights\n",
    "            decoder_optimizer.step()  # Update decoder weights\n",
    "\n",
    "        # Print epoch loss for tracking progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "        attention_matrix.append(attention_weights.cpu().detach().numpy())  # Convert to NumPy for plotting\n",
    "    \n",
    "    # **SAVE TRAINED MODELS**\n",
    "    torch.save(encoder.state_dict(), f\"{encoder_save_name}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"{decoder_save_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 9.421306610107422\n",
      "Epoch 2/100, Loss: 8.789353370666504\n",
      "Epoch 3/100, Loss: 8.177886962890625\n",
      "Epoch 4/100, Loss: 7.560642242431641\n",
      "Epoch 5/100, Loss: 6.940435409545898\n",
      "Epoch 6/100, Loss: 6.323278427124023\n",
      "Epoch 7/100, Loss: 5.718662261962891\n",
      "Epoch 8/100, Loss: 5.139210224151611\n",
      "Epoch 9/100, Loss: 4.596834182739258\n",
      "Epoch 10/100, Loss: 4.099666118621826\n",
      "Epoch 11/100, Loss: 3.651379108428955\n",
      "Epoch 12/100, Loss: 3.2523274421691895\n",
      "Epoch 13/100, Loss: 2.900872230529785\n",
      "Epoch 14/100, Loss: 2.593749523162842\n",
      "Epoch 15/100, Loss: 2.3259994983673096\n",
      "Epoch 16/100, Loss: 2.091553211212158\n",
      "Epoch 17/100, Loss: 1.8844846487045288\n",
      "Epoch 18/100, Loss: 1.7002047300338745\n",
      "Epoch 19/100, Loss: 1.535854458808899\n",
      "Epoch 20/100, Loss: 1.3896598815917969\n",
      "Epoch 21/100, Loss: 1.2597596645355225\n",
      "Epoch 22/100, Loss: 1.143677830696106\n",
      "Epoch 23/100, Loss: 1.0390279293060303\n",
      "Epoch 24/100, Loss: 0.9443627595901489\n",
      "Epoch 25/100, Loss: 0.8590519428253174\n",
      "Epoch 26/100, Loss: 0.7824059724807739\n",
      "Epoch 27/100, Loss: 0.7131574749946594\n",
      "Epoch 28/100, Loss: 0.6500041484832764\n",
      "Epoch 29/100, Loss: 0.5924587249755859\n",
      "Epoch 30/100, Loss: 0.5405193567276001\n",
      "Epoch 31/100, Loss: 0.4939231276512146\n",
      "Epoch 32/100, Loss: 0.4519956409931183\n",
      "Epoch 33/100, Loss: 0.41398242115974426\n",
      "Epoch 34/100, Loss: 0.37935152649879456\n",
      "Epoch 35/100, Loss: 0.34782952070236206\n",
      "Epoch 36/100, Loss: 0.3192681074142456\n",
      "Epoch 37/100, Loss: 0.29350152611732483\n",
      "Epoch 38/100, Loss: 0.2702966034412384\n",
      "Epoch 39/100, Loss: 0.24939356744289398\n",
      "Epoch 40/100, Loss: 0.23055410385131836\n",
      "Epoch 41/100, Loss: 0.21358054876327515\n",
      "Epoch 42/100, Loss: 0.19829292595386505\n",
      "Epoch 43/100, Loss: 0.18451562523841858\n",
      "Epoch 44/100, Loss: 0.1720706969499588\n",
      "Epoch 45/100, Loss: 0.1607944667339325\n",
      "Epoch 46/100, Loss: 0.15054842829704285\n",
      "Epoch 47/100, Loss: 0.14122401177883148\n",
      "Epoch 48/100, Loss: 0.13273832201957703\n",
      "Epoch 49/100, Loss: 0.12501837313175201\n",
      "Epoch 50/100, Loss: 0.11799685657024384\n",
      "Epoch 51/100, Loss: 0.11160608381032944\n",
      "Epoch 52/100, Loss: 0.10577908158302307\n",
      "Epoch 53/100, Loss: 0.10045357793569565\n",
      "Epoch 54/100, Loss: 0.0955725759267807\n",
      "Epoch 55/100, Loss: 0.09108595550060272\n",
      "Epoch 56/100, Loss: 0.08695167303085327\n",
      "Epoch 57/100, Loss: 0.08313387632369995\n",
      "Epoch 58/100, Loss: 0.0796007439494133\n",
      "Epoch 59/100, Loss: 0.07632552087306976\n",
      "Epoch 60/100, Loss: 0.0732835978269577\n",
      "Epoch 61/100, Loss: 0.07045390456914902\n",
      "Epoch 62/100, Loss: 0.06781584769487381\n",
      "Epoch 63/100, Loss: 0.06535271555185318\n",
      "Epoch 64/100, Loss: 0.06304784119129181\n",
      "Epoch 65/100, Loss: 0.060887787491083145\n",
      "Epoch 66/100, Loss: 0.05885937809944153\n",
      "Epoch 67/100, Loss: 0.05695188045501709\n",
      "Epoch 68/100, Loss: 0.05515506863594055\n",
      "Epoch 69/100, Loss: 0.053460583090782166\n",
      "Epoch 70/100, Loss: 0.051859088242053986\n",
      "Epoch 71/100, Loss: 0.05034413933753967\n",
      "Epoch 72/100, Loss: 0.04890858381986618\n",
      "Epoch 73/100, Loss: 0.04754665121436119\n",
      "Epoch 74/100, Loss: 0.0462527871131897\n",
      "Epoch 75/100, Loss: 0.04502213001251221\n",
      "Epoch 76/100, Loss: 0.04384944960474968\n",
      "Epoch 77/100, Loss: 0.042731981724500656\n",
      "Epoch 78/100, Loss: 0.041665177792310715\n",
      "Epoch 79/100, Loss: 0.04064696282148361\n",
      "Epoch 80/100, Loss: 0.03967384621500969\n",
      "Epoch 81/100, Loss: 0.038744211196899414\n",
      "Epoch 82/100, Loss: 0.03785525634884834\n",
      "Epoch 83/100, Loss: 0.037005357444286346\n",
      "Epoch 84/100, Loss: 0.03619242459535599\n",
      "Epoch 85/100, Loss: 0.03541482239961624\n",
      "Epoch 86/100, Loss: 0.03467022255063057\n",
      "Epoch 87/100, Loss: 0.03395722061395645\n",
      "Epoch 88/100, Loss: 0.03327324241399765\n",
      "Epoch 89/100, Loss: 0.032616764307022095\n",
      "Epoch 90/100, Loss: 0.031986869871616364\n",
      "Epoch 91/100, Loss: 0.031380727887153625\n",
      "Epoch 92/100, Loss: 0.030797407031059265\n",
      "Epoch 93/100, Loss: 0.03023550659418106\n",
      "Epoch 94/100, Loss: 0.02969372645020485\n",
      "Epoch 95/100, Loss: 0.02917102538049221\n",
      "Epoch 96/100, Loss: 0.028666099533438683\n",
      "Epoch 97/100, Loss: 0.028177309781312943\n",
      "Epoch 98/100, Loss: 0.02770477719604969\n",
      "Epoch 99/100, Loss: 0.027246853336691856\n",
      "Epoch 100/100, Loss: 0.026803545653820038\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"basic_math_problem_encoder\", \"basic_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention(input_tokens, output_tokens, attention_matrix):\n",
    "    \"\"\"\n",
    "    Plot the attention weights as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        input_tokens (list of str): Tokens in the input sequence.\n",
    "        output_tokens (list of str): Tokens in the output sequence.\n",
    "        attention_matrix (np.array): Attention weights matrix (output_tokens x input_tokens).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attention_matrix, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=np.arange(len(input_tokens)), labels=input_tokens, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks=np.arange(len(output_tokens)), labels=output_tokens)\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.xlabel(\"Input Tokens\")\n",
    "    plt.ylabel(\"Output Tokens\")\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=100, with_attention_plot=False):\n",
    "    # Set the models to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Forward pass through the encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "    # Initialize the decoder\n",
    "    decoder_input = torch.tensor([word_to_index[\"<SOS>\"]], dtype=torch.long)  # Start-of-Sequence token\n",
    "    decoder_hidden = hidden\n",
    "    decoder_cell = cell\n",
    "\n",
    "    # Generate output sequence and collect attention weights\n",
    "    output_sequence = []\n",
    "    attention_matrices = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get the token with the highest probability\n",
    "\n",
    "        if predicted_token == word_to_index[\"<EOS>\"]:  # Stop at End-of-Sequence token\n",
    "            break\n",
    "\n",
    "        output_sequence.append(predicted_token)\n",
    "        attention_matrices.append(attention_weights.cpu().detach().numpy())  # Save attention weights\n",
    "\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)  # Next decoder input\n",
    "\n",
    "    # Convert input and output tokens to words\n",
    "    input_sentence_tokens = [index_to_word[token] for token in input_tokens]\n",
    "    output_sentence_tokens = [index_to_word[token] for token in output_sequence]\n",
    "\n",
    "    # Stack attention matrices into a 2D array (output_tokens x input_tokens)\n",
    "    attention_matrix = np.vstack(attention_matrices)\n",
    "\n",
    "    # Print the input and output sentences\n",
    "    print(\"Input Sentence:\", input_sentence_tokens)\n",
    "    print(\"Generated Sentence:\", output_sentence_tokens)\n",
    "    print(\"Attention Weights Shape:\", attention_matrix)\n",
    "\n",
    "    # Visualize attention\n",
    "    if with_attention_plot:\n",
    "        # Plot the attention weights\n",
    "        plot_attention(input_sentence_tokens, output_sentence_tokens, attention_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ['one', 'plus', 'two']\n",
      "Generated Sentence: ['<SOS>', 'equals', 'six']\n",
      "Attention Weights Shape: [[0.16776235 0.6371486  0.19508912]\n",
      " [0.19927078 0.6194938  0.1812354 ]\n",
      " [0.15201887 0.6196428  0.22833826]]\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 128  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"basic_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"basic_math_problem_decoder.pth\"))\n",
    "\n",
    "input_sentence = \"one plus two\"\n",
    "\n",
    "word_to_index = {\"<SOS>\": 0, \"<EOS>\": 1, \"<PAD>\": 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Test and visualize\n",
    "test(encoder, decoder, input_sentence, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a more complex dataset and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline vocabulary and tokenization capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "word_to_index = {\"<SOS>\": 0, \"<EOS>\": 1, \"<PAD>\": 2}  # establish only the baseline tokens this time\n",
    " \n",
    "# Function to tokenize a sentence and update mapping dynamically\n",
    "def tokenize(sentence, word_to_index):\n",
    "    tokens = []\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)  # Assign index to new words\n",
    "        tokens.append(word_to_index[word])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "def load_sequences_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df[\"Problem\"].tolist(), df[\"Solution\"].tolist()\n",
    "\n",
    "csv_file = \"simple_math_problems_addition_only.csv\"\n",
    "input_sentences, target_sentences = load_sequences_from_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize input and target sentences and convert into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and target sentences\n",
    "input_data = [tokenize(sentence, word_to_index) for sentence in input_sentences]\n",
    "target_data = [[word_to_index[\"<SOS>\"]] + tokenize(sentence, word_to_index) + [word_to_index[\"<EOS>\"]]\n",
    "               for sentence in target_sentences]\n",
    "\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "\n",
    "# Convert tokenized sentences into tensors\n",
    "input_tensors = [torch.tensor(seq) for seq in input_data]\n",
    "target_tensors = [torch.tensor(seq) for seq in target_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data, including padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic padding\n",
    "input_padded = pad_sequence(input_tensors, batch_first=True, padding_value=word_to_index[\"<PAD>\"])\n",
    "target_padded = pad_sequence(target_tensors, batch_first=True, padding_value=word_to_index[\"<PAD>\"])\n",
    "\n",
    "class DynamicMathWordProblemDataset(Dataset):\n",
    "    def __init__(self, input_padded, target_padded):\n",
    "        self.input_data = input_padded\n",
    "        self.target_data = target_padded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "dataset = DynamicMathWordProblemDataset(input_padded, target_padded)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 6.253276348114014\n",
      "Epoch 2/100, Loss: 4.660191059112549\n",
      "Epoch 3/100, Loss: 4.057714462280273\n",
      "Epoch 4/100, Loss: 3.254704475402832\n",
      "Epoch 5/100, Loss: 2.457012176513672\n",
      "Epoch 6/100, Loss: 1.8879307508468628\n",
      "Epoch 7/100, Loss: 1.0339797735214233\n",
      "Epoch 8/100, Loss: 0.4540209472179413\n",
      "Epoch 9/100, Loss: 0.39220842719078064\n",
      "Epoch 10/100, Loss: 0.18521679937839508\n",
      "Epoch 11/100, Loss: 0.10625506192445755\n",
      "Epoch 12/100, Loss: 0.17500664293766022\n",
      "Epoch 13/100, Loss: 0.08446114510297775\n",
      "Epoch 14/100, Loss: 0.04551088064908981\n",
      "Epoch 15/100, Loss: 0.03134765848517418\n",
      "Epoch 16/100, Loss: 0.026969825848937035\n",
      "Epoch 17/100, Loss: 0.022070562466979027\n",
      "Epoch 18/100, Loss: 0.018413597717881203\n",
      "Epoch 19/100, Loss: 0.015657030045986176\n",
      "Epoch 20/100, Loss: 0.013501527719199657\n",
      "Epoch 21/100, Loss: 0.011711714789271355\n",
      "Epoch 22/100, Loss: 0.010253261774778366\n",
      "Epoch 23/100, Loss: 0.009016893804073334\n",
      "Epoch 24/100, Loss: 0.007956234738230705\n",
      "Epoch 25/100, Loss: 0.007043604273349047\n",
      "Epoch 26/100, Loss: 0.006246873177587986\n",
      "Epoch 27/100, Loss: 0.005582251586019993\n",
      "Epoch 28/100, Loss: 0.004998332355171442\n",
      "Epoch 29/100, Loss: 0.004495877772569656\n",
      "Epoch 30/100, Loss: 0.004044949542731047\n",
      "Epoch 31/100, Loss: 0.0036387075670063496\n",
      "Epoch 32/100, Loss: 0.0032945319544523954\n",
      "Epoch 33/100, Loss: 0.0029900893568992615\n",
      "Epoch 34/100, Loss: 0.0026966012082993984\n",
      "Epoch 35/100, Loss: 0.002456382615491748\n",
      "Epoch 36/100, Loss: 0.0022436475846916437\n",
      "Epoch 37/100, Loss: 0.0020266652572900057\n",
      "Epoch 38/100, Loss: 0.0018462891457602382\n",
      "Epoch 39/100, Loss: 0.001682220259681344\n",
      "Epoch 40/100, Loss: 0.0015344965504482388\n",
      "Epoch 41/100, Loss: 0.0014008686412125826\n",
      "Epoch 42/100, Loss: 0.001279503689147532\n",
      "Epoch 43/100, Loss: 0.0011677712900564075\n",
      "Epoch 44/100, Loss: 0.001065432676114142\n",
      "Epoch 45/100, Loss: 0.0009744564304128289\n",
      "Epoch 46/100, Loss: 0.0008913639467209578\n",
      "Epoch 47/100, Loss: 0.0008157790289260447\n",
      "Epoch 48/100, Loss: 0.0007468504481948912\n",
      "Epoch 49/100, Loss: 0.0006834183586761355\n",
      "Epoch 50/100, Loss: 0.000624154054094106\n",
      "Epoch 51/100, Loss: 0.0005689588724635541\n",
      "Epoch 52/100, Loss: 0.0005205615889281034\n",
      "Epoch 53/100, Loss: 0.0004767787759192288\n",
      "Epoch 54/100, Loss: 0.00043696927605196834\n",
      "Epoch 55/100, Loss: 0.00040044798515737057\n",
      "Epoch 56/100, Loss: 0.00036715660826303065\n",
      "Epoch 57/100, Loss: 0.0003365773882251233\n",
      "Epoch 58/100, Loss: 0.00030844580032862723\n",
      "Epoch 59/100, Loss: 0.00028284743893891573\n",
      "Epoch 60/100, Loss: 0.00025945864035747945\n",
      "Epoch 61/100, Loss: 0.0002379015350015834\n",
      "Epoch 62/100, Loss: 0.00021826713054906577\n",
      "Epoch 63/100, Loss: 0.00020017549104522914\n",
      "Epoch 64/100, Loss: 0.00018359252135269344\n",
      "Epoch 65/100, Loss: 0.0001681953144725412\n",
      "Epoch 66/100, Loss: 0.0001540033263154328\n",
      "Epoch 67/100, Loss: 0.0001407126837875694\n",
      "Epoch 68/100, Loss: 0.00012878375127911568\n",
      "Epoch 69/100, Loss: 0.00011816850019386038\n",
      "Epoch 70/100, Loss: 0.00010847693920368329\n",
      "Epoch 71/100, Loss: 9.96788585325703e-05\n",
      "Epoch 72/100, Loss: 9.180703636957332e-05\n",
      "Epoch 73/100, Loss: 8.435982454102486e-05\n",
      "Epoch 74/100, Loss: 7.765313785057515e-05\n",
      "Epoch 75/100, Loss: 7.168500451371074e-05\n",
      "Epoch 76/100, Loss: 6.588817632291466e-05\n",
      "Epoch 77/100, Loss: 6.0603939346037805e-05\n",
      "Epoch 78/100, Loss: 5.5775137298041955e-05\n",
      "Epoch 79/100, Loss: 5.124035305925645e-05\n",
      "Epoch 80/100, Loss: 4.7095469199121e-05\n",
      "Epoch 81/100, Loss: 4.321679807617329e-05\n",
      "Epoch 82/100, Loss: 3.619112014770508\n",
      "Epoch 83/100, Loss: 0.2479521930217743\n",
      "Epoch 84/100, Loss: 0.053460218012332916\n",
      "Epoch 85/100, Loss: 0.026484506204724312\n",
      "Epoch 86/100, Loss: 0.01738785393536091\n",
      "Epoch 87/100, Loss: 0.012608245946466923\n",
      "Epoch 88/100, Loss: 0.009912979789078236\n",
      "Epoch 89/100, Loss: 0.007615230046212673\n",
      "Epoch 90/100, Loss: 0.00650509400293231\n",
      "Epoch 91/100, Loss: 0.005656820721924305\n",
      "Epoch 92/100, Loss: 0.004929333925247192\n",
      "Epoch 93/100, Loss: 0.004292753990739584\n",
      "Epoch 94/100, Loss: 0.0037731605116277933\n",
      "Epoch 95/100, Loss: 0.003360474482178688\n",
      "Epoch 96/100, Loss: 0.003023565048351884\n",
      "Epoch 97/100, Loss: 0.0027307181153446436\n",
      "Epoch 98/100, Loss: 0.002474331995472312\n",
      "Epoch 99/100, Loss: 0.002244815928861499\n",
      "Epoch 100/100, Loss: 0.0020386616233736277\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = len(word_to_index)  # Vocabulary size\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<PAD>\"])\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0005)#0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0005)#0.001)\n",
    "\n",
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"addition_only_math_problem_encoder\", \"addition_only_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the revised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ['thirty', 'eight', 'plus', 'twenty', 'six']\n",
      "Generated Sentence: ['<SOS>', 'sixty', 'four']\n",
      "Attention Weights Shape: [[6.0139322e-05 8.1848382e-05 1.6479433e-03 9.6513665e-01 3.3073444e-02]\n",
      " [4.7311822e-01 5.2379602e-01 3.6086902e-05 1.5776999e-04 2.8918239e-03]\n",
      " [6.1024840e-15 1.0028849e-12 1.8556992e-13 8.2386191e-07 9.9999917e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"addition_only_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"addition_only_math_problem_decoder.pth\"))\n",
    "\n",
    "#thirty eight plus twenty seven,sixty five\n",
    "\n",
    "input_sentence = \"thirty eight plus twenty six\" #sixty four\n",
    "\n",
    "# Test and visualize\n",
    "test(encoder, decoder, input_sentence, word_to_index, index_to_word, with_attention_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: thirty eight plus twenty six\n",
      "Ground Truth: ['<SOS>', 'sixty', 'four']\n",
      "Predicted: ['<SOS>', 'sixty', 'four']\n",
      "Edit Distance: 0\n",
      "\n",
      "Average Edit Distance: 0.0\n"
     ]
    }
   ],
   "source": [
    "import edit_distance\n",
    "\n",
    "def generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "    # Initialize decoder with <SOS> token\n",
    "    decoder_input = torch.tensor([word_to_index[\"<SOS>\"]], dtype=torch.long)\n",
    "    decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "    # Generate output sequence\n",
    "    output_sequence = []\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get token with highest probability\n",
    "        if predicted_token == word_to_index[\"<EOS>\"]:  # Stop at <EOS> token\n",
    "            break\n",
    "        output_sequence.append(predicted_token)\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert token indices to words\n",
    "    return [index_to_word[token] for token in output_sequence]\n",
    "\n",
    "def evaluate_with_edit_distance(test_data, encoder, decoder, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Evaluate the sequence-to-sequence model using edit distance.\n",
    "\n",
    "    Args:\n",
    "        test_data (list): List of (input_sentence, ground_truth_sentence) pairs.\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        word_to_index (dict): Word-to-index mapping.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "\n",
    "    Returns:\n",
    "        float: Average edit distance across the test dataset.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    for input_sentence, ground_truth_sentence in test_data:\n",
    "        # Generate predicted sequence\n",
    "        predicted_sentence = generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word)\n",
    "\n",
    "        # Compute edit distance\n",
    "        distance = edit_distance.SequenceMatcher(a=ground_truth_sentence, b=predicted_sentence).distance()\n",
    "        total_distance += distance\n",
    "\n",
    "        # Debugging: Print example results\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Ground Truth: {ground_truth_sentence}\")\n",
    "        print(f\"Predicted: {predicted_sentence}\")\n",
    "        print(f\"Edit Distance: {distance}\\n\")\n",
    "\n",
    "    # Calculate average edit distance\n",
    "    average_distance = total_distance / len(test_data)\n",
    "    print(f\"Average Edit Distance: {average_distance}\")\n",
    "    return average_distance\n",
    "\n",
    "\n",
    "# Example dataset: [(input_sentence, ground_truth_sentence)]\n",
    "test_data = [\n",
    "    (\"thirty eight plus twenty six\", [\"<SOS>\", \"sixty\", \"four\"]),\n",
    "]\n",
    "\n",
    "# Assume you have trained encoder and decoder models\n",
    "average_distance = evaluate_with_edit_distance(\n",
    "    test_data, encoder, decoder, word_to_index, index_to_word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to be tuned\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 128, 512)  # Hidden layer size\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)  # LR (log-scaled search)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate\n",
    "    \n",
    "    # Define encoder-decoder with suggested hyperparameters\n",
    "    encoder = Encoder(input_size, hidden_size).to(device)\n",
    "    decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "    # Apply dropout in LSTM layers\n",
    "    encoder.lstm.dropout = dropout_rate\n",
    "    decoder.lstm.dropout = dropout_rate\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[\"<PAD>\"])\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Run one epoch to evaluate hyperparameter performance\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "        decoder_input = torch.tensor([word_to_index[\"<SOS>\"]] * input_seq.size(0), device=device)\n",
    "        decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(target_seq.size(1)):\n",
    "            output, decoder_hidden, decoder_cell, _ = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "            loss += criterion(output, target_seq[:, t])\n",
    "            decoder_input = target_seq[:, t]  # Teacher forcing\n",
    "        \n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)  # Optuna minimizes this loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform optimum hyperparameter search\n",
    "\n",
    "\n",
    "Best hyperparameters: {'hidden_size': 283, 'learning_rate': 0.004869044905756817, 'dropout_rate': 0.28901830138217327}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize loss\n",
    "study.optimize(objective, n_trials=30)  # Run 30 optimization trials\n",
    "\n",
    "# Print best hyperparameter combination\n",
    "print(\"Best hyperparameters:\", study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
