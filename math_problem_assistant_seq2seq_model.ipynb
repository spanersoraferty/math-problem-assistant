{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Add our overview narrative here!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This Encoder class is a part of the larger sequence-to-sequence model designed for processing mathematical word problems using an LSTM. It takes input sequences (math question) represented as token indices and converts them into dense vector embeddings using an embedding layer, allowing the model to capture meaningful semantic relationships between words. The embedded representations then pass through an LSTM layer, which extracts temporal dependencies and generates both hidden and cell states that encode contextual information about the input sequence. A dropout layer is applied to the embeddings to reduce overfitting and improve generalization. Ultimately, the encoder produces a set of outputs from each time step and a final hidden representation, which the decoder later uses to generate responses or solutions (the expected answer to our math question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an LSTM. \n",
    "    Converts a sequence of token indices into a hidden representation \n",
    "    that will be used by the decoder for sequence generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder module.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): The size of the input vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Embedding layer converts token indices into dense vector representations.\n",
    "        # Instead of using raw word indices (which lack meaning), embeddings allow\n",
    "        # the LSTM to learn meaningful semantic relationships between words.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "\n",
    "        # LSTM layer processes embedded input sequences to generate hidden states.\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Dropout layer applied to embeddings to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing token indices for a batch of input sentences.\n",
    "\n",
    "        Returns:\n",
    "            outputs (Tensor): Encoder outputs at each time step.\n",
    "            hidden (Tensor): Final hidden state of the LSTM.\n",
    "            cell (Tensor): Final cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_seq)           # Convert input tokens into embeddings\n",
    "        embedded = self.dropout(embedded)             # Apply dropout before LSTM processing\n",
    "        outputs, (hidden, cell) = self.lstm(embedded) # Process embeddings through LSTM\n",
    "\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The Attention class implements 'Bahdanau attention', which dynamically calculates attention scores based on the decoder’s hidden state and the encoder’s outputs. Instead of relying on a fixed context vector, this method allows the decoder to focus on specific parts of the input sequence at each decoding step. The attention mechanism works by concatenating the decoder's hidden state with the encoder’s outputs, passing them through a linear transformation, and applying the tanh activation function. A learnable vector (v) helps compute alignment scores, which are then normalized using softmax to generate attention weights—these determine the importance of each encoder output when predicting the next token. A dropout layer is also applied to prevent overfitting. The output is a set of attention weights that guides the decoder in generating more context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Implements Bahdanau attention mechanism for a sequence-to-sequence model. \n",
    "    Dynamically computes attention scores based on the decoder’s hidden state \n",
    "    and the encoder’s outputs, allowing the decoder to focus on relevant \n",
    "    parts of the input sequence at each decoding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden state of the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        # Linear layer to compute alignment scores\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        # Learnable vector for attention computation\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "        # Dropout layer to regularize attention mechanism and prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Computes attention weights using Bahdanau's additive attention method.\n",
    "\n",
    "        Args:\n",
    "            hidden (Tensor): Decoder hidden state at the current time step.\n",
    "            encoder_outputs (Tensor): Encoder outputs at all time steps.\n",
    "\n",
    "        Returns:\n",
    "            attention_weights (Tensor): Softmax-normalized attention scores.\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "\n",
    "        # Expand the decoder hidden state across the sequence length\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "\n",
    "        # Concatenate hidden state with encoder outputs to compute alignment scores\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "\n",
    "        # Apply dropout to energy scores before computing attention weights\n",
    "        energy = self.dropout(energy)\n",
    "\n",
    "        # Transpose energy tensor for compatibility with the attention vector\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "\n",
    "        # Expand learned vector `v` across batch size\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "\n",
    "        # Compute attention weights via matrix multiplication\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "\n",
    "        # Apply softmax to normalize scores across sequence length\n",
    "        return torch.softmax(attention_weights, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The Decoder class plays a crucial role in our sequence-to-sequence model designed to solve math word problems. When given a problem as an input sequence, the Encoder first processes it into a hidden representation, which the Decoder then uses to generate the solution step by step. At each step, the attention mechanism dynamically selects the most relevant parts of the encoded problem statement, allowing the Decoder to focus on specific numerical relationships and mathematical operations. The LSTM maintains contextual understanding across time steps, helping track dependencies between numbers and mathematical operators. As the model generates each token in the solution, it refines its prediction using previously computed values, making the process similar to how humans break down word problems into logical steps. This structure ensures that the model interprets and solves math problems contextually, rather than simply memorizing formulas, enabling it to generalize across different problem types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Decoder for the sequence-to-sequence math problem assistant model using an \n",
    "    LSTM with Bahdanau attention. The decoder generates output tokens one by one \n",
    "    while dynamically focusing on relevant parts of the encoder’s outputs using \n",
    "    the attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Embedding layer converts token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "\n",
    "        # LSTM layer processes embeddings and maintains hidden state across timesteps\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "        # Linear layer maps concatenated attention context & LSTM output to vocab space\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        # Dropout layer to regularize embeddings before passing them to LSTM\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "\n",
    "        # Attention mechanism for dynamic focus on encoder outputs\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Current token input to the decoder.\n",
    "            hidden (Tensor): Previous hidden state from the LSTM.\n",
    "            cell (Tensor): Previous cell state from the LSTM.\n",
    "            encoder_outputs (Tensor): Encoder outputs from all timesteps.\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): Predicted token probabilities.\n",
    "            hidden (Tensor): Updated hidden state.\n",
    "            cell (Tensor): Updated cell state.\n",
    "            attention_weights (Tensor): Attention scores for each encoder timestep.\n",
    "        \"\"\"\n",
    "        # Expand input dimensions to match expected input shape for embedding\n",
    "        input = input.unsqueeze(1)  \n",
    "\n",
    "        # Convert token indices into dense embeddings & apply dropout for regularization\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        # Forward pass through LSTM to generate new hidden and cell states\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "\n",
    "        # Compute attention weights using the current hidden state and encoder outputs\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "\n",
    "        # Apply attention: generate weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "        # Flatten tensors for the fully connected layer\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "\n",
    "        # Generate token probabilities using concatenated LSTM output and attention context\n",
    "        output = self.fc(torch.cat((lstm_output, context), dim=1))\n",
    "\n",
    "        return output, hidden, cell, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic tokenisation and vocabulary Setup\n",
    "\n",
    "Here we set up the constants and tokenisation process for a sequence-to-sequence math problem assistant. It first defines special tokens <SOS>, <EOS>, and <PAD> to mark the start, end, and padding of sequences. Then, it creates a vocabulary mapping that converts words into numerical indices, allowing the model to process text as numbers. The reverse mapping (index_to_word) ensures that predictions can be decoded back into words. Finally, the script tokenizes example input and target sequences, transforming for example, \"two plus four\" into a list of indices [3, 4, 5] and the target \"equals six\" into [0, 6, 7, 1], ensuring that the decoder starts with <SOS> and ends with <EOS>. This setup enables the neural network to work with structured input-output pairs for training our expected math-solving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estalish constants\n",
    "START_OF_SEQUENCE = \"<SOS>\"\n",
    "END_OF_SEQUENCE = \"<EOS>\"\n",
    "PADDING_SEQUENCE = \"<PAD>\"\n",
    "\n",
    "# Tokenization and vocab setup\n",
    "# Create a vocabulary mapping words to indices\n",
    "word_to_index = {START_OF_SEQUENCE: 0, END_OF_SEQUENCE: 1, PADDING_SEQUENCE: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Example input and target sequences\n",
    "input_sentence = \"two plus four\"\n",
    "target_sentence = \"equals six\"\n",
    "\n",
    "# Test Tokenized input and targets\n",
    "input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "target_tokens = [word_to_index[START_OF_SEQUENCE]] + [word_to_index[word] for word in target_sentence.split()] + [word_to_index[END_OF_SEQUENCE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data\n",
    "\n",
    "This BasicMathWordProblemDataset class creates a custom PyTorch Dataset for training our sequence-to-sequence model that solves basic math word problems. It converts input and target sentences (our math questions) into lists of token indices using a predefined vocabulary (word_to_index). The target sequence is prepended with <SOS> and appended with <EOS> tokens to indicate the start and end of the output. The class implements essential dataset methods, notablu __len__() which returns the number of samples, while __getItem__() retrieves tokenized tensors for input and target sequences, ensuring compatibility with PyTorch models. Finally, the dataset is wrapped in a DataLoader, allowing efficient batch processing with shuffling to improve learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BasicMathWordProblemDataset(Dataset):\n",
    "    def __init__(self, input_sentences, target_sentences, word_to_index):\n",
    "        self.input_data = [[word_to_index[word] for word in sentence.split()] for sentence in input_sentences]\n",
    "        self.target_data = [[word_to_index[START_OF_SEQUENCE]] +\n",
    "                            [word_to_index[word] for word in sentence.split()] +\n",
    "                            [word_to_index[END_OF_SEQUENCE]] for sentence in target_sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.input_data[idx], dtype=torch.long), torch.tensor(self.target_data[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "input_sentences = [\"two plus four\"]\n",
    "target_sentences = [\"equals six\"]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BasicMathWordProblemDataset(input_sentences, target_sentences, word_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters\n",
    "\n",
    "Here we initialise the key components for training our sequence-to-sequence math word problem solving model. The encoder and decoder are instantiated with the vocabulary size (input_size and output_size) and a hidden state of 128, allowing the model to process and generate our math solutions. The CrossEntropy loss function is used with padding tokens ignored to prevent unnecessary calculations from affecting training. Adam optimizers are applied to both the encoder and decoder with a learning rate of 0.001, enabling stable gradient updates. Finally, an attention matrix is initialized to store attention weights, which will later be used for visualization, thus helping to analyze how the model focuses on different parts of the input when generating solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "input_size = len(word_to_index)  # Total vocabulary size\n",
    "output_size = len(word_to_index)  # Vocabulary size\n",
    "hidden_size = 128\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "# Ignore padding tokens\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE])  \n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# Store attention weights for visualization\n",
    "attention_matrix = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss over epochs\n",
    "\n",
    "This function visualizes the training loss over epochs using Matplotlib. It takes a list of loss values (epoch_losses), representing the loss at each epoch, and plots them on a graph to track performance trends over time. This is useful for monitoring model convergence and diagnosing potential issues such as overfitting or slow learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(epoch_losses):\n",
    "    \"\"\"Plots the training loss over epochs.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', label='Training Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the training regime\n",
    "\n",
    "This train() function is responsible for training our sequence-to-sequence model with an encoder-decoder architecture with 'Bahdanau attention'. It iterates through a dataset of tokenized math problems for multiple epochs, processing each batch by passing inputs through the encoder, which generates a context vector. The decoder then predicts output tokens step by step, using teacher forcing to guide learning while dynamically focusing on relevant parts of the input via attention mechanisms. The function tracks masked loss, ensuring that padding tokens don't affect optimization, and updates model weights using backpropagation with Adam optimizers. After training, it stores attention weights for visualization, logs epoch losses, plots them for monitoring, and saves the trained models to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, encoder_save_name, decoder_save_name):\n",
    "    \"\"\"\n",
    "    Trains the sequence-to-sequence model using an encoder-decoder architecture.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        encoder_save_name (str): File name for saving the trained encoder model.\n",
    "        decoder_save_name (str): File name for saving the trained decoder model.\n",
    "    \"\"\"\n",
    "    epoch_losses = []  # Stores loss values for tracking progress\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0  # Accumulate total loss for the epoch\n",
    "\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            # Move data to GPU if available\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "\n",
    "            # Reset gradients before each batch\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "\n",
    "            # Encoder forward pass - processes input sequences to generate hidden states\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "            # Decoder initialization - starts with the encoder's final hidden state\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "            # Get actual sequence lengths (excluding padding)\n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()\n",
    "\n",
    "            loss = 0  # Tracks batch loss\n",
    "            \n",
    "            # Iterate through target sequence timesteps\n",
    "            for t in range(max_target_length):\n",
    "                # Determine active sequences\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():  # If all sequences finished, break loop\n",
    "                    break\n",
    "\n",
    "                # Decoder forward pass - generates output token probabilities\n",
    "                output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "\n",
    "                # Masked loss calculation - only consider active sequences\n",
    "                loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "\n",
    "                # Teacher forcing: Use actual target token as next input\n",
    "                decoder_input = target_seq[:, t]  \n",
    "\n",
    "            # Backpropagation - compute gradients and update model parameters\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Store normalized loss\n",
    "        epoch_losses.append(epoch_loss / len(dataloader))\n",
    "\n",
    "        # Print training progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_losses[-1]:.4f}\")\n",
    "\n",
    "        # Store attention weights for visualization\n",
    "        attention_matrix.append(attention_weights.cpu().detach().numpy())\n",
    "\n",
    "    # Plot training loss\n",
    "    plot_loss(epoch_losses)\n",
    "\n",
    "    # Save the trained models\n",
    "    torch.save(encoder.state_dict(), f\"{encoder_save_name}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"{decoder_save_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We'll start training for 100 epochs to refine our sequence-to-sequence model. This will help our encoder-decoder system learn better representations and improve predictions for solving math word problems. We'll expect to see loss decreasing over time, but if it stagnates or spikes, we can adjust learning rate, batch size, and/or other hyperparameters to stabilise training. Also, our ability to monitor attention weights will help us verify whether the model is correctly focusing on key parts of the input during decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 9.6464\n",
      "Epoch 2/100, Loss: 8.9517\n",
      "Epoch 3/100, Loss: 8.1775\n",
      "Epoch 4/100, Loss: 7.5406\n",
      "Epoch 5/100, Loss: 6.8955\n",
      "Epoch 6/100, Loss: 6.3454\n",
      "Epoch 7/100, Loss: 5.7458\n",
      "Epoch 8/100, Loss: 5.1202\n",
      "Epoch 9/100, Loss: 4.6462\n",
      "Epoch 10/100, Loss: 4.1824\n",
      "Epoch 11/100, Loss: 3.7260\n",
      "Epoch 12/100, Loss: 3.4011\n",
      "Epoch 13/100, Loss: 2.9852\n",
      "Epoch 14/100, Loss: 2.7587\n",
      "Epoch 15/100, Loss: 2.3901\n",
      "Epoch 16/100, Loss: 2.1689\n",
      "Epoch 17/100, Loss: 1.9994\n",
      "Epoch 18/100, Loss: 1.7870\n",
      "Epoch 19/100, Loss: 1.6325\n",
      "Epoch 20/100, Loss: 1.4670\n",
      "Epoch 21/100, Loss: 1.3234\n",
      "Epoch 22/100, Loss: 1.1725\n",
      "Epoch 23/100, Loss: 1.0869\n",
      "Epoch 24/100, Loss: 0.9704\n",
      "Epoch 25/100, Loss: 0.8914\n",
      "Epoch 26/100, Loss: 0.8354\n",
      "Epoch 27/100, Loss: 0.7640\n",
      "Epoch 28/100, Loss: 0.6730\n",
      "Epoch 29/100, Loss: 0.6229\n",
      "Epoch 30/100, Loss: 0.5628\n",
      "Epoch 31/100, Loss: 0.5154\n",
      "Epoch 32/100, Loss: 0.4701\n",
      "Epoch 33/100, Loss: 0.4334\n",
      "Epoch 34/100, Loss: 0.4027\n",
      "Epoch 35/100, Loss: 0.3665\n",
      "Epoch 36/100, Loss: 0.3317\n",
      "Epoch 37/100, Loss: 0.3092\n",
      "Epoch 38/100, Loss: 0.2931\n",
      "Epoch 39/100, Loss: 0.2616\n",
      "Epoch 40/100, Loss: 0.2484\n",
      "Epoch 41/100, Loss: 0.2262\n",
      "Epoch 42/100, Loss: 0.2181\n",
      "Epoch 43/100, Loss: 0.2038\n",
      "Epoch 44/100, Loss: 0.1811\n",
      "Epoch 45/100, Loss: 0.1742\n",
      "Epoch 46/100, Loss: 0.1639\n",
      "Epoch 47/100, Loss: 0.1572\n",
      "Epoch 48/100, Loss: 0.1490\n",
      "Epoch 49/100, Loss: 0.1358\n",
      "Epoch 50/100, Loss: 0.1310\n",
      "Epoch 51/100, Loss: 0.1215\n",
      "Epoch 52/100, Loss: 0.1143\n",
      "Epoch 53/100, Loss: 0.1055\n",
      "Epoch 54/100, Loss: 0.1045\n",
      "Epoch 55/100, Loss: 0.1031\n",
      "Epoch 56/100, Loss: 0.0965\n",
      "Epoch 57/100, Loss: 0.0921\n",
      "Epoch 58/100, Loss: 0.0874\n",
      "Epoch 59/100, Loss: 0.0844\n",
      "Epoch 60/100, Loss: 0.0785\n",
      "Epoch 61/100, Loss: 0.0762\n",
      "Epoch 62/100, Loss: 0.0721\n",
      "Epoch 63/100, Loss: 0.0721\n",
      "Epoch 64/100, Loss: 0.0663\n",
      "Epoch 65/100, Loss: 0.0665\n",
      "Epoch 66/100, Loss: 0.0628\n",
      "Epoch 67/100, Loss: 0.0630\n",
      "Epoch 68/100, Loss: 0.0599\n",
      "Epoch 69/100, Loss: 0.0597\n",
      "Epoch 70/100, Loss: 0.0553\n",
      "Epoch 71/100, Loss: 0.0543\n",
      "Epoch 72/100, Loss: 0.0540\n",
      "Epoch 73/100, Loss: 0.0511\n",
      "Epoch 74/100, Loss: 0.0525\n",
      "Epoch 75/100, Loss: 0.0486\n",
      "Epoch 76/100, Loss: 0.0489\n",
      "Epoch 77/100, Loss: 0.0479\n",
      "Epoch 78/100, Loss: 0.0481\n",
      "Epoch 79/100, Loss: 0.0426\n",
      "Epoch 80/100, Loss: 0.0422\n",
      "Epoch 81/100, Loss: 0.0418\n",
      "Epoch 82/100, Loss: 0.0440\n",
      "Epoch 83/100, Loss: 0.0402\n",
      "Epoch 84/100, Loss: 0.0395\n",
      "Epoch 85/100, Loss: 0.0393\n",
      "Epoch 86/100, Loss: 0.0410\n",
      "Epoch 87/100, Loss: 0.0366\n",
      "Epoch 88/100, Loss: 0.0364\n",
      "Epoch 89/100, Loss: 0.0353\n",
      "Epoch 90/100, Loss: 0.0346\n",
      "Epoch 91/100, Loss: 0.0353\n",
      "Epoch 92/100, Loss: 0.0345\n",
      "Epoch 93/100, Loss: 0.0336\n",
      "Epoch 94/100, Loss: 0.0332\n",
      "Epoch 95/100, Loss: 0.0326\n",
      "Epoch 96/100, Loss: 0.0317\n",
      "Epoch 97/100, Loss: 0.0335\n",
      "Epoch 98/100, Loss: 0.0298\n",
      "Epoch 99/100, Loss: 0.0295\n",
      "Epoch 100/100, Loss: 0.0297\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"basic_math_problem_encoder\", \"basic_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate heatmap\n",
    "\n",
    "Function to allow us to visualise attention weights as a heatmap, helping us analyse which input tokens the decoder focuses on while generating each output token in a sequence-to-sequence model. Using Matplotlib, it plots the attention_matrix as a color-coded intensity map, where darker shades indicate stronger attention at specific positions. This visualization is helpful for allowing us to understand how our model prioritises different parts of the input, making it inciteful when attempting to optimise our attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention(input_tokens, output_tokens, attention_matrix):\n",
    "    \"\"\"\n",
    "    Plot the attention weights as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        input_tokens (list of str): Tokens in the input sequence.\n",
    "        output_tokens (list of str): Tokens in the output sequence.\n",
    "        attention_matrix (np.array): Attention weights matrix (output_tokens x input_tokens).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attention_matrix, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=np.arange(len(input_tokens)), labels=input_tokens, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks=np.arange(len(output_tokens)), labels=output_tokens)\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.xlabel(\"Input Tokens\")\n",
    "    plt.ylabel(\"Output Tokens\")\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup\n",
    "\n",
    "Function to allow us to test our trained sequence-to-sequence model by encoding our math question input sentence into hidden states and decoding it step-by-step to generate our expected answer. It tokenizes the input using a predefined vocabulary and passes it through the encoder, which extracts a contextual representation. The decoder then predicts each output token using an iterative approach, incorporating attention weights to dynamically focus on relevant parts of the input. The function tracks attention matrices, enabling visualization of how the model distributes focus across tokens. It prints the input sentence, generated output, and attention shape, and optionally plots a heatmap for deeper analysis. The final output is a list of predicted words, representing the answer to our math problem, which can then be evaluated for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=100, with_attention_plot=False):\n",
    "    # Set the models to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Forward pass through the encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "    # Initialize the decoder\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]], dtype=torch.long)  # Start-of-Sequence token\n",
    "    decoder_hidden = hidden\n",
    "    decoder_cell = cell\n",
    "\n",
    "    # Generate output sequence and collect attention weights\n",
    "    output_sequence = []\n",
    "    attention_matrices = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get the token with the highest probability\n",
    "\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE]:  # Stop at End-of-Sequence token\n",
    "            break\n",
    "\n",
    "        output_sequence.append(predicted_token)\n",
    "        attention_matrices.append(attention_weights.cpu().detach().numpy())  # Save attention weights\n",
    "\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)  # Next decoder input\n",
    "\n",
    "    # Convert input and output tokens to words\n",
    "    input_sentence_tokens = [index_to_word[token] for token in input_tokens]\n",
    "    output_sentence_tokens = [index_to_word[token] for token in output_sequence]\n",
    "\n",
    "    # Stack attention matrices into a 2D array (output_tokens x input_tokens)\n",
    "    attention_matrix = np.vstack(attention_matrices)\n",
    "\n",
    "    # Print the input and output sentences\n",
    "    print(\"Input Sentence:\", input_sentence_tokens)\n",
    "    print(\"Generated Sentence:\", output_sentence_tokens)\n",
    "    print(\"Attention Weights Shape:\", attention_matrix)\n",
    "\n",
    "    # Visualize attention\n",
    "    if with_attention_plot:\n",
    "        # Plot the attention weights\n",
    "        plot_attention(input_sentence_tokens, output_sentence_tokens, attention_matrix)\n",
    "\n",
    "    # Convert token indices to words\n",
    "    return [index_to_word[token] for token in output_sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model - basic approach\n",
    "\n",
    "Here we load our pre-trained sequence-to-sequence model for solving basic math word problems. We first initializes the encoder and decoder architectures with the same parameters used during training, ensuring consistency. Then, we load the pretrained model weights from the associated **.pth** files, restoring previously learned knowledge. After defining a sample input sentence, **\"two plus four\"**, we set up **word-to-index** and **index-to-word** mappings, allowing tokenisation and conversion between words and numerical indices. Finally, the test function is called, running the encoder-decoder model on the input question to generate a predicted answer while optionally visualizing attention weights so we can attempt analyse how the model focuses on different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ['two', 'plus', 'four']\n",
      "Generated Sentence: ['<SOS>', 'equals', 'six']\n",
      "Attention Weights Shape: [[5.8697307e-13 1.2477790e-07 9.9999988e-01]\n",
      " [1.2839609e-12 1.8266903e-07 9.9999976e-01]\n",
      " [7.8026789e-13 1.4139404e-07 9.9999988e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<SOS>', 'equals', 'six']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 128  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"basic_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"basic_math_problem_decoder.pth\"))\n",
    "\n",
    "input_sentence = \"two plus four\"\n",
    "\n",
    "word_to_index = {START_OF_SEQUENCE: 0, END_OF_SEQUENCE: 1, PADDING_SEQUENCE: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Test and visualize\n",
    "test(encoder, decoder, input_sentence, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a more complex dataset and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline vocabulary and tokenization capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "word_to_index = {START_OF_SEQUENCE: 0, END_OF_SEQUENCE: 1, PADDING_SEQUENCE: 2}  # establish only the baseline tokens this time\n",
    " \n",
    "# Function to tokenize a sentence and update mapping dynamically\n",
    "def tokenize(sentence, word_to_index):\n",
    "    tokens = []\n",
    "    for word in sentence.lower().split():\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)  # Assign index to new words\n",
    "        tokens.append(word_to_index[word])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "def load_sequences_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df[\"Problem\"].tolist(), df[\"Solution\"].tolist()\n",
    "\n",
    "csv_file = \"simple_math_problems_addition_only.csv\"\n",
    "input_sentences, target_sentences = load_sequences_from_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize input and target sentences and convert into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and target sentences\n",
    "input_data = [tokenize(sentence, word_to_index) for sentence in input_sentences]\n",
    "target_data = [[word_to_index[START_OF_SEQUENCE]] + tokenize(sentence, word_to_index) + [word_to_index[END_OF_SEQUENCE]]\n",
    "               for sentence in target_sentences]\n",
    "\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "\n",
    "# Convert tokenized sentences into tensorsj\n",
    "input_tensors = [torch.tensor(seq) for seq in input_data]\n",
    "target_tensors = [torch.tensor(seq) for seq in target_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokenzied data, including padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic padding\n",
    "input_padded = pad_sequence(input_tensors, batch_first=True, padding_value=word_to_index[PADDING_SEQUENCE])\n",
    "target_padded = pad_sequence(target_tensors, batch_first=True, padding_value=word_to_index[PADDING_SEQUENCE])\n",
    "\n",
    "class DynamicMathWordProblemDataset(Dataset):\n",
    "    def __init__(self, input_padded, target_padded):\n",
    "        self.input_data = input_padded\n",
    "        self.target_data = target_padded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "dataset = DynamicMathWordProblemDataset(input_padded, target_padded)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 9.1870\n",
      "Epoch 2/100, Loss: 5.5312\n",
      "Epoch 3/100, Loss: 4.6704\n",
      "Epoch 4/100, Loss: 4.1333\n",
      "Epoch 5/100, Loss: 3.5660\n",
      "Epoch 6/100, Loss: 2.6794\n",
      "Epoch 7/100, Loss: 1.6561\n",
      "Epoch 8/100, Loss: 0.9916\n",
      "Epoch 9/100, Loss: 0.6046\n",
      "Epoch 10/100, Loss: 0.3978\n",
      "Epoch 11/100, Loss: 0.2719\n",
      "Epoch 12/100, Loss: 0.2141\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = len(word_to_index)  # Vocabulary size\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE]) # Ignore padding tokens\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0005)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0005)\n",
    "\n",
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"addition_only_math_problem_encoder\", \"addition_only_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the revised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"addition_only_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"addition_only_math_problem_decoder.pth\"))\n",
    "\n",
    "#thirty eight plus twenty seven,sixty five\n",
    "\n",
    "input_sentence = \"thirty eight plus twenty six\" #sixty four\n",
    "\n",
    "# Test and visualize\n",
    "test_token_indices_to_words = test(encoder, decoder, input_sentence, word_to_index, index_to_word, with_attention_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the test predictions for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=10):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "    # Initialize decoder with <SOS> token\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]], dtype=torch.long)\n",
    "    decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "    # Generate output sequence\n",
    "    output_sequence = []\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get token with highest probability\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE]:  # Stop at <EOS> token\n",
    "            break\n",
    "        output_sequence.append(predicted_token)\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert token indices to words\n",
    "    return [index_to_word[token] for token in output_sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edit_distance\n",
    "\n",
    "def evaluate_with_edit_distance(test_data, encoder, decoder, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Evaluate the sequence-to-sequence model using edit distance.\n",
    "\n",
    "    Args:\n",
    "        test_data (list): List of (input_sentence, ground_truth_sentence) pairs.\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        word_to_index (dict): Word-to-index mapping.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "\n",
    "    Returns:\n",
    "        float: Average edit distance across the test dataset.\n",
    "    \"\"\"\n",
    "    total_distance = 0\n",
    "    for input_sentence, ground_truth_sentence in test_data:\n",
    "        # Generate predicted sequence\n",
    "        predicted_sentence = generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word)\n",
    "\n",
    "        # Compute edit distance\n",
    "        distance = edit_distance.SequenceMatcher(a=ground_truth_sentence, b=predicted_sentence).distance()\n",
    "        total_distance += distance\n",
    "\n",
    "        # Debugging: Print example results\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Ground Truth: {ground_truth_sentence}\")\n",
    "        print(f\"Predicted: {predicted_sentence}\")\n",
    "        print(f\"Edit Distance: {distance}\\n\")\n",
    "\n",
    "    # Calculate average edit distance\n",
    "    average_distance = total_distance / len(test_data)\n",
    "    print(f\"Average Edit Distance: {average_distance}\")\n",
    "    return average_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using edit distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset: [(input_sentence, ground_truth_sentence)]\n",
    "test_data = [\n",
    "    (\"thirty eight plus twenty six\", [START_OF_SEQUENCE, \"sixty\", \"four\"]),\n",
    "    (\"thirty eight plus twenty five\", [START_OF_SEQUENCE, \"sixty\", \"three\"]),\n",
    "    (\"thirty eight plus twenty four\", [START_OF_SEQUENCE, \"sixty\", \"two\"]),\n",
    "    (\"thirty eight plus twenty three\", [START_OF_SEQUENCE, \"sixty\", \"one\"]),\n",
    "    (\"thirty eight plus twenty two\", [START_OF_SEQUENCE, \"sixty\"]),\n",
    "    (\"thirty eight plus twenty one\", [START_OF_SEQUENCE, \"fifty\", \"nine\"]),\n",
    "    (\"thirty eight plus twenty\", [START_OF_SEQUENCE, \"fifty\", \"eight\"]),\n",
    "    (\"thirty eight plus nineteen\", [START_OF_SEQUENCE, \"fifty\", \"seven\"]),\n",
    "    (\"thirty eight plus eighteen\", [START_OF_SEQUENCE, \"fifty\", \"six\"]),\n",
    "    (\"thirty eight plus seventeen\", [START_OF_SEQUENCE, \"fifty\", \"five\"]),\n",
    "]\n",
    "\n",
    "# Assume you have trained encoder and decoder models\n",
    "average_distance = evaluate_with_edit_distance(\n",
    "    test_data, encoder, decoder, word_to_index, index_to_word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1, max_seq_len=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.positional_encodings = nn.Embedding(max_seq_len, hidden_size)  # Positional encodings\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        positions = torch.arange(0, input_seq.size(1), device=input_seq.device).unsqueeze(0)\n",
    "        positional_enc = self.positional_encodings(positions)\n",
    "        embedded = self.dropout(self.embedding(input_seq) + positional_enc)  # Add positional encodings\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters to be tuned\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 128, 512)  # Hidden layer size\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)  # LR (log-scaled search)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate\n",
    "    \n",
    "    # Define encoder-decoder with suggested hyperparameters\n",
    "    encoder = Encoder(input_size, hidden_size).to(device)\n",
    "    decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "    # Apply dropout in LSTM layers\n",
    "    encoder.lstm.dropout = dropout_rate\n",
    "    decoder.lstm.dropout = dropout_rate\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE])\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Run one epoch to evaluate hyperparameter performance\n",
    "    total_loss = 0\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "        decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]] * input_seq.size(0), device=device)\n",
    "        decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "        loss = 0\n",
    "        for t in range(target_seq.size(1)):\n",
    "            output, decoder_hidden, decoder_cell, _ = decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "            loss += criterion(output, target_seq[:, t])\n",
    "            decoder_input = target_seq[:, t]  # Teacher forcing\n",
    "        \n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)  # Optuna minimizes this loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform optimum hyperparameter search\n",
    "\n",
    "\n",
    "Best hyperparameters: {'hidden_size': 283, 'learning_rate': 0.004869044905756817, 'dropout_rate': 0.28901830138217327}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")  # Minimize loss\n",
    "study.optimize(objective, n_trials=30)  # Run 30 optimization trials\n",
    "\n",
    "# Print best hyperparameter combination\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "best_hidden_size = best_params[\"hidden_size\"]\n",
    "best_learning_rate = best_params[\"learning_rate\"]\n",
    "best_dropout_rate = best_params[\"dropout_rate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with k-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def train_with_k_fold(num_epochs, encoder_save_name, decoder_save_name, k_folds=5):\n",
    "    \"\"\"\n",
    "    Implements k-fold cross-validation for training the sequence-to-sequence model.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of epochs for training each fold.\n",
    "        encoder_save_name (str): Base name for saving the trained encoder model.\n",
    "        decoder_save_name (str): Base name for saving the trained decoder model.\n",
    "        k_folds (int): Number of folds for cross-validation.\n",
    "    \"\"\"\n",
    "    # Create k-fold object\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_index = 1  # Track the current fold\n",
    "    for train_indices, val_indices in kfold.split(dataset):  # `dataset` should be preloaded\n",
    "        \n",
    "        # Split dataset into training and validation sets for the current fold\n",
    "        train_data = Subset(dataset, train_indices)\n",
    "        val_data = Subset(dataset, val_indices)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print(f\"Training fold {fold_index}/{k_folds}...\")\n",
    "        \n",
    "        # Reset models and optimizers for each fold\n",
    "        encoder.apply(weight_init)  # `weight_init` resets model weights\n",
    "        decoder.apply(weight_init)\n",
    "        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Train for the specified number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0\n",
    "            for input_seq, target_seq in train_loader:\n",
    "                input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "                \n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "                \n",
    "                # **ENCODER & DECODER TRAINING LOGIC (Same as your original code)** #\n",
    "                encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "                decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]] * input_seq.size(0), device=device)\n",
    "                decoder_hidden, decoder_cell = hidden, cell\n",
    "                \n",
    "                target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE]).sum(dim=1)\n",
    "                max_target_length = target_lengths.max().item()\n",
    "                loss = 0\n",
    "                \n",
    "                for t in range(max_target_length):\n",
    "                    still_active = t < target_lengths\n",
    "                    if not still_active.any():\n",
    "                        break\n",
    "\n",
    "                    output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                        decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                    )\n",
    "                    \n",
    "                    loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "                    decoder_input = target_seq[:, t]\n",
    "\n",
    "                loss.backward()\n",
    "                encoder_optimizer.step()\n",
    "                decoder_optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss = evaluate_model(val_loader)  # Define a validation evaluation function\n",
    "        print(f\"Validation Loss for fold {fold_index}: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save the models for the current fold\n",
    "        torch.save(encoder.state_dict(), f\"{encoder_save_name}_fold{fold_index}.pth\")\n",
    "        torch.save(decoder.state_dict(), f\"{decoder_save_name}_fold{fold_index}.pth\")\n",
    "        \n",
    "        fold_index += 1\n",
    "\n",
    "    print(\"K-fold cross-validation complete!\")\n",
    "\n",
    "def evaluate_model(dataloader):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set and compute loss.\n",
    "\n",
    "    Args:\n",
    "        dataloader: DataLoader for the validation set.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation loss.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            \n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "            \n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()\n",
    "            \n",
    "            for t in range(max_target_length):\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():\n",
    "                    break\n",
    "\n",
    "                output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "                \n",
    "                val_loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum().item() / still_active.sum().item()\n",
    "                decoder_input = target_seq[:, t]\n",
    "    \n",
    "    return val_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement hyperparameter changes and re-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_size = len(word_to_index)  # Vocabulary size\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = best_hidden_size  # Use best hidden size from tuning\n",
    "dropout_rate = best_dropout_rate # Use best dropout rate from tuning\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size, dropout_rate).to(device)\n",
    "decoder = Decoder(output_size, hidden_size, dropout_rate).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE]) # Ignore padding tokens\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=best_learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=best_learning_rate)\n",
    "\n",
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"addition_only_math_problem_encoder\", \"addition_only_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset: [(input_sentence, ground_truth_sentence)]\n",
    "test_data = [\n",
    "    (\"thirty eight plus twenty six\", [START_OF_SEQUENCE, \"sixty\", \"four\"]),\n",
    "    (\"thirty eight plus twenty five\", [START_OF_SEQUENCE, \"sixty\", \"three\"]),\n",
    "    (\"thirty eight plus twenty four\", [START_OF_SEQUENCE, \"sixty\", \"two\"]),\n",
    "    (\"thirty eight plus twenty three\", [START_OF_SEQUENCE, \"sixty\", \"one\"]),\n",
    "    (\"thirty eight plus twenty two\", [START_OF_SEQUENCE, \"sixty\"]),\n",
    "    (\"thirty eight plus twenty one\", [START_OF_SEQUENCE, \"fifty\", \"nine\"]),\n",
    "    (\"thirty eight plus twenty\", [START_OF_SEQUENCE, \"fifty\", \"eight\"]),\n",
    "    (\"thirty eight plus nineteen\", [START_OF_SEQUENCE, \"fifty\", \"seven\"]),\n",
    "    (\"thirty eight plus eighteen\", [START_OF_SEQUENCE, \"fifty\", \"six\"]),\n",
    "    (\"thirty eight plus seventeen\", [START_OF_SEQUENCE, \"fifty\", \"five\"]),\n",
    "]\n",
    "\n",
    "# Assume you have trained encoder and decoder models\n",
    "average_distance = evaluate_with_edit_distance(\n",
    "    test_data, encoder, decoder, word_to_index, index_to_word\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################\n",
    "\n",
    "## K-Fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = np.array([\"3 + 5\", \"12 / 4\", \"10 * 2\", \"15 - 7\", \"20 / 5\"])\n",
    "labels = np.array([\"8\", \"3\", \"20\", \"8\", \"4\"])\n",
    "\n",
    "# Define K-Fold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Splitting the data\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(data)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "    print(\"Train Indices:\", train_idx)\n",
    "    print(\"Test Indices:\", test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Placeholders for metrics\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(data)):\n",
    "    print(f\"Training Fold {fold + 1}\")\n",
    "    \n",
    "    # Split data\n",
    "    train_data, test_data = data[train_idx], data[test_idx]\n",
    "    train_labels, test_labels = labels[train_idx], labels[test_idx]\n",
    "    \n",
    "    # Prepare PyTorch Datasets and Dataloaders\n",
    "    train_dataset = TensorDataset(torch.tensor(train_data), torch.tensor(train_labels))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_data), torch.tensor(test_labels))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    # Initialize model, optimizer, criterion\n",
    "    model = Seq2SeqModel()  # Replace with your seq2seq model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train your model\n",
    "    train_seq2seq(model, train_loader, optimizer, criterion)\n",
    "\n",
    "    # Evaluate your model\n",
    "    test_loss = evaluate_seq2seq(model, test_loader, criterion)\n",
    "    print(f\"Fold {fold + 1} Loss: {test_loss:.4f}\")\n",
    "    fold_metrics.append(test_loss)\n",
    "\n",
    "# Average loss across folds\n",
    "average_loss = sum(fold_metrics) / len(fold_metrics)\n",
    "print(f\"Average Loss Across Folds: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_seq2seq(encoder, decoder, dataloader, max_target_len):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    references = []  # List of ground truth sequences\n",
    "    candidates = []  # List of predicted sequences\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq = input_seq.to(device)\n",
    "\n",
    "            # Encoder forward pass\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "\n",
    "            # Initialize decoder input (<SOS> token)\n",
    "            decoder_input = torch.tensor([word_to_index[\"<SOS>\"]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "\n",
    "            predictions = []\n",
    "            for t in range(max_target_len):\n",
    "                # Decoder forward pass\n",
    "                output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "\n",
    "                # Get the token with the highest probability\n",
    "                top_token = output.argmax(1)\n",
    "                predictions.append(top_token)\n",
    "\n",
    "                # Stop decoding if <EOS> token is generated\n",
    "                if top_token.item() == word_to_index[\"<EOS>\"]:\n",
    "                    break\n",
    "\n",
    "                decoder_input = top_token\n",
    "\n",
    "            # Convert predicted and target sequences to words\n",
    "            predicted_seq = [index_to_word[token.item()] for token in predictions]\n",
    "            target_seq_words = [[index_to_word[token.item()] for token in target_seq[0]]]\n",
    "\n",
    "            # Append to BLEU evaluation lists\n",
    "            references.append(target_seq_words)  # Ground truth\n",
    "            candidates.append(predicted_seq)  # Model predictions\n",
    "\n",
    "    # Calculate BLEU\n",
    "    smooth = SmoothingFunction().method1\n",
    "    bleu_score = corpus_bleu(references, candidates, smoothing_function=smooth)\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    return bleu_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
