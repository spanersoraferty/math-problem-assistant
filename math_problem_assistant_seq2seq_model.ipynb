{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Add our overview narrative here!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This **Encoder** class is a part of the larger sequence-to-sequence model designed for processing mathematical word problems using an **LSTM**. It takes input sequences (math questions) represented as token indices and converts them into dense vector embeddings using an embedding layer, allowing the model to capture meaningful semantic relationships between words. The embedded representations then pass through an LSTM layer, which extracts temporal dependencies and generates both hidden and cell states that encode contextual information about the question sequence. A dropout layer is applied to the embeddings to reduce overfitting and improve generalisation. Ultimately, the encoder produces a set of outputs from each time step and a final hidden representation, which the decoder later uses to generate responses (the expected answer to our math question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an LSTM. \n",
    "    Converts a sequence of token indices into a hidden representation \n",
    "    that will be used by the decoder for sequence generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialises the Encoder module.\n",
    "        Args:\n",
    "            input_size (int): The size of the input vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer converts token indices into dense vector representations.\n",
    "        # Instead of using raw word indices (which lack meaning), embeddings allow\n",
    "        # the LSTM to learn meaningful semantic relationships between words.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # LSTM layer processes embedded input sequences to generate hidden states.\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Dropout layer applied to embeddings to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing token indices for a batch of input sentences.\n",
    "        Returns:\n",
    "            outputs (Tensor): Encoder outputs at each time step.\n",
    "            hidden (Tensor): Final hidden state of the LSTM.\n",
    "            cell (Tensor): Final cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_seq)           # Convert input tokens into embeddings\n",
    "        embedded = self.dropout(embedded)             # Apply dropout before LSTM processing\n",
    "        outputs, (hidden, cell) = self.lstm(embedded) # Process embeddings through LSTM\n",
    "\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The **Attention** class implements **'Bahdanau attention'**, which dynamically calculates attention scores based on the decoder’s hidden state and the encoder’s outputs. Instead of relying on a fixed context vector, this method allows the decoder to focus on specific parts of the input sequence at each decoding step. The attention mechanism works by concatenating the decoder's hidden state with the encoder’s outputs, passing them through a linear transformation, and applying the tanh activation function. A learnable vector (**v**) helps compute alignment scores, which are then normalised using softmax to generate attention weights—these determine the importance of each encoder output when predicting the next token. A dropout layer is also applied to prevent overfitting. The output is a set of attention weights that guides the decoder in generating more context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Implements Bahdanau attention mechanism for a sequence-to-sequence model. \n",
    "    Dynamically computes attention scores based on the decoder’s hidden state \n",
    "    and the encoder’s outputs, allowing the decoder to focus on relevant \n",
    "    parts of the input sequence at each decoding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        initialises the attention mechanism.\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden state of the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        # Linear layer to compute alignment scores\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # Learnable vector for attention computation\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        # Dropout layer to regularise attention mechanism and prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Computes attention weights using Bahdanau's additive attention method.\n",
    "        Args:\n",
    "            hidden (Tensor): Decoder hidden state at the current time step.\n",
    "            encoder_outputs (Tensor): Encoder outputs at all time steps.\n",
    "        Returns:\n",
    "            attention_weights (Tensor): Softmax-normalised attention scores.\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        # Expand the decoder hidden state across the sequence length\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # Concatenate hidden state with encoder outputs to compute alignment scores\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # Apply dropout to energy scores before computing attention weights\n",
    "        energy = self.dropout(energy)\n",
    "        # Transpose energy tensor for compatibility with the attention vector\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        # Expand learned vector `v` across batch size\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        # Compute attention weights via matrix multiplication\n",
    "        attention_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        # Apply softmax to normalise scores across sequence length\n",
    "        return torch.softmax(attention_weights, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The Decoder class plays a crucial role in our sequence-to-sequence model designed to solve math word problems. When given a problem as an input sequence, the Encoder first processes it into a hidden representation, which the Decoder then uses to generate the solution step by step. At each step, the attention mechanism dynamically selects the most relevant parts of the encoded problem statement, allowing the Decoder to focus on specific numerical relationships and mathematical operations. The LSTM maintains contextual understanding across time steps, helping track dependencies between numbers and mathematical operators. As the model generates each token in the solution, it refines its prediction using previously computed values, making the process similar to how humans break down word problems into logical steps. This structure ensures that the model interprets and solves math problems contextually, rather than simply memorising formulas, enabling it to generalise across different problem types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Decoder for the sequence-to-sequence math problem assistant model using an \n",
    "    LSTM with Bahdanau attention. The decoder generates output tokens one by one \n",
    "    while dynamically focusing on relevant parts of the encoder’s outputs using \n",
    "    the attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        initialises the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        # Embedding layer converts token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # LSTM layer processes embeddings and maintains hidden state across timesteps\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Linear layer maps concatenated attention context & LSTM output to vocab space\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        # Dropout layer to regularise embeddings before passing them to LSTM\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        # Attention mechanism for dynamic focus on encoder outputs\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder.\n",
    "        Args:\n",
    "            input (Tensor): Current token input to the decoder.\n",
    "            hidden (Tensor): Previous hidden state from the LSTM.\n",
    "            cell (Tensor): Previous cell state from the LSTM.\n",
    "            encoder_outputs (Tensor): Encoder outputs from all timesteps.\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): Predicted token probabilities.\n",
    "            hidden (Tensor): Updated hidden state.\n",
    "            cell (Tensor): Updated cell state.\n",
    "            attention_weights (Tensor): Attention scores for each encoder timestep.\n",
    "        \"\"\"\n",
    "        # Expand input dimensions to match expected input shape for embedding\n",
    "        input = input.unsqueeze(1)  \n",
    "        # Convert token indices into dense embeddings & apply dropout for regularisation\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # Forward pass through LSTM to generate new hidden and cell states\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # Compute attention weights using the current hidden state and encoder outputs\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        # Apply attention: generate weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # Flatten tensors for the fully connected layer\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        # Generate token probabilities using concatenated LSTM output and attention context\n",
    "        output = self.fc(torch.cat((lstm_output, context), dim=1))\n",
    "\n",
    "        return output, hidden, cell, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic tokenisation and vocabulary Setup\n",
    "\n",
    "Here we set up the constants and tokenisation process for a sequence-to-sequence math problem assistant. It first defines special tokens **<SOS>**, **<EOS>**, and **<PAD>** to mark the start, end, and padding of sequences. Then, it creates a vocabulary mapping that converts words into numerical indices, allowing the model to process text as numbers. The reverse mapping (**index_to_word**) ensures that predictions can be decoded back into words. Finally, the script tokenises example input and target sequences, transforming for example, **\"two plus four\"** into a list of indices **[3, 4, 5]** and the target **\"equals six\"** into **[0, 6, 7, 1]**, ensuring that the decoder starts with <SOS> and ends with <EOS>. This setup enables the neural network to work with structured input-output pairs for training our expected math-solving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estalish constants\n",
    "START_OF_SEQUENCE_TOKEN = \"<SOS>\"\n",
    "END_OF_SEQUENCE_TOKEN = \"<EOS>\"\n",
    "PADDING_SEQUENCE_TOKEN = \"<PAD>\"\n",
    "\n",
    "# Tokenisation and vocab setup\n",
    "# Create a vocabulary mapping words to indices\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Example input and target sequences\n",
    "input_sentence = \"two plus four\"\n",
    "target_sentence = \"equals six\"\n",
    "\n",
    "# Test Tokenised input and targets\n",
    "input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "target_tokens = [word_to_index[START_OF_SEQUENCE_TOKEN]] + [word_to_index[word] for word in target_sentence.split()] + [word_to_index[END_OF_SEQUENCE_TOKEN]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokensied data\n",
    "\n",
    "This **BasicMathWordProblemDataset** class creates a custom PyTorch Dataset for training our sequence-to-sequence model that solves basic math word problems. It converts input and target sentences (our math questions) into lists of token indices using a predefined vocabulary (**word_to_index**). The target sequence is prepended with <SOS> and appended with <EOS> tokens to indicate the start and end of the output. The class implements essential dataset methods, notablu __len__() which returns the number of samples, while __getItem__() retrieves tokenised tensors for input and target sequences, ensuring compatibility with PyTorch models. Finally, the dataset is wrapped in a DataLoader, allowing efficient batch processing with shuffling to improve learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMathWordProblemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for tokenised math word problems.\n",
    "    This class converts input sentences into tokenised sequences using \n",
    "    a predefined vocabulary (`word_to_index`) and structures target \n",
    "    sequences with <SOS> (start-of-sequence) and <EOS> (end-of-sequence) \n",
    "    tokens for proper handling by the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_sentences, target_sentences, word_to_index):\n",
    "        \"\"\"\n",
    "        initialises the dataset by converting words into token indices.\n",
    "        Args:\n",
    "            input_sentences (list of str): List of math word problems (input).\n",
    "            target_sentences (list of str): Corresponding solutions (output).\n",
    "            word_to_index (dict): Mapping of words to numerical token indices.\n",
    "        \"\"\"\n",
    "        # Tokenise input sentences into numerical sequences\n",
    "        self.input_data = [[word_to_index[word] for word in sentence.split()] for sentence in input_sentences]\n",
    "        # Tokenise target sentences while adding special tokens <SOS> and <EOS>\n",
    "        self.target_data = [[word_to_index[START_OF_SEQUENCE_TOKEN]] + \n",
    "                            [word_to_index[word] for word in sentence.split()] + \n",
    "                            [word_to_index[END_OF_SEQUENCE_TOKEN]] for sentence in target_sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Returns:\n",
    "            int: Total number of input-target pairs.\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the tokenised input and target sequences as PyTorch tensors.\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: Tokenised input and target sequences.\n",
    "        \"\"\"\n",
    "        return torch.tensor(self.input_data[idx], dtype=torch.long), torch.tensor(self.target_data[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "input_sentences = [\"two plus four\"]\n",
    "target_sentences = [\"equals six\"]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BasicMathWordProblemDataset(input_sentences, target_sentences, word_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparameters\n",
    "\n",
    "Here we initialise the key components for training our sequence-to-sequence math word problem solving model. The encoder and decoder are instantiated with the vocabulary size (**input_size** and **output_size**) and a hidden state of **128**, allowing the model to process and generate our math solutions. The **CrossEntropy loss function** is used with padding tokens ignored to prevent unnecessary calculations from affecting training. Adam optimisers are applied to both the encoder and decoder with a learning rate of **0.001**, enabling stable gradient updates. Finally, an attention matrix is initialised to store attention weights, which will later be used for visualisation, thus helping to analyse how the model focuses on different parts of the input when generating solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "input_size = len(word_to_index)  # Total vocabulary size\n",
    "output_size = len(word_to_index)  # Vocabulary size\n",
    "hidden_size = 128\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "# Ignore padding tokens\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN])  \n",
    "encoder_optimiser = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimiser = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# Store attention weights for visualisation\n",
    "attention_matrix = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss over epochs\n",
    "\n",
    "This function visualises the training loss over epochs using Matplotlib. It takes a list of loss values (**epoch_losses**), representing the loss at each epoch, and plots them on a graph to track performance trends over time. This is useful for monitoring model convergence and diagnosing potential issues such as overfitting or slow learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(epoch_losses):\n",
    "    \"\"\"\n",
    "    Plots the training loss over epochs.\n",
    "    Args:\n",
    "        epoch_losses (list): A list containing the loss values for each epoch.\n",
    "    \"\"\"\n",
    "    # Create a figure with a predefined size for better readability\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Plot the loss values over epochs with markers for each epoch\n",
    "    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', label='Training Loss')\n",
    "    # Label the x-axis to indicate the epoch number\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    # Label the y-axis to indicate the loss value\n",
    "    plt.ylabel(\"Loss\")\n",
    "    # Add a title to the plot for better context\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    # Display a legend to clarify the plotted line\n",
    "    plt.legend()\n",
    "    # Add a grid to improve readability of the trend\n",
    "    plt.grid()\n",
    "    # Show the final plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the training regime\n",
    "\n",
    "This **train()** function is responsible for training our sequence-to-sequence model with an encoder-decoder architecture with **'Bahdanau attention'**. It iterates through a dataset of tokenised math problems for multiple epochs, processing each batch by passing inputs through the encoder, which generates a context vector. The decoder then predicts output tokens step by step, using teacher forcing to guide learning while dynamically focusing on relevant parts of the input via attention mechanisms. The function tracks masked loss, ensuring that padding tokens don't affect optimisation, and updates model weights using backpropagation with **Adam optimisers**. After training, it stores attention weights for visualisation, logs epoch losses, plots them for monitoring, and saves the trained models to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, encoder_save_name, decoder_save_name):\n",
    "    \"\"\"\n",
    "    Trains the sequence-to-sequence model using an encoder-decoder architecture.\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        encoder_save_name (str): File name for saving the trained encoder model.\n",
    "        decoder_save_name (str): File name for saving the trained decoder model.\n",
    "    \"\"\"\n",
    "    epoch_losses = []  # Stores loss values for tracking progress\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0  # Accumulate total loss for the epoch\n",
    "\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            # Move data to GPU if available\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            # Reset gradients before each batch\n",
    "            encoder_optimiser.zero_grad()\n",
    "            decoder_optimiser.zero_grad()\n",
    "            # Encoder forward pass - processes input sequences to generate hidden states\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "            # Decoder initialisation - starts with the encoder's final hidden state\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "            # Get actual sequence lengths (excluding padding)\n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()\n",
    "\n",
    "            loss = 0  # Tracks batch loss\n",
    "            \n",
    "            # Iterate through target sequence timesteps\n",
    "            for t in range(max_target_length):\n",
    "                # Determine active sequences\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():  # If all sequences finished, break loop\n",
    "                    break\n",
    "\n",
    "                # Decoder forward pass - generates output token probabilities\n",
    "                output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "                # Masked loss calculation - only consider active sequences\n",
    "                loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "                # Teacher forcing: Use actual target token as next input\n",
    "                decoder_input = target_seq[:, t]  \n",
    "\n",
    "            # Backpropagation - compute gradients and update model parameters\n",
    "            loss.backward()\n",
    "            encoder_optimiser.step()\n",
    "            decoder_optimiser.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Store normalised loss\n",
    "        epoch_losses.append(epoch_loss / len(dataloader))\n",
    "        # Print training progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_losses[-1]:.4f}\")\n",
    "        # Store attention weights for visualisation\n",
    "        attention_matrix.append(attention_weights.cpu().detach().numpy())\n",
    "\n",
    "    # Plot training loss\n",
    "    plot_loss(epoch_losses)\n",
    "\n",
    "    # Save the trained models\n",
    "    torch.save(encoder.state_dict(), f\"{encoder_save_name}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"{decoder_save_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We'll start training for **100** epochs to refine our sequence-to-sequence model. This will help our encoder-decoder system learn better representations and improve predictions for solving math word problems. We'll expect to see loss decreasing over time, but if it stagnates or spikes, we can adjust learning rate, batch size, and/or other hyperparameters to stabilise training. Also, our ability to monitor attention weights will help us verify whether the model is correctly focusing on key parts of the input during decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 9.3534\n",
      "Epoch 2/100, Loss: 8.7412\n",
      "Epoch 3/100, Loss: 8.1829\n",
      "Epoch 4/100, Loss: 7.4364\n",
      "Epoch 5/100, Loss: 6.9117\n",
      "Epoch 6/100, Loss: 6.3252\n",
      "Epoch 7/100, Loss: 5.6645\n",
      "Epoch 8/100, Loss: 5.1851\n",
      "Epoch 9/100, Loss: 4.7948\n",
      "Epoch 10/100, Loss: 4.2927\n",
      "Epoch 11/100, Loss: 3.9033\n",
      "Epoch 12/100, Loss: 3.4851\n",
      "Epoch 13/100, Loss: 3.1078\n",
      "Epoch 14/100, Loss: 2.9039\n",
      "Epoch 15/100, Loss: 2.6850\n",
      "Epoch 16/100, Loss: 2.4377\n",
      "Epoch 17/100, Loss: 2.2098\n",
      "Epoch 18/100, Loss: 1.9943\n",
      "Epoch 19/100, Loss: 1.8687\n",
      "Epoch 20/100, Loss: 1.7045\n",
      "Epoch 21/100, Loss: 1.5587\n",
      "Epoch 22/100, Loss: 1.3985\n",
      "Epoch 23/100, Loss: 1.3172\n",
      "Epoch 24/100, Loss: 1.1810\n",
      "Epoch 25/100, Loss: 1.1043\n",
      "Epoch 26/100, Loss: 1.0400\n",
      "Epoch 27/100, Loss: 0.9160\n",
      "Epoch 28/100, Loss: 0.8428\n",
      "Epoch 29/100, Loss: 0.7614\n",
      "Epoch 30/100, Loss: 0.7086\n",
      "Epoch 31/100, Loss: 0.6691\n",
      "Epoch 32/100, Loss: 0.6157\n",
      "Epoch 33/100, Loss: 0.5572\n",
      "Epoch 34/100, Loss: 0.5050\n",
      "Epoch 35/100, Loss: 0.4716\n",
      "Epoch 36/100, Loss: 0.4321\n",
      "Epoch 37/100, Loss: 0.3853\n",
      "Epoch 38/100, Loss: 0.3623\n",
      "Epoch 39/100, Loss: 0.3363\n",
      "Epoch 40/100, Loss: 0.3209\n",
      "Epoch 41/100, Loss: 0.2844\n",
      "Epoch 42/100, Loss: 0.2677\n",
      "Epoch 43/100, Loss: 0.2604\n",
      "Epoch 44/100, Loss: 0.2299\n",
      "Epoch 45/100, Loss: 0.2132\n",
      "Epoch 46/100, Loss: 0.1986\n",
      "Epoch 47/100, Loss: 0.1906\n",
      "Epoch 48/100, Loss: 0.1769\n",
      "Epoch 49/100, Loss: 0.1680\n",
      "Epoch 50/100, Loss: 0.1555\n",
      "Epoch 51/100, Loss: 0.1496\n",
      "Epoch 52/100, Loss: 0.1387\n",
      "Epoch 53/100, Loss: 0.1293\n",
      "Epoch 54/100, Loss: 0.1246\n",
      "Epoch 55/100, Loss: 0.1172\n",
      "Epoch 56/100, Loss: 0.1150\n",
      "Epoch 57/100, Loss: 0.1113\n",
      "Epoch 58/100, Loss: 0.1034\n",
      "Epoch 59/100, Loss: 0.1013\n",
      "Epoch 60/100, Loss: 0.0932\n",
      "Epoch 61/100, Loss: 0.0898\n",
      "Epoch 62/100, Loss: 0.0890\n",
      "Epoch 63/100, Loss: 0.0842\n",
      "Epoch 64/100, Loss: 0.0795\n",
      "Epoch 65/100, Loss: 0.0765\n",
      "Epoch 66/100, Loss: 0.0729\n",
      "Epoch 67/100, Loss: 0.0719\n",
      "Epoch 68/100, Loss: 0.0684\n",
      "Epoch 69/100, Loss: 0.0657\n",
      "Epoch 70/100, Loss: 0.0648\n",
      "Epoch 71/100, Loss: 0.0622\n",
      "Epoch 72/100, Loss: 0.0611\n",
      "Epoch 73/100, Loss: 0.0593\n",
      "Epoch 74/100, Loss: 0.0586\n",
      "Epoch 75/100, Loss: 0.0543\n",
      "Epoch 76/100, Loss: 0.0557\n",
      "Epoch 77/100, Loss: 0.0510\n",
      "Epoch 78/100, Loss: 0.0518\n",
      "Epoch 79/100, Loss: 0.0508\n",
      "Epoch 80/100, Loss: 0.0497\n",
      "Epoch 81/100, Loss: 0.0474\n",
      "Epoch 82/100, Loss: 0.0477\n",
      "Epoch 83/100, Loss: 0.0453\n",
      "Epoch 84/100, Loss: 0.0447\n",
      "Epoch 85/100, Loss: 0.0444\n",
      "Epoch 86/100, Loss: 0.0426\n",
      "Epoch 87/100, Loss: 0.0412\n",
      "Epoch 88/100, Loss: 0.0415\n",
      "Epoch 89/100, Loss: 0.0410\n",
      "Epoch 90/100, Loss: 0.0391\n",
      "Epoch 91/100, Loss: 0.0398\n",
      "Epoch 92/100, Loss: 0.0397\n",
      "Epoch 93/100, Loss: 0.0378\n",
      "Epoch 94/100, Loss: 0.0366\n",
      "Epoch 95/100, Loss: 0.0356\n",
      "Epoch 96/100, Loss: 0.0360\n",
      "Epoch 97/100, Loss: 0.0345\n",
      "Epoch 98/100, Loss: 0.0338\n",
      "Epoch 99/100, Loss: 0.0336\n",
      "Epoch 100/100, Loss: 0.0330\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"basic_math_problem_encoder\", \"basic_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate heatmap\n",
    "\n",
    "This function allows us to visualise attention weights as a **heatmap**, helping us analyse which input tokens the decoder focuses on while generating each output token in our model. Using Matplotlib, it plots the attention_matrix as a color-coded intensity map, where darker shades indicate stronger attention at specific positions. This visualisation is helpful for allowing us to understand how our model prioritises different parts of the input, making it inciteful when attempting to optimise our attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention(input_tokens, output_tokens, attention_matrix):\n",
    "    \"\"\"\n",
    "    Plot the attention weights as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        input_tokens (list of str): Tokens in the input sequence.\n",
    "        output_tokens (list of str): Tokens in the output sequence.\n",
    "        attention_matrix (np.array): Attention weights matrix (output_tokens x input_tokens).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attention_matrix, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=np.arange(len(input_tokens)), labels=input_tokens, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks=np.arange(len(output_tokens)), labels=output_tokens)\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.xlabel(\"Input Tokens\")\n",
    "    plt.ylabel(\"Output Tokens\")\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup\n",
    "\n",
    "This function allow us to test our trained model by encoding our math question input sentence into hidden states and decoding it step-by-step to generate our expected answer. It tokenises the input using a predefined vocabulary and passes it through the encoder, which extracts a contextual representation. The decoder then predicts each output token using an iterative approach, incorporating attention weights to dynamically focus on relevant parts of the input. The function tracks attention matrices, enabling visualisation of how the model distributes focus across tokens. It prints the input sentence, generated output, and attention shape, and optionally plots a heatmap for deeper analysis. The final output is a list of predicted words, representing the answer to our math problem, which can then be evaluated for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=100, with_attention_plot=False):\n",
    "    # Set the models to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    # Tokenise the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "    # Forward pass through the encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "    # initialise the decoder\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]], dtype=torch.long)  # Start-of-Sequence token\n",
    "    decoder_hidden = hidden\n",
    "    decoder_cell = cell\n",
    "    # Generate output sequence and collect attention weights\n",
    "    output_sequence = []\n",
    "    attention_matrices = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get the token with the highest probability\n",
    "\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE_TOKEN]:  # Stop at End-of-Sequence token\n",
    "            break\n",
    "\n",
    "        output_sequence.append(predicted_token)\n",
    "        attention_matrices.append(attention_weights.cpu().detach().numpy())  # Save attention weights\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)  # Next decoder input\n",
    "\n",
    "    # Convert input and output tokens to words\n",
    "    input_sentence_tokens = [index_to_word[token] for token in input_tokens]\n",
    "    output_sentence_tokens = [index_to_word[token] for token in output_sequence]\n",
    "    # Stack attention matrices into a 2D array (output_tokens x input_tokens)\n",
    "    attention_matrix = np.vstack(attention_matrices)\n",
    "    # Print the input and output sentences\n",
    "    print(\"Input Sentence:\", input_sentence_tokens)\n",
    "    print(\"Generated Sentence:\", output_sentence_tokens)\n",
    "    print(\"Attention Weights Shape:\", attention_matrix)\n",
    "    # Visualise attention\n",
    "    if with_attention_plot:\n",
    "        # Plot the attention weights\n",
    "        plot_attention(input_sentence_tokens, output_sentence_tokens, attention_matrix)\n",
    "\n",
    "    # Convert token indices to words\n",
    "    return [index_to_word[token] for token in output_sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model - basic approach\n",
    "\n",
    "Here we load our pre-trained sequence-to-sequence model for solving basic math word problems. We first initialises the encoder and decoder architectures with the same parameters used during training, ensuring consistency. Then, we load the pretrained model weights from the associated **.pth** files, restoring previously learned knowledge. After defining a sample input sentence, **\"two plus four\"**, we set up **word-to-index** and **index-to-word** mappings, allowing tokenisation and conversion between words and numerical indices. Finally, the test function is called, running the encoder-decoder model on the input question to generate a predicted answer while optionally visualising attention weights so we can attempt analyse how the model focuses on different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ['two', 'plus', 'four']\n",
      "Generated Sentence: ['<SOS>', 'equals', 'six']\n",
      "Attention Weights Shape: [[5.8233423e-09 1.0000000e+00 2.8802999e-10]\n",
      " [1.6224634e-08 1.0000000e+00 8.5898072e-10]\n",
      " [8.4138581e-09 1.0000000e+00 4.7569887e-10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<SOS>', 'equals', 'six']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 128  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"basic_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"basic_math_problem_decoder.pth\"))\n",
    "\n",
    "input_sentence = \"two plus four\"\n",
    "\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Test and visualise\n",
    "test(encoder, decoder, input_sentence, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Establish a more complex dataset and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline vocabulary and tokenisation capability\n",
    "\n",
    "Here we set up a dynamic tokenisation function for our model by defining a baseline vocabulary containing special tokens (<SOS>, <EOS>, <PAD>). The function processes an input sentence by splitting words, checking if they already exist in the **word_to_index** mapping, and assigning a new index to any unseen words. This ensures that the vocabulary expands dynamically as new words are encountered, allowing the model to handle previously unknown input without predefined restrictions, with the function ultimately generating a list of token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens for sequence processing - initially, the vocabulary only contains special tokens (<SOS>, <EOS>, <PAD>)\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2}  \n",
    "\n",
    "def tokenise(sentence, word_to_index):\n",
    "    \"\"\"\n",
    "    tokenises a sentence and dynamically updates the vocabulary mapping.\n",
    "    Args:\n",
    "        sentence (str): The input sentence to be tokenised.\n",
    "        word_to_index (dict): Dictionary mapping words to unique indices.\n",
    "    Returns:\n",
    "        list: A list of token indices representing the sentence.\n",
    "    \"\"\"\n",
    "    tokens = []  # Stores tokenised word indices\n",
    "\n",
    "    # Process each word in the sentence\n",
    "    for word in sentence.lower().split():\n",
    "        # Add unseen words to the vocabulary and assign a unique index\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)  \n",
    "        \n",
    "        # Append the tokenised index to the token list\n",
    "        tokens.append(word_to_index[word])\n",
    "\n",
    "    return tokens  # Return tokenised sentence as a list of indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV dataset from file\n",
    "\n",
    "Here we load a dataset of math questions and their associated answers from our previously generated CSV file, using Pandas. The function reads the CSV file into a DataFrame, extracts the \"Question\" column (math word question) and \"Answer\" column (corresponding answers), and returns them as lists. The script then specifies **\"simple_math_problems_addition_only.csv\"** as the dataset and calls the function to populate **input_sentences** and **target_sentences**, which can be tokenised and fed into the model for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_sequences_from_csv(csv_file):\n",
    "    \"\"\"\n",
    "    Loads math problem sequences from a CSV file.\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing math problems and solutions.\n",
    "    Returns:\n",
    "        tuple: Two lists containing problem statements and their corresponding solutions.\n",
    "    \"\"\"\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Extract the \"Problem\" column as a list of math problems\n",
    "    input_sentences = df[\"Problem\"].tolist()\n",
    "    # Extract the \"Solution\" column as a list of corresponding answers\n",
    "    target_sentences = df[\"Solution\"].tolist()\n",
    "    return input_sentences, target_sentences  # Return both lists for processing\n",
    "\n",
    "# Specify the dataset file name\n",
    "csv_file = \"simple_math_problems_addition_only.csv\"\n",
    "# Load problem and solution sequences from the CSV file\n",
    "input_sentences, target_sentences = load_sequences_from_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise input and target sentences and convert into tensors\n",
    "\n",
    "We now tokenise the question and answer sentences and convert them into PyTorch tensors, preparing them for processing in a sequence-to-sequence model. It first transforms input sentences into numerical token lists using a predefined vocabulary (**word_to_index**). For target sentences, thus ensuring a structured decoding by appending **<SOS>** at the beginning and **<SOS>** at the end, guiding the model in output generation. A reverse mapping (**index_to_word**) is also created, allowing numerical tokens to be converted back into words for interpretation. Finally, both input and target tokenised sequences are converted into PyTorch tensors, ensuring they are formatted correctly for our models training and inference requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise input sentences by converting words to numerical indices\n",
    "input_data = [tokenise(sentence, word_to_index) for sentence in input_sentences]\n",
    "\n",
    "# tokenise target sentences while adding <SOS> (start token) and <EOS> (end token)\n",
    "target_data = [[word_to_index[START_OF_SEQUENCE_TOKEN]] + \n",
    "               tokenise(sentence, word_to_index) + \n",
    "               [word_to_index[END_OF_SEQUENCE_TOKEN]] \n",
    "               for sentence in target_sentences]\n",
    "\n",
    "# Create a reverse mapping dictionary to convert token indices back into words\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "\n",
    "# Convert tokenised sentences into PyTorch tensors for model compatibility\n",
    "input_tensors = [torch.tensor(seq) for seq in input_data]  # Convert input sequences to tensors\n",
    "target_tensors = [torch.tensor(seq) for seq in target_data]  # Convert target sequences to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokensied data, including padding\n",
    "\n",
    "This code prepares a dataset of math word problems for our sequence-to-sequence learning model, ensuring uniform sequence lengths using dynamic padding. The input and target sequences are padded with a special token **<PAD>** to maintain consistency in batch processing. The **DynamicMathWordProblemDataset** class organises the padded data into a PyTorch-compatible format, allowing efficient access to input-target pairs. The dataset is then wrapped in a PyTorch , which facilitates batch-wise loading of data, enabling smooth training with a batch size of **64**. By disabling shuffling, the sequences maintain their original order, optimising structured learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dynamic padding to ensure uniform sequence length\n",
    "input_padded = pad_sequence(input_tensors, padding_value=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "target_padded = pad_sequence(target_tensors, padding_value=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "\n",
    "class DynamicMathWordProblemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for padded math word problems.\n",
    "    This class stores input and target sequences that have been padded \n",
    "    to a fixed length, ensuring consistency in batch processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_padded, target_padded):\n",
    "        \"\"\"\n",
    "        initialises the dataset with padded sequences.\n",
    "        Args:\n",
    "            input_padded (Tensor): Padded input sequences.\n",
    "            target_padded (Tensor): Padded target sequences.\n",
    "        \"\"\"\n",
    "        self.input_data = input_padded  # Store padded input sequences\n",
    "        self.target_data = target_padded  # Store padded target sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Returns:\n",
    "            int: Total number of input-target pairs.\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a padded input-target pair as tensors.\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: Padded input and target sequences.\n",
    "        \"\"\"\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "# Create a dataset instance using the padded sequences\n",
    "dataset = DynamicMathWordProblemDataset(input_padded, target_padded)\n",
    "# initialise a DataLoader for batch processing\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)  # Batch size of 64, shuffle disabled for sequential processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters\n",
    "\n",
    "We now set-up the hyperparameters for our trainining model. We first initialise the encoder and decoder models with a hidden size of **256**, then a **cross-entropy loss function** is defined with **ignore_index**, ensuring that our padding tokens do not affect training. Both models use the Adam optimiser with a learning rate of **0.0005** to adjust weights and improve performance. Finally, the we define training for **100** epochs, allowing saving of the learned model parameters under specified filenames for later inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define model parameters\n",
    "input_size = len(word_to_index)  # Vocabulary size for the encoder\n",
    "output_size = len(word_to_index)  # Vocabulary size for the decoder (same as input)\n",
    "hidden_size = 256  # Number of hidden units in encoder and decoder\n",
    "# initialise the encoder and move it to the selected device\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "# initialise the decoder and move it to the selected device\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "# Define the loss function (CrossEntropyLoss), ignoring padding tokens during training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN]) \n",
    "# Set up Adam optimisers for both encoder and decoder with a learning rate of 0.0005\n",
    "encoder_optimiser = optim.Adam(encoder.parameters(), lr=0.0005)\n",
    "decoder_optimiser = optim.Adam(decoder.parameters(), lr=0.0005)\n",
    "# Define the number of epochs for training\n",
    "num_epochs = 100  \n",
    "# Start the training process with specified epoch count and model save names\n",
    "train(num_epochs, \"addition_only_math_problem_encoder\", \"addition_only_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the revised model\n",
    "\n",
    "We'll load our previously trained model, restoring the encoder and decoder models using their saved weights. Next we initialise the models with our predefined hyperparameters and load the previously trained model states using **torch.load()**. We then setup a question input (**\"thirty-eight plus twenty-six\"**) to test the model's ability to predict the correct outputted answer (**\"sixty-four\"**). The **test()** function runs inference, generating predictions from the trained model and optionally visualising the attention mechanism, allowing inspection of a heatmap that focuses on different parts of the input when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256  # Same as during training\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"addition_only_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"addition_only_math_problem_decoder.pth\"))\n",
    "#thirty eight plus twenty seven,sixty five\n",
    "input_sentence = \"thirty eight plus twenty six\" #sixty four\n",
    "# Test and visualise\n",
    "test_token_indices_to_words = test(encoder, decoder, input_sentence, word_to_index, index_to_word, with_attention_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the test predictions for evaluation\n",
    "\n",
    "We look to generate predictions using our trained sequence-to-sequence model and evaluate its performance using 'edit distance'. Edit distance is a measure of how different our sequences are from each other, calculated by the minimum number of operations required to transform one sequence into another. A lower edit distance indicates that the two sequences are highly similar, while a higher edit distance suggests greater differences. The **generate_predictions** function processes an input question, tokenises it into indices, and runs it through the encoder to obtain hidden states, which are then passed to the decoder. The decoder iteratively predicts the answer tokens, stopping when the **<EOS>** token is encountered. The predicted answer tokens are converted back into words for interpretation. The **evaluate_with_edit_distance** function allows us to compare generated outputs with ground truth sentences by computing the edit distance, which quantifies the differences between predictions and expected answers. The final output is the average edit distance, helping us to assess model accuracy when solving math word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edit_distance\n",
    "\n",
    "def generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=10):\n",
    "    \"\"\"\n",
    "    Generates a predicted output sequence for a given input sentence using a trained encoder-decoder model.\n",
    "    Args:\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        input_sentence (str): The input sentence to be processed.\n",
    "        word_to_index (dict): Mapping of words to numerical token indices.\n",
    "        index_to_word (dict): Reverse mapping of indices to words.\n",
    "        max_target_length (int): Maximum length of the output sequence.\n",
    "    Returns:\n",
    "        list: A list of words representing the predicted output sequence.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "\n",
    "    # tokenise the input sentence by converting words to numerical token indices\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "    # Forward pass through the encoder to obtain hidden and cell states\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "    # initialise decoder with <SOS> token as the first input\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]], dtype=torch.long)\n",
    "    decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs to initialise decoder states\n",
    "    \n",
    "    # Generate output sequence iteratively\n",
    "    output_sequence = []\n",
    "    for _ in range(max_target_length):\n",
    "        # Pass current decoder input through the decoder\n",
    "        output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        # Get the token with the highest probability from the decoder output\n",
    "        predicted_token = output.argmax(1).item()\n",
    "        # Stop generation if <EOS> token is predicted\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE_TOKEN]:\n",
    "            break\n",
    "        # Append the predicted token to the output sequence\n",
    "        output_sequence.append(predicted_token)\n",
    "        # Use predicted token as the next decoder input\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert token indices back into words for interpretation\n",
    "    return [index_to_word[token] for token in output_sequence]\n",
    "\n",
    "\n",
    "def evaluate_with_edit_distance(test_data, encoder, decoder, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Evaluates the sequence-to-sequence model using edit distance.\n",
    "    Args:\n",
    "        test_data (list): List of (input_sentence, ground_truth_sentence) pairs.\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        word_to_index (dict): Word-to-index mapping.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "    Returns:\n",
    "        float: Average edit distance across the test dataset.\n",
    "    \"\"\"\n",
    "    total_distance = 0  # initialise total edit distance accumulator\n",
    "    for input_sentence, ground_truth_sentence in test_data:\n",
    "        # Generate predicted sequence for the input sentence\n",
    "        predicted_sentence = generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word)\n",
    "        # Compute edit distance between ground truth and predicted sentence\n",
    "        distance = edit_distance.SequenceMatcher(a=ground_truth_sentence, b=predicted_sentence).distance()\n",
    "        total_distance += distance  # Accumulate total distance\n",
    "        # Debugging: Print example results for verification\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Ground Truth: {ground_truth_sentence}\")\n",
    "        print(f\"Predicted: {predicted_sentence}\")\n",
    "        print(f\"Edit Distance: {distance}\\n\")\n",
    "\n",
    "    # Compute average edit distance across the dataset\n",
    "    average_distance = total_distance / len(test_data)\n",
    "    print(f\"Average Edit Distance: {average_distance}\")\n",
    "\n",
    "    return average_distance  # Return evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using edit distance\n",
    "\n",
    "We'll setup a small test dataset consisting of math word problems and their expected answers, formatted as tokenised sequences with a start token (**<SOS**). We'll evaluate the trained model using 'edit distance' (as described above). Each test case follows a structured pattern where a simple math question is presented in text, and associated with the corresponding answer, again in text format. The **evaluate_with_edit_distance** is called to generate answer predictions and compare them against the ground truth answers. Finally, we calculate the average edit distance across all test cases, with the aim of assessing model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an example test dataset with math addition problems and expected solutions\n",
    "test_data = [\n",
    "    (\"thirty eight plus twenty six\", [START_OF_SEQUENCE_TOKEN, \"sixty\", \"four\"]),   # Expected output: \"sixty four\"\n",
    "    (\"thirty eight plus twenty five\", [START_OF_SEQUENCE_TOKEN, \"sixty\", \"three\"]), # Expected output: \"sixty three\"\n",
    "    (\"thirty eight plus twenty four\", [START_OF_SEQUENCE_TOKEN, \"sixty\", \"two\"]),   # Expected output: \"sixty two\"\n",
    "    (\"thirty eight plus twenty three\", [START_OF_SEQUENCE_TOKEN, \"sixty\", \"one\"]),  # Expected output: \"sixty one\"\n",
    "    (\"thirty eight plus twenty two\", [START_OF_SEQUENCE_TOKEN, \"sixty\"]),           # Expected output: \"sixty\"\n",
    "    (\"thirty eight plus twenty one\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"nine\"]),   # Expected output: \"fifty nine\"\n",
    "    (\"thirty eight plus twenty\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"eight\"]),      # Expected output: \"fifty eight\"\n",
    "    (\"thirty eight plus nineteen\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"seven\"]),    # Expected output: \"fifty seven\"\n",
    "    (\"thirty eight plus eighteen\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"six\"]),      # Expected output: \"fifty six\"\n",
    "    (\"thirty eight plus seventeen\", [START_OF_SEQUENCE_TOKEN, \"fifty\", \"five\"]),    # Expected output: \"fifty five\"\n",
    "]\n",
    "\n",
    "# Evaluate the trained sequence-to-sequence model using edit distance\n",
    "average_distance = evaluate_with_edit_distance(\n",
    "    test_data, encoder, decoder, word_to_index, index_to_word\n",
    ")\n",
    "\n",
    "# Print the final average edit distance to assess model performance\n",
    "print(f\"Final Average Edit Distance: {average_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add positional encodings to our encode model\n",
    "\n",
    "We look to further extend our original encoder model with the addition of a positional encoding layer to inject sequential information. Positional encodings allow us to enforce built-in order awareness, ensuring that questions and answers maintain their relative positioning within a given sequence.\n",
    "Thes encodings are implemented using learned embeddings, which generate continuous patterns that help the model differentiate token positions. The intention here is to ensure the model can effectively understand and retain the order of words, improving context understanding, translation accuracy, and sentence structure in an attempt to improve training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM-based encoder with positional encodings for sequence-to-sequence tasks.\n",
    "    This model encodes input sequences using embeddings, positional encodings, \n",
    "    and a stacked LSTM, returning hidden and cell states for use by a decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1, max_seq_len=100):\n",
    "        \"\"\"\n",
    "        initialises the encoder model.\n",
    "        Args:\n",
    "            input_size (int): Vocabulary size (number of unique tokens).\n",
    "            hidden_size (int): Dimensionality of hidden states in the LSTM.\n",
    "            dropout (float): Dropout rate for regularisation.\n",
    "            max_seq_len (int): Maximum sequence length for positional encodings.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer to convert token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Positional encodings to retain word order information\n",
    "        self.positional_encodings = nn.Embedding(max_seq_len, hidden_size)\n",
    "        # LSTM layer for sequential processing with batch support\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Dropout for regularisation to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing input token indices with shape (batch_size, seq_length).\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tuple[Tensor, Tensor]]: LSTM outputs and final hidden & cell states.\n",
    "        \"\"\"\n",
    "        # Generate positional indices for the input sequence\n",
    "        positions = torch.arange(0, input_seq.size(1), device=input_seq.device).unsqueeze(0)\n",
    "        # Retrieve positional encodings for each token position\n",
    "        positional_enc = self.positional_encodings(positions)\n",
    "        # Compute embeddings, add positional encodings, and apply dropout\n",
    "        embedded = self.dropout(self.embedding(input_seq) + positional_enc)\n",
    "        # Process embedded input sequence through the LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return outputs, (hidden, cell)  # Return LSTM outputs and final hidden/cell states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters using Opuna\n",
    "\n",
    "This code utilises Optuna, a powerful hyperparameter optimisation framework, to fine-tune an encoder-decoder model for sequence-to-sequence tasks. The **objective** function defines the hyperparameters to be optimised, including hidden size, learning rate, and dropout rate, which are sampled across a predefined range. It then initialises the encoder and decoder models with these parameters, applies dropout regularisation, and sets up cross-entropy loss while ignoring padding tokens. The models are trained for one epoch with teacher forcing, where the actual target sequence is used as input to the decoder, helping accelerate learning. The script runs 30 trials, minimising loss to find the best-performing hyperparameter combination, which is then printed for further experimentation. We aslo provide visualisations of the hyperparameter tuning process, which should help to identify trends in model performance based on different parameter values, with plots including an optimisation history graph that tracks how loss decreases across trials, a hyperparameter importance chart highlighting which parameters most affect performance, and a loss vs. hyperparameter relationships plot to visualise individual parameter effects. The final visualisation offers insights into how different hidden sizes, learning rates, and dropout rates impact model accuracy, enabling informed decisions on optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "# Objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines the optimisation objective for hyperparameter tuning using Optuna.\n",
    "    Args:\n",
    "        trial (optuna.Trial): A single optimisation trial where hyperparameters are sampled.\n",
    "    Returns:\n",
    "        float: The averaged loss computed over one epoch, which Optuna minimises.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters for tuning\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 128, 512)  # Hidden layer size\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)  # Log-scaled learning rate\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate for regularisation\n",
    "    # initialise encoder-decoder models with suggested hyperparameters\n",
    "    encoder = Encoder(input_size, hidden_size).to(device)\n",
    "    decoder = Decoder(output_size, hidden_size).to(device)\n",
    "    # Apply dropout in LSTM layers for both models\n",
    "    encoder.lstm.dropout = dropout_rate\n",
    "    decoder.lstm.dropout = dropout_rate\n",
    "    # Define loss function, ignoring padding tokens in label sequences\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "    # Set up Adam optimisers with the suggested learning rate\n",
    "    encoder_optimiser = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimiser = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    # initialise total loss for tracking performance\n",
    "    total_loss = 0\n",
    "\n",
    "    # Run one epoch of training to evaluate hyperparameter performance\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        # Move data to the selected device (CPU/GPU)\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        # Reset gradients before forward pass\n",
    "        encoder_optimiser.zero_grad()\n",
    "        decoder_optimiser.zero_grad()\n",
    "        # Forward pass through encoder to obtain initial hidden states\n",
    "        encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "        # initialise decoder input with <SOS> token for each sequence in batch\n",
    "        decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "        decoder_hidden, decoder_cell = hidden, cell  # Set decoder states from encoder outputs\n",
    "\n",
    "        loss = 0  # Track cumulative loss across target sequence\n",
    "\n",
    "        for t in range(target_seq.size(1)):  # Iterate over target sequence length\n",
    "            # Forward pass through decoder\n",
    "            output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "            )\n",
    "\n",
    "            # Compute loss for current timestep\n",
    "            loss += criterion(output, target_seq[:, t])\n",
    "            # Apply teacher forcing: use ground-truth target as next input\n",
    "            decoder_input = target_seq[:, t]\n",
    "\n",
    "        # Backpropagate loss and update optimiser\n",
    "        loss.backward()\n",
    "        encoder_optimiser.step()\n",
    "        decoder_optimiser.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "    return total_loss / len(dataloader)  # Return average loss per batch for Optuna optimisation\n",
    "\n",
    "# Create Optuna study for hyperparameter tuning (minimising loss)\n",
    "study = optuna.create_study(direction=\"minimise\")\n",
    "study.optimise(objective, n_trials=30)  # Run 30 trials to find the best parameters\n",
    "# Extract best-performing hyperparameters from optimisation results\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "# Store best hyperparameter values for model training\n",
    "best_hidden_size = best_params[\"hidden_size\"]\n",
    "best_learning_rate = best_params[\"learning_rate\"]\n",
    "best_dropout_rate = best_params[\"dropout_rate\"]\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot optimisation history (loss over trials)\n",
    "optuna.visualisation.plot_optimisation_history(study)\n",
    "# Plot relationship between hyperparameters\n",
    "optuna.visualisation.plot_param_importances(study)\n",
    "# Additional plots (Loss vs. specific hyperparameters)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "optuna.visualisation.matplotlib.plot_slice(study).set_title(\"Loss vs. Hyperparameters\")\n",
    "plt.show()  # Display plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with k-fold\n",
    "\n",
    "We look to further improvements by implementing **k-fold cross-validation** for training our model.\n",
    "\n",
    "K-fold cross-validation is a technique used to evaluate a model’s generalisation ability by splitting the dataset into multiple equal-sized folds (typically 5, as in our case). The model is trained on (k-1) folds while the remaining 1 fold is used for validation, ensuring every data point is tested at least once. This process repeats (k) times, each time using a different fold as the validation set. The final performance metric is the average over all iterations, making this method highly effective in reducing bias and variance compared to simple train-test splits. It helps us assess how well our model performs on unseen data, ensuring reliable real-world predictions.\n",
    "\n",
    "To achieve this, we split our dataset into multiple folds (default: 5), ensuring each portion is used for both training and validation, which helps to assess model generalisation. During each fold, the encoder and decoder weights are reset, trained for multiple epochs using teacher forcing, and optimised with Adam optimisers. After training, the validation step computes loss while ignoring padded tokens, ensuring evaluation accuracy. The trained models are then saved per fold for further analysis. Finally, the **evaluate_model** function calculates validation loss, ensuring we have some means of performance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def train_with_k_fold(num_epochs, encoder_save_name, decoder_save_name, batch_size, weight_init, learning_rate, k_folds=5):\n",
    "    \"\"\"\n",
    "    Implements k-fold cross-validation for training a sequence-to-sequence model.\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int): Number of epochs for training each fold.\n",
    "        encoder_save_name (str): Base name for saving the trained encoder model.\n",
    "        decoder_save_name (str): Base name for saving the trained decoder model.\n",
    "        batch_size (int): Number of samples per batch.\n",
    "        weight_init (function): Function to initialise model weights.\n",
    "        learning_rate (float): Learning rate for the optimiser.\n",
    "        k_folds (int): Number of folds for cross-validation.\n",
    "    \"\"\"\n",
    "    # Create a KFold object with shuffled data for better generalisation\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    fold_index = 1  # Track the current fold number\n",
    "    \n",
    "    # Loop over each fold, splitting dataset into training and validation sets\n",
    "    for train_indices, val_indices in kfold.split(dataset):  # Ensure `dataset` is preloaded\n",
    "        # Create subsets for training and validation based on fold indices\n",
    "        train_data = Subset(dataset, train_indices)\n",
    "        val_data = Subset(dataset, val_indices)\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        print(f\"Training fold {fold_index}/{k_folds}...\")\n",
    "        # Reset encoder and decoder model weights for each fold\n",
    "        encoder.apply(weight_init)  # Reset encoder weights\n",
    "        decoder.apply(weight_init)  # Reset decoder weights\n",
    "        # initialise Adam optimisers with the given learning rate\n",
    "        encoder_optimiser = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimiser = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train the model for the specified number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0  # Track loss for the epoch\n",
    "        \n",
    "            for input_seq, target_seq in train_loader:\n",
    "                # Move data to the selected device (GPU or CPU)\n",
    "                input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "                # Reset gradients before forward pass\n",
    "                encoder_optimiser.zero_grad()\n",
    "                decoder_optimiser.zero_grad()\n",
    "                # Forward pass through the encoder to generate hidden states\n",
    "                encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "                # initialise decoder input with <SOS> token for batch processing\n",
    "                decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "                decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs to initialise decoder states\n",
    "                # Compute target sequence lengths (excluding padding tokens)\n",
    "                target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "                max_target_length = target_lengths.max().item()\n",
    "\n",
    "                loss = 0  # Track cumulative loss per batch\n",
    "                \n",
    "                # Iterate through the target sequence length\n",
    "                for t in range(max_target_length):\n",
    "                    still_active = t < target_lengths  # Ensure valid sequence length\n",
    "                    if not still_active.any():\n",
    "                        break\n",
    "                    # Forward pass through decoder\n",
    "                    output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                        decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                    )\n",
    "                    # Compute loss while ignoring padded sequences\n",
    "                    loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "                    # Apply teacher forcing: use actual target sequence as next input\n",
    "                    decoder_input = target_seq[:, t]\n",
    "\n",
    "                # Backpropagate and update model parameters\n",
    "                loss.backward()\n",
    "                encoder_optimiser.step()\n",
    "                decoder_optimiser.step()\n",
    "                epoch_loss += loss.item()  # Accumulate loss over the epoch\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Evaluate model performance on validation set\n",
    "        val_loss = evaluate_model(val_loader)  # Call validation function\n",
    "        print(f\"Validation Loss for fold {fold_index}: {val_loss:.4f}\")\n",
    "\n",
    "        # Save trained model for the current fold\n",
    "        torch.save(encoder.state_dict(), f\"{encoder_save_name}_fold{fold_index}.pth\")\n",
    "        torch.save(decoder.state_dict(), f\"{decoder_save_name}_fold{fold_index}.pth\")\n",
    "\n",
    "        fold_index += 1  # Move to the next fold\n",
    "\n",
    "    print(\"K-fold cross-validation complete!\")\n",
    "\n",
    "def evaluate_model(dataloader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation dataset and computes loss.\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for validation samples.\n",
    "    Returns:\n",
    "        float: Average validation loss over all batches.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "\n",
    "    val_loss = 0  # Track validation loss\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            # Generate encoder outputs\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "            # initialise decoder input with <SOS> token\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs for decoder initialization\n",
    "            # Compute target sequence lengths\n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()\n",
    "\n",
    "            for t in range(max_target_length):\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():\n",
    "                    break\n",
    "\n",
    "                # Forward pass through decoder\n",
    "                output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "                # Compute validation loss while ignoring padded sequences\n",
    "                val_loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum().item() / still_active.sum().item()\n",
    "\n",
    "                decoder_input = target_seq[:, t]  # Teacher forcing\n",
    "\n",
    "    return val_loss / len(dataloader)  # Return average validation loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement hyperparameter changes and re-test\n",
    "\n",
    "Finally, we initialise and train our revised model using hyperparameter-tuned values and k-fold cross-validation to enhance generalisation. It first sets-up the encoder and decoder models using the best hyperparameters obtained from tuning (**hidden_size**, **dropout_rate**, and **learning_rate**), and applies cross-entropy loss with ignored padding tokens to prevent irrelevant data from influencing training. The Adam optimiser refines both models, ensuring efficient weight adjustments. We then invoke **k-fold cross-validation** (5 folds), systematically splitting the dataset into training and validation subsets, enabling performance tracking across different data partitions. Each fold resets model weights, trains for 100 epochs, evaluates validation performance, and saves the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model parameters using best hyperparameters from tuning\n",
    "input_size = len(word_to_index)  # Vocabulary size (total unique tokens)\n",
    "output_size = len(word_to_index)  # Decoder vocabulary size (same as input)\n",
    "hidden_size = best_hidden_size  # optimised hidden layer size from tuning\n",
    "dropout_rate = best_dropout_rate  # optimised dropout rate from tuning\n",
    "\n",
    "# initialise encoder and decoder with tuned parameters, moving them to the selected device\n",
    "encoder = Encoder(input_size, hidden_size, dropout_rate).to(device)\n",
    "decoder = Decoder(output_size, hidden_size, dropout_rate).to(device)\n",
    "\n",
    "# Define loss function, ignoring padding tokens to avoid unnecessary error influence\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN]) \n",
    "\n",
    "# Set up Adam optimisers using the best learning rate obtained from hyperparameter tuning\n",
    "encoder_optimiser = optim.Adam(encoder.parameters(), lr=best_learning_rate)\n",
    "decoder_optimiser = optim.Adam(decoder.parameters(), lr=best_learning_rate)\n",
    "\n",
    "# Define the number of training epochs\n",
    "num_epochs = 100  \n",
    "\n",
    "# Train using k-fold cross-validation (ensures better generalisation)\n",
    "train_with_k_fold(\n",
    "    num_epochs, \n",
    "    \"addition_only_math_problem_encoder\",  # Base filename for saving encoder model\n",
    "    \"addition_only_math_problem_decoder\",  # Base filename for saving decoder model\n",
    "    batch_size=32,  # Mini-batch size for training\n",
    "    weight_init=0.01,  # Weight initialization parameter\n",
    "    learning_rate=best_learning_rate,  # optimised learning rate from tuning\n",
    "    k_folds=5  # Number of folds for cross-validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_model(encoder, decoder, dataloader, max_target_len):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a trained sequence-to-sequence model using BLEU score.\n",
    "    Args:\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        dataloader: DataLoader containing test data.\n",
    "        max_target_len (int): Maximum length of the output sequence.\n",
    "    Returns:\n",
    "        float: BLEU score representing translation accuracy.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode (disable dropout)\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "\n",
    "    references = []  # List to store ground truth sequences\n",
    "    candidates = []  # List to store predicted sequences\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq = input_seq.to(device)  # Move input sequence to the appropriate device\n",
    "            # Forward pass through the encoder to obtain hidden states\n",
    "            _, (hidden, cell) = encoder(input_seq)\n",
    "            # initialise decoder input with <SOS> token for each sequence in the batch\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs to initialise decoder states\n",
    "\n",
    "            predictions = []  # List to store predicted tokens for each timestep\n",
    "            for t in range(max_target_len):\n",
    "                # Forward pass through the decoder\n",
    "                output, decoder_hidden, decoder_cell = decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                # Select the token with the highest probability\n",
    "                top_token = output.argmax(1)  # Get index of the most probable token\n",
    "                predictions.append(top_token)  # Append predicted token to sequence\n",
    "                # Stop decoding if <EOS> token is generated\n",
    "                if top_token.item() == word_to_index[END_OF_SEQUENCE_TOKEN]:\n",
    "                    break\n",
    "                # Use predicted token as next input to the decoder (auto-regressive decoding)\n",
    "                decoder_input = top_token\n",
    "\n",
    "            # Convert predicted and target sequences from indices to words\n",
    "            predicted_seq = [index_to_word[token.item()] for token in predictions]\n",
    "            target_seq_words = [[index_to_word[token.item()] for token in target_seq[0]]]  # Wrap target in a nested list for BLEU\n",
    "            # Append sequences to BLEU evaluation lists\n",
    "            references.append(target_seq_words)  # Store ground-truth sequence\n",
    "            candidates.append(predicted_seq)  # Store model-predicted sequence\n",
    "\n",
    "    # Apply smoothing and compute corpus BLEU score\n",
    "    smooth = SmoothingFunction().method1  # Helps with short sequences\n",
    "    bleu_score = corpus_bleu(references, candidates, smoothing_function=smooth)\n",
    "    # Print BLEU score to evaluate translation quality\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "\n",
    "    return bleu_score  # Return computed BLEU score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## TODO: we need to implement this function to evaluate the model using BLEU score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
