{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Add our overview narrative here!!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Local\\Temp\\ipykernel_21404\\2775953100.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 39, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Local\\Temp\\ipykernel_21404\\2775953100.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Local\\Temp\\ipykernel_21404\\2775953100.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.4 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Local\\Temp\\ipykernel_21404\\2775953100.py\", line 3, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr_name)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.4 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "This **Encoder** class is a part of the larger sequence-to-sequence model designed for processing mathematical word problems using an **LSTM**. It takes input sequences (math questions) represented as token indices and converts them into dense vector embeddings using an embedding layer, allowing the model to capture meaningful semantic relationships between words. The embedded representations then pass through an LSTM layer, which extracts temporal dependencies and generates both hidden and cell states that encode contextual information about the question sequence. A dropout layer is applied to the embeddings to reduce overfitting and improve generalisation. Ultimately, the encoder produces a set of outputs from each time step and a final hidden representation, which the decoder later uses to generate responses (the expected answer to our math question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Encoder for the sequence-to-sequence math problem assistant model using an LSTM. \n",
    "    Converts a sequence of token indices into a hidden representation \n",
    "    that will be used by the decoder for sequence generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialises the Encoder module.\n",
    "        Args:\n",
    "            input_size (int): The size of the input vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer converts token indices into dense vector representations.\n",
    "        # Instead of using raw word indices (which lack meaning), embeddings allow\n",
    "        # the LSTM to learn meaningful semantic relationships between words.\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # LSTM layer processes embedded input sequences to generate hidden states.\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Dropout layer applied to embeddings to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing token indices for a batch of input sentences.\n",
    "        Returns:\n",
    "            outputs (Tensor): Encoder outputs at each time step.\n",
    "            hidden (Tensor): Final hidden state of the LSTM.\n",
    "            cell (Tensor): Final cell state of the LSTM.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(input_seq)           # Convert input tokens into embeddings\n",
    "        embedded = self.dropout(embedded)             # Apply dropout before LSTM processing\n",
    "        outputs, (hidden, cell) = self.lstm(embedded) # Process embeddings through LSTM\n",
    "\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "The **Attention** class implements **'Bahdanau attention'**, which dynamically calculates attention scores based on the decoder’s hidden state and the encoder’s outputs. Instead of relying on a fixed context vector, this method allows the decoder to focus on specific parts of the input sequence at each decoding step. The attention mechanism works by concatenating the decoder's hidden state with the encoder’s outputs, passing them through a linear transformation, and applying the tanh activation function. A learnable vector (**v**) helps compute alignment scores, which are then normalised using softmax to generate attention weights—these determine the importance of each encoder output when predicting the next token. A dropout layer is also applied to prevent overfitting. The output is a set of attention weights that guides the decoder in generating more context-aware responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" \n",
    "    Implements Bahdanau attention mechanism for a sequence-to-sequence model. \n",
    "    Dynamically computes attention scores based on the decoder’s hidden state \n",
    "    and the encoder’s outputs, allowing the decoder to focus on relevant \n",
    "    parts of the input sequence at each decoding step.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the attention mechanism.\n",
    "        Args:\n",
    "            hidden_size (int): The size of the hidden state of the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        # Linear layer to compute alignment scores\n",
    "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        # Learnable vector for attention computation\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        # Dropout layer to regularize attention mechanism\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Computes attention weights using Bahdanau's additive attention method.\n",
    "        Args:\n",
    "            hidden (Tensor): Decoder hidden state at the current time step.\n",
    "            encoder_outputs (Tensor): Encoder outputs at all time steps.\n",
    "        Returns:\n",
    "            attention_weights (Tensor): Softmax-normalized attention scores.\n",
    "        \"\"\"\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        # Expand the decoder hidden state across the sequence length\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # Concatenate hidden state with encoder outputs to compute alignment scores\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # Apply dropout to energy scores before computing attention weights\n",
    "        energy = self.dropout(energy)\n",
    "        # Compute attention weights using a dot-product operation\n",
    "        v = self.v.unsqueeze(0).expand(batch_size, -1).unsqueeze(1)  # Ensure proper batch handling\n",
    "        attention_weights = torch.bmm(v, energy.transpose(1, 2)).squeeze(1)\n",
    "        # Apply softmax to normalize scores across sequence length\n",
    "        return torch.softmax(attention_weights, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The Decoder class plays a crucial role in our sequence-to-sequence model designed to solve math word problems. When given a problem as an input sequence, the Encoder first processes it into a hidden representation, which the Decoder then uses to generate the solution step by step. At each step, the attention mechanism dynamically selects the most relevant parts of the encoded problem statement, allowing the Decoder to focus on specific numerical relationships and mathematical operations. The LSTM maintains contextual understanding across time steps, helping track dependencies between numbers and mathematical operators. As the model generates each token in the solution, it refines its prediction using previously computed values, making the process similar to how humans break down word problems into logical steps. This structure ensures that the model interprets and solves math problems contextually, rather than simply memorising formulas, enabling it to generalise across different problem types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\" \n",
    "    Decoder for the sequence-to-sequence math problem assistant model using an \n",
    "    LSTM with Bahdanau attention. The decoder generates output tokens one by one \n",
    "    while dynamically focusing on relevant parts of the encoder’s outputs using \n",
    "    the attention mechanism.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_size, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        initialises the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            output_size (int): The size of the output vocabulary.\n",
    "            hidden_size (int): The number of hidden units in the LSTM.\n",
    "            dropout (float): Dropout rate to prevent overfitting.\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        # Embedding layer converts token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # LSTM layer processes embeddings and maintains hidden state across timesteps\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Linear layer maps concatenated attention context & LSTM output to vocab space\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        # Dropout layer to regularise embeddings before passing them to LSTM\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "        # Attention mechanism for dynamic focus on encoder outputs\n",
    "        self.attention = Attention(hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Forward pass for the decoder.\n",
    "        Args:\n",
    "            input (Tensor): Current token input to the decoder.\n",
    "            hidden (Tensor): Previous hidden state from the LSTM.\n",
    "            cell (Tensor): Previous cell state from the LSTM.\n",
    "            encoder_outputs (Tensor): Encoder outputs from all timesteps.\n",
    "\n",
    "        Returns:\n",
    "            output (Tensor): Predicted token probabilities.\n",
    "            hidden (Tensor): Updated hidden state.\n",
    "            cell (Tensor): Updated cell state.\n",
    "            attention_weights (Tensor): Attention scores for each encoder timestep.\n",
    "        \"\"\"\n",
    "        # Expand input dimensions to match expected input shape for embedding\n",
    "        input = input.unsqueeze(1)  \n",
    "        # Convert token indices into dense embeddings & apply dropout for regularisation\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # Forward pass through LSTM to generate new hidden and cell states\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # Compute attention weights using the current hidden state and encoder outputs\n",
    "        attention_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        # Apply attention: generate weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # Flatten tensors for the fully connected layer\n",
    "        lstm_output = lstm_output.squeeze(1)\n",
    "        context = context.squeeze(1)\n",
    "        # Generate token probabilities using concatenated LSTM output and attention context\n",
    "        output = self.fc(torch.cat((lstm_output, context), dim=1))\n",
    "\n",
    "        return output, hidden, cell, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Basic tokenisation and vocabulary Setup\n",
    "\n",
    "Here we set up the constants and tokenisation process for a sequence-to-sequence math problem assistant. It first defines special tokens **<SOS>**, **<EOS>**, and **<PAD>** to mark the start, end, and padding of sequences. Then, it creates a vocabulary mapping that converts words into numerical indices, allowing the model to process text as numbers. The reverse mapping (**index_to_word**) ensures that predictions can be decoded back into words. Finally, the script tokenises example input and target sequences, transforming for example, **\"two plus four\"** into a list of indices **[3, 4, 5]** and the target **\"equals six\"** into **[0, 6, 7, 1]**, ensuring that the decoder starts with <SOS> and ends with <EOS>. This setup enables the neural network to work with structured input-output pairs for training our expected math-solving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estalish constants\n",
    "START_OF_SEQUENCE_TOKEN = \"<SOS>\"\n",
    "END_OF_SEQUENCE_TOKEN = \"<EOS>\"\n",
    "PADDING_SEQUENCE_TOKEN = \"<PAD>\"\n",
    "\n",
    "# Tokenisation and vocab setup\n",
    "# Create a vocabulary mapping words to indices\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Example input and target sequences\n",
    "input_sentence = \"two plus four\"\n",
    "target_sentence = \"equals six\"\n",
    "\n",
    "# Test Tokenised input and targets\n",
    "input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "target_tokens = [word_to_index[START_OF_SEQUENCE_TOKEN]] + [word_to_index[word] for word in target_sentence.split()] + [word_to_index[END_OF_SEQUENCE_TOKEN]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokensied data\n",
    "\n",
    "This **BasicMathWordProblemDataset** class creates a custom PyTorch Dataset for training our sequence-to-sequence model that solves basic math word problems. It converts input and target sentences (our math questions) into lists of token indices using a predefined vocabulary (**word_to_index**). The target sequence is prepended with <SOS> and appended with <EOS> tokens to indicate the start and end of the output. The class implements essential dataset methods, notablu __len__() which returns the number of samples, while __getItem__() retrieves tokenised tensors for input and target sequences, ensuring compatibility with PyTorch models. Finally, the dataset is wrapped in a DataLoader, allowing efficient batch processing with shuffling to improve learning dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicMathWordProblemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for tokenised math word problems.\n",
    "    This class converts input sentences into tokenised sequences using \n",
    "    a predefined vocabulary (`word_to_index`) and structures target \n",
    "    sequences with <SOS> (start-of-sequence) and <EOS> (end-of-sequence) \n",
    "    tokens for proper handling by the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_sentences, target_sentences, word_to_index):\n",
    "        \"\"\"\n",
    "        initialises the dataset by converting words into token indices.\n",
    "        Args:\n",
    "            input_sentences (list of str): List of math word problems (input).\n",
    "            target_sentences (list of str): Corresponding solutions (output).\n",
    "            word_to_index (dict): Mapping of words to numerical token indices.\n",
    "        \"\"\"\n",
    "        # Tokenise input sentences into numerical sequences\n",
    "        self.input_data = [[word_to_index[word] for word in sentence.split()] for sentence in input_sentences]\n",
    "        # Tokenise target sentences while adding special tokens <SOS> and <EOS>\n",
    "        self.target_data = [[word_to_index[START_OF_SEQUENCE_TOKEN]] + \n",
    "                            [word_to_index[word] for word in sentence.split()] + \n",
    "                            [word_to_index[END_OF_SEQUENCE_TOKEN]] for sentence in target_sentences]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Returns:\n",
    "            int: Total number of input-target pairs.\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the tokenised input and target sequences as PyTorch tensors.\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: Tokenised input and target sequences.\n",
    "        \"\"\"\n",
    "        return torch.tensor(self.input_data[idx], dtype=torch.long), torch.tensor(self.target_data[idx], dtype=torch.long)\n",
    "\n",
    "# Example data\n",
    "input_sentences = [\"two plus four\"]\n",
    "target_sentences = [\"equals six\"]\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BasicMathWordProblemDataset(input_sentences, target_sentences, word_to_index)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparameters\n",
    "\n",
    "Here we initialise the key components for training our sequence-to-sequence math word problem solving model. The encoder and decoder are instantiated with the vocabulary size (**input_size** and **output_size**) and a hidden state of **128**, allowing the model to process and generate our math solutions. The **CrossEntropy loss function** is used with padding tokens ignored to prevent unnecessary calculations from affecting training. Adam optimisers are applied to both the encoder and decoder with a learning rate of **0.001**, enabling stable gradient updates. Finally, an attention matrix is initialised to store attention weights, which will later be used for visualisation, thus helping to analyse how the model focuses on different parts of the input when generating solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "input_size = len(word_to_index)  # Total vocabulary size\n",
    "output_size = len(word_to_index)  # Vocabulary size\n",
    "hidden_size = 128\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "\n",
    "# Ignore padding tokens\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN])  \n",
    "encoder_optimiser = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimiser = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "# Store attention weights for visualisation\n",
    "attention_matrix = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training loss over epochs\n",
    "\n",
    "This function visualises the training loss over epochs using Matplotlib. It takes a list of loss values (**epoch_losses**), representing the loss at each epoch, and plots them on a graph to track performance trends over time. This is useful for monitoring model convergence and diagnosing potential issues such as overfitting or slow learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(epoch_losses):\n",
    "    \"\"\"\n",
    "    Plots the training loss over epochs.\n",
    "    Args:\n",
    "        epoch_losses (list): A list containing the loss values for each epoch.\n",
    "    \"\"\"\n",
    "    # Create a figure with a predefined size for better readability\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Plot the loss values over epochs with markers for each epoch\n",
    "    plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, marker='o', label='Training Loss')\n",
    "    # Label the x-axis to indicate the epoch number\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    # Label the y-axis to indicate the loss value\n",
    "    plt.ylabel(\"Loss\")\n",
    "    # Add a title to the plot for better context\n",
    "    plt.title(\"Training Loss Over Epochs\")\n",
    "    # Display a legend to clarify the plotted line\n",
    "    plt.legend()\n",
    "    # Add a grid to improve readability of the trend\n",
    "    plt.grid()\n",
    "    # Show the final plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish the training regime\n",
    "\n",
    "This **train()** function is responsible for training our sequence-to-sequence model with an encoder-decoder architecture with **'Bahdanau attention'**. It iterates through a dataset of tokenised math problems for multiple epochs, processing each batch by passing inputs through the encoder, which generates a context vector. The decoder then predicts output tokens step by step, using teacher forcing to guide learning while dynamically focusing on relevant parts of the input via attention mechanisms. The function tracks masked loss, ensuring that padding tokens don't affect optimisation, and updates model weights using backpropagation with **Adam optimisers**. After training, it stores attention weights for visualisation, logs epoch losses, plots them for monitoring, and saves the trained models to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, encoder_save_name, decoder_save_name):\n",
    "    \"\"\"\n",
    "    Trains the sequence-to-sequence model using an encoder-decoder architecture.\n",
    "    Args:\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        encoder_save_name (str): File name for saving the trained encoder model.\n",
    "        decoder_save_name (str): File name for saving the trained decoder model.\n",
    "    \"\"\"\n",
    "    epoch_losses = []  # Stores loss values for tracking progress\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0  # Accumulate total loss for the epoch\n",
    "\n",
    "        for input_seq, target_seq in dataloader:\n",
    "            # Move data to GPU if available\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            # Reset gradients before each batch\n",
    "            encoder_optimiser.zero_grad()\n",
    "            decoder_optimiser.zero_grad()\n",
    "            # Encoder forward pass - processes input sequences to generate hidden states\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "            # Decoder initialisation - starts with the encoder's final hidden state\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell\n",
    "            # Get actual sequence lengths (excluding padding)\n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()\n",
    "\n",
    "            loss = 0  # Tracks batch loss\n",
    "            \n",
    "            # Iterate through target sequence timesteps\n",
    "            for t in range(max_target_length):\n",
    "                # Determine active sequences\n",
    "                still_active = t < target_lengths\n",
    "                if not still_active.any():  # If all sequences finished, break loop\n",
    "                    break\n",
    "\n",
    "                # Decoder forward pass - generates output token probabilities\n",
    "                output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "                # Masked loss calculation - only consider active sequences\n",
    "                loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "                # Teacher forcing: Use actual target token as next input\n",
    "                decoder_input = target_seq[:, t]  \n",
    "\n",
    "            # Backpropagation - compute gradients and update model parameters\n",
    "            loss.backward()\n",
    "            encoder_optimiser.step()\n",
    "            decoder_optimiser.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Store normalised loss\n",
    "        epoch_losses.append(epoch_loss / len(dataloader))\n",
    "        # Print training progress\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_losses[-1]:.4f}\")\n",
    "        # Store attention weights for visualisation\n",
    "        attention_matrix.append(attention_weights.cpu().detach().numpy())\n",
    "\n",
    "    # Plot training loss\n",
    "    plot_loss(epoch_losses)\n",
    "\n",
    "    # Save the trained models\n",
    "    torch.save(encoder.state_dict(), f\"{encoder_save_name}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"{decoder_save_name}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "We'll start training for **100** epochs to refine our sequence-to-sequence model. This will help our encoder-decoder system learn better representations and improve predictions for solving math word problems. We'll expect to see loss decreasing over time, but if it stagnates or spikes, we can adjust learning rate, batch size, and/or other hyperparameters to stabilise training. Also, our ability to monitor attention weights will help us verify whether the model is correctly focusing on key parts of the input during decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 9.9870\n",
      "Epoch 2/100, Loss: 9.2801\n",
      "Epoch 3/100, Loss: 8.7533\n",
      "Epoch 4/100, Loss: 8.2164\n",
      "Epoch 5/100, Loss: 7.7435\n",
      "Epoch 6/100, Loss: 7.0442\n",
      "Epoch 7/100, Loss: 6.5527\n",
      "Epoch 8/100, Loss: 5.8938\n",
      "Epoch 9/100, Loss: 5.2673\n",
      "Epoch 10/100, Loss: 4.8010\n",
      "Epoch 11/100, Loss: 4.3403\n",
      "Epoch 12/100, Loss: 3.8793\n",
      "Epoch 13/100, Loss: 3.4926\n",
      "Epoch 14/100, Loss: 3.1290\n",
      "Epoch 15/100, Loss: 2.8984\n",
      "Epoch 16/100, Loss: 2.5403\n",
      "Epoch 17/100, Loss: 2.2788\n",
      "Epoch 18/100, Loss: 2.1298\n",
      "Epoch 19/100, Loss: 1.8200\n",
      "Epoch 20/100, Loss: 1.7339\n",
      "Epoch 21/100, Loss: 1.5037\n",
      "Epoch 22/100, Loss: 1.3863\n",
      "Epoch 23/100, Loss: 1.2209\n",
      "Epoch 24/100, Loss: 1.1386\n",
      "Epoch 25/100, Loss: 1.0477\n",
      "Epoch 26/100, Loss: 0.9325\n",
      "Epoch 27/100, Loss: 0.8713\n",
      "Epoch 28/100, Loss: 0.7622\n",
      "Epoch 29/100, Loss: 0.7101\n",
      "Epoch 30/100, Loss: 0.6464\n",
      "Epoch 31/100, Loss: 0.5733\n",
      "Epoch 32/100, Loss: 0.5124\n",
      "Epoch 33/100, Loss: 0.4883\n",
      "Epoch 34/100, Loss: 0.4548\n",
      "Epoch 35/100, Loss: 0.4001\n",
      "Epoch 36/100, Loss: 0.3737\n",
      "Epoch 37/100, Loss: 0.3418\n",
      "Epoch 38/100, Loss: 0.3157\n",
      "Epoch 39/100, Loss: 0.2976\n",
      "Epoch 40/100, Loss: 0.2654\n",
      "Epoch 41/100, Loss: 0.2486\n",
      "Epoch 42/100, Loss: 0.2290\n",
      "Epoch 43/100, Loss: 0.2101\n",
      "Epoch 44/100, Loss: 0.1933\n",
      "Epoch 45/100, Loss: 0.1878\n",
      "Epoch 46/100, Loss: 0.1682\n",
      "Epoch 47/100, Loss: 0.1567\n",
      "Epoch 48/100, Loss: 0.1500\n",
      "Epoch 49/100, Loss: 0.1424\n",
      "Epoch 50/100, Loss: 0.1345\n",
      "Epoch 51/100, Loss: 0.1262\n",
      "Epoch 52/100, Loss: 0.1182\n",
      "Epoch 53/100, Loss: 0.1180\n",
      "Epoch 54/100, Loss: 0.1066\n",
      "Epoch 55/100, Loss: 0.1003\n",
      "Epoch 56/100, Loss: 0.1037\n",
      "Epoch 57/100, Loss: 0.0925\n",
      "Epoch 58/100, Loss: 0.0887\n",
      "Epoch 59/100, Loss: 0.0868\n",
      "Epoch 60/100, Loss: 0.0823\n",
      "Epoch 61/100, Loss: 0.0785\n",
      "Epoch 62/100, Loss: 0.0773\n",
      "Epoch 63/100, Loss: 0.0759\n",
      "Epoch 64/100, Loss: 0.0709\n",
      "Epoch 65/100, Loss: 0.0668\n",
      "Epoch 66/100, Loss: 0.0656\n",
      "Epoch 67/100, Loss: 0.0623\n",
      "Epoch 68/100, Loss: 0.0607\n",
      "Epoch 69/100, Loss: 0.0578\n",
      "Epoch 70/100, Loss: 0.0579\n",
      "Epoch 71/100, Loss: 0.0557\n",
      "Epoch 72/100, Loss: 0.0535\n",
      "Epoch 73/100, Loss: 0.0544\n",
      "Epoch 74/100, Loss: 0.0528\n",
      "Epoch 75/100, Loss: 0.0503\n",
      "Epoch 76/100, Loss: 0.0493\n",
      "Epoch 77/100, Loss: 0.0479\n",
      "Epoch 78/100, Loss: 0.0468\n",
      "Epoch 79/100, Loss: 0.0469\n",
      "Epoch 80/100, Loss: 0.0447\n",
      "Epoch 81/100, Loss: 0.0430\n",
      "Epoch 82/100, Loss: 0.0427\n",
      "Epoch 83/100, Loss: 0.0429\n",
      "Epoch 84/100, Loss: 0.0411\n",
      "Epoch 85/100, Loss: 0.0397\n",
      "Epoch 86/100, Loss: 0.0397\n",
      "Epoch 87/100, Loss: 0.0388\n",
      "Epoch 88/100, Loss: 0.0384\n",
      "Epoch 89/100, Loss: 0.0372\n",
      "Epoch 90/100, Loss: 0.0373\n",
      "Epoch 91/100, Loss: 0.0374\n",
      "Epoch 92/100, Loss: 0.0348\n",
      "Epoch 93/100, Loss: 0.0345\n",
      "Epoch 94/100, Loss: 0.0349\n",
      "Epoch 95/100, Loss: 0.0330\n",
      "Epoch 96/100, Loss: 0.0332\n",
      "Epoch 97/100, Loss: 0.0321\n",
      "Epoch 98/100, Loss: 0.0312\n",
      "Epoch 99/100, Loss: 0.0315\n",
      "Epoch 100/100, Loss: 0.0307\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100  # Define the number of epochs\n",
    "train(num_epochs, \"basic_math_problem_encoder\", \"basic_math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate heatmap\n",
    "\n",
    "This function allows us to visualise attention weights as a **heatmap**, helping us analyse which input tokens the decoder focuses on while generating each output token in our model. Using Matplotlib, it plots the attention_matrix as a color-coded intensity map, where darker shades indicate stronger attention at specific positions. This visualisation is helpful for allowing us to understand how our model prioritises different parts of the input, making it inciteful when attempting to optimise our attention mechanism. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_attention(input_tokens, output_tokens, attention_matrix):\n",
    "    \"\"\"\n",
    "    Plot the attention weights as a heatmap.\n",
    "\n",
    "    Args:\n",
    "        input_tokens (list of str): Tokens in the input sequence.\n",
    "        output_tokens (list of str): Tokens in the output sequence.\n",
    "        attention_matrix (np.array): Attention weights matrix (output_tokens x input_tokens).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(attention_matrix, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "    plt.xticks(ticks=np.arange(len(input_tokens)), labels=input_tokens, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks=np.arange(len(output_tokens)), labels=output_tokens)\n",
    "    plt.colorbar(label=\"Attention Weight\")\n",
    "    plt.xlabel(\"Input Tokens\")\n",
    "    plt.ylabel(\"Output Tokens\")\n",
    "    plt.title(\"Attention Heatmap\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test setup\n",
    "\n",
    "This function allow us to test our trained model by encoding our math question input sentence into hidden states and decoding it step-by-step to generate our expected answer. It tokenises the input using a predefined vocabulary and passes it through the encoder, which extracts a contextual representation. The decoder then predicts each output token using an iterative approach, incorporating attention weights to dynamically focus on relevant parts of the input. The function tracks attention matrices, enabling visualisation of how the model distributes focus across tokens. It prints the input sentence, generated output, and attention shape, and optionally plots a heatmap for deeper analysis. The final output is a list of predicted words, representing the answer to our math problem, which can then be evaluated for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=100, with_attention_plot=False):\n",
    "    # Set the models to evaluation mode\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    # Tokenise the input sentence\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "    # Forward pass through the encoder\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "    # initialise the decoder\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]], dtype=torch.long)  # Start-of-Sequence token\n",
    "    decoder_hidden = hidden\n",
    "    decoder_cell = cell\n",
    "    # Generate output sequence and collect attention weights\n",
    "    output_sequence = []\n",
    "    attention_matrices = []\n",
    "\n",
    "    for _ in range(max_target_length):\n",
    "        output, decoder_hidden, decoder_cell, attention_weights = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        predicted_token = output.argmax(1).item()  # Get the token with the highest probability\n",
    "\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE_TOKEN]:  # Stop at End-of-Sequence token\n",
    "            break\n",
    "\n",
    "        output_sequence.append(predicted_token)\n",
    "        attention_matrices.append(attention_weights.cpu().detach().numpy())  # Save attention weights\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)  # Next decoder input\n",
    "\n",
    "    # Convert input and output tokens to words\n",
    "    input_sentence_tokens = [index_to_word[token] for token in input_tokens]\n",
    "    output_sentence_tokens = [index_to_word[token] for token in output_sequence]\n",
    "    # Stack attention matrices into a 2D array (output_tokens x input_tokens)\n",
    "    attention_matrix = np.vstack(attention_matrices)\n",
    "    # Print the input and output sentences\n",
    "    print(\"Input Sentence:\", input_sentence_tokens)\n",
    "    print(\"Generated Sentence:\", output_sentence_tokens)\n",
    "    print(\"Attention Weights Shape:\", attention_matrix)\n",
    "    # Visualise attention\n",
    "    if with_attention_plot:\n",
    "        # Plot the attention weights\n",
    "        plot_attention(input_sentence_tokens, output_sentence_tokens, attention_matrix)\n",
    "\n",
    "    # Convert token indices to words\n",
    "    return [index_to_word[token] for token in output_sequence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the trained model - basic approach\n",
    "\n",
    "Here we load our pre-trained sequence-to-sequence model for solving basic math word problems. We first initialises the encoder and decoder architectures with the same parameters used during training, ensuring consistency. Then, we load the pretrained model weights from the associated **.pth** files, restoring previously learned knowledge. After defining a sample input sentence, **\"two plus four\"**, we set up **word-to-index** and **index-to-word** mappings, allowing tokenisation and conversion between words and numerical indices. Finally, the test function is called, running the encoder-decoder model on the input question to generate a predicted answer while optionally visualising attention weights so we can attempt analyse how the model focuses on different tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ['two', 'plus', 'four']\n",
      "Generated Sentence: ['<SOS>', 'equals', 'six']\n",
      "Attention Weights Shape: [[3.6116724e-10 7.6156641e-05 9.9992383e-01]\n",
      " [1.6152478e-10 4.6348876e-05 9.9995363e-01]\n",
      " [3.6396572e-10 7.6627890e-05 9.9992335e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<SOS>', 'equals', 'six']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 128  # Same as during training\n",
    "# Initialise the encoder and decoder\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"basic_math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"basic_math_problem_decoder.pth\"))\n",
    "\n",
    "input_sentence = \"two plus four\"\n",
    "\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2, \"two\": 3, \"plus\": 4, \"four\": 5, \"equals\": 6, \"six\": 7, \"three\": 8, \"minus\": 9, \"one\": 10}\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}  # Reverse mapping for decoding\n",
    "\n",
    "# Test and visualise\n",
    "test(encoder, decoder, input_sentence, word_to_index, index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish a baseline vocabulary and tokenisation capability\n",
    "\n",
    "Here we set up a dynamic tokenisation function for our model by defining a baseline vocabulary containing special tokens (<SOS>, <EOS>, <PAD>). The function processes an input sentence by splitting words, checking if they already exist in the **word_to_index** mapping, and assigning a new index to any unseen words. This ensures that the vocabulary expands dynamically as new words are encountered, allowing the model to handle previously unknown input without predefined restrictions, with the function ultimately generating a list of token indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens for sequence processing - initially, the vocabulary only contains special tokens (<SOS>, <EOS>, <PAD>)\n",
    "word_to_index = {START_OF_SEQUENCE_TOKEN: 0, END_OF_SEQUENCE_TOKEN: 1, PADDING_SEQUENCE_TOKEN: 2}  \n",
    "\n",
    "def tokenise(sentence, word_to_index):\n",
    "    \"\"\"\n",
    "    tokenises a sentence and dynamically updates the vocabulary mapping.\n",
    "    Args:\n",
    "        sentence (str): The input sentence to be tokenised.\n",
    "        word_to_index (dict): Dictionary mapping words to unique indices.\n",
    "    Returns:\n",
    "        list: A list of token indices representing the sentence.\n",
    "    \"\"\"\n",
    "    tokens = []  # Stores tokenised word indices\n",
    "\n",
    "    # Process each word in the sentence\n",
    "    for word in sentence.lower().split():\n",
    "        # Add unseen words to the vocabulary and assign a unique index\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = len(word_to_index)  \n",
    "        \n",
    "        # Append the tokenised index to the token list\n",
    "        tokens.append(word_to_index[word])\n",
    "\n",
    "    return tokens  # Return tokenised sentence as a list of indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV dataset from file\n",
    "\n",
    "Here we load a dataset of math questions and their associated answers from our previously generated CSV file, using Pandas. The function reads the CSV file into a DataFrame, extracts the \"Question\" column (math word question) and \"Answer\" column (corresponding answers), and returns them as lists. The script then specifies **\"simple_math_problems_addition_only.csv\"** as the dataset and calls the function to populate **input_sentences** and **target_sentences**, which can be tokenised and fed into the model for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def clean_question_and_answer_data(text):\n",
    "    \"\"\"\n",
    "    Cleans text by converting to lowercase, removing special characters, and stripping whitespace.\n",
    "    Args:\n",
    "        text (str): Input text to be cleaned.\n",
    "    Returns:\n",
    "        str: Cleaned text.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase for consistency\n",
    "        text = text.lower()  \n",
    "        # Regular expression to remove special characters except basic math symbols\n",
    "        text = re.sub(r\"[^a-z0-9+\\-*/=? ]\", \"\", text)\n",
    "        # Remove extra whitespace\n",
    "        text = text.strip()  \n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def load_question_and_answer_sequences_from_csv(csv_file):\n",
    "    \"\"\"\n",
    "    Loads math problem sequences from a CSV file with error handling.\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file containing math problems and solutions.\n",
    "    Returns:\n",
    "        tuple: Two lists containing problem statements and their corresponding solutions.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to read the CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # Check if required columns exist\n",
    "        required_columns = {\"Question\", \"Answer\"}\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            raise ValueError(f\"Missing required columns: {required_columns - set(df.columns)}\")\n",
    "        # Handle missing values by dropping rows with NaN values in necessary columns\n",
    "        df = df.dropna(subset=[\"Question\", \"Answer\"])\n",
    "        # Apply text cleaning function to both columns\n",
    "        df[\"Question\"] = df[\"Question\"].apply(clean_question_and_answer_data)\n",
    "        df[\"Answer\"] = df[\"Answer\"].apply(clean_question_and_answer_data)\n",
    "        # Convert cleaned sequences to lists\n",
    "        input_sentences = df[\"Question\"].tolist()\n",
    "        target_sentences = df[\"Answer\"].tolist()\n",
    "        # Return the question and answer sequence lists\n",
    "        return input_sentences, target_sentences \n",
    "    # File not found error\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_file}' was not found.\") \n",
    "        return [], []\n",
    "    # Handle missing columns or other value errors\n",
    "    except ValueError as ve: \n",
    "        print(f\"Error: {ve}\")\n",
    "        return [], []\n",
    "    # Catch-all for any other exceptions\n",
    "    except Exception as e: \n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return [], []\n",
    "\n",
    "# Specify the dataset file name\n",
    "csv_file = \"math_word_problems.csv\"\n",
    "# Load question and answer sequences\n",
    "question_sequences, answer_sequences = load_question_and_answer_sequences_from_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenise input and target sentences and convert into tensors\n",
    "\n",
    "We now tokenise the question and answer sentences and convert them into PyTorch tensors, preparing them for processing in a sequence-to-sequence model. It first transforms input sentences into numerical token lists using a predefined vocabulary (**word_to_index**). For target sentences, thus ensuring a structured decoding by appending **<SOS>** at the beginning and **<SOS>** at the end, guiding the model in output generation. A reverse mapping (**index_to_word**) is also created, allowing numerical tokens to be converted back into words for interpretation. Finally, both input and target tokenised sequences are converted into PyTorch tensors, ensuring they are formatted correctly for our models training and inference requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_and_answer_sequences(question_sequences, answer_sequences):\n",
    "    \"\"\"\n",
    "    Creates tokenised input and target sequences from the loaded sentences.\n",
    "    Args:\n",
    "        input_sentences (list of str): List of math problems.\n",
    "        target_sentences (list of str): List of corresponding solutions.\n",
    "    Returns:\n",
    "        tuple: Two lists containing tokenised input and target sequences.\n",
    "    \"\"\"\n",
    "    # Tokenise input sequences by converting words to numerical indices\n",
    "    input_data = [tokenise(sequence, word_to_index) for sequence in question_sequences]\n",
    "    # Tokenise target sequences while adding <SOS> (start token) and <EOS> (end token)\n",
    "    target_data = [[word_to_index[START_OF_SEQUENCE_TOKEN]] + \n",
    "                   tokenise(sequence, word_to_index) + \n",
    "                   [word_to_index[END_OF_SEQUENCE_TOKEN]] \n",
    "                   for sequence in answer_sequences]\n",
    "    \n",
    "    return input_data, target_data  # Return both tokenised sequences\n",
    "\n",
    "# Tokenise the question and answer sequences and create input-target pairs\n",
    "input_data, target_data = create_question_and_answer_sequences(question_sequences, answer_sequences) \n",
    "# Create a reverse mapping dictionary to convert token indices back into words\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "# Convert tokenised sentences into PyTorch tensors for model compatibility\n",
    "question_tensors = [torch.tensor(seq) for seq in input_data]  # Convert question sequences to tensors\n",
    "answer_tensors = [torch.tensor(seq) for seq in target_data]  # Convert answer sequences to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the math problem dataset from the tokensied data, including padding\n",
    "\n",
    "This code prepares a dataset of math word problems for our sequence-to-sequence learning model, ensuring uniform sequence lengths using dynamic padding. The input and target sequences are padded with a special token **<PAD>** to maintain consistency in batch processing. The **DynamicMathWordProblemDataset** class organises the padded data into a PyTorch-compatible format, allowing efficient access to input-target pairs. The dataset is then wrapped in a PyTorch , which facilitates batch-wise loading of data, enabling smooth training with a batch size of **64**. By disabling shuffling, the sequences maintain their original order, optimising structured learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Apply dynamic padding to ensure uniform sequence length\n",
    "questions_padded = pad_sequence(question_tensors, batch_first=True, padding_value=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "answers_padded = pad_sequence(answer_tensors, batch_first=True, padding_value=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "\n",
    "class MathWordProblemDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset for padded math word problems.\n",
    "    This class stores input and target sequences that have been padded \n",
    "    to a fixed length, ensuring consistency in batch processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, questions_padded, answers_padded):\n",
    "        \"\"\"\n",
    "        initialises the dataset with padded sequences.\n",
    "        Args:\n",
    "            input_padded (Tensor): Padded input sequences.\n",
    "            target_padded (Tensor): Padded target sequences.\n",
    "        \"\"\"\n",
    "        self.input_data = questions_padded  # Store padded input sequences\n",
    "        self.target_data = answers_padded  # Store padded target sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        Returns:\n",
    "            int: Total number of input-target pairs.\n",
    "        \"\"\"\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a padded input-target pair as tensors.\n",
    "        Args:\n",
    "            idx (int): Index of the data sample.\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: Padded input and target sequences.\n",
    "        \"\"\"\n",
    "        return self.input_data[idx], self.target_data[idx]\n",
    "\n",
    "# Create a dataset instance using the padded sequences\n",
    "dataset = MathWordProblemDataset(questions_padded, answers_padded)\n",
    "# initialise a DataLoader for batch processing\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False)  # Batch size of 64, shuffle disabled for sequential processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish hyperparamters\n",
    "\n",
    "We now set-up the hyperparameters for our trainining model. We first initialise the encoder and decoder models with a hidden size of **256**, then a **cross-entropy loss function** is defined with **ignore_index**, ensuring that our padding tokens do not affect training. Both models use the Adam optimiser with a learning rate of **0.0005** to adjust weights and improve performance. Finally, the we define training for **100** epochs, allowing saving of the learned model parameters under specified filenames for later inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 5.8430\n",
      "Epoch 2/100, Loss: 3.6108\n",
      "Epoch 3/100, Loss: 3.5335\n",
      "Epoch 4/100, Loss: 3.2628\n",
      "Epoch 5/100, Loss: 2.6837\n",
      "Epoch 6/100, Loss: 2.2523\n",
      "Epoch 7/100, Loss: 1.9868\n",
      "Epoch 8/100, Loss: 1.7964\n",
      "Epoch 9/100, Loss: 1.6161\n",
      "Epoch 10/100, Loss: 1.4725\n",
      "Epoch 11/100, Loss: 1.3404\n",
      "Epoch 12/100, Loss: 1.2654\n",
      "Epoch 13/100, Loss: 1.1664\n",
      "Epoch 14/100, Loss: 1.0412\n",
      "Epoch 15/100, Loss: 0.9829\n",
      "Epoch 16/100, Loss: 0.9126\n",
      "Epoch 17/100, Loss: 0.8030\n",
      "Epoch 18/100, Loss: 0.7682\n",
      "Epoch 19/100, Loss: 0.7263\n",
      "Epoch 20/100, Loss: 0.6564\n",
      "Epoch 21/100, Loss: 0.6589\n",
      "Epoch 22/100, Loss: 0.5700\n",
      "Epoch 23/100, Loss: 0.5290\n",
      "Epoch 24/100, Loss: 0.4922\n",
      "Epoch 25/100, Loss: 0.4253\n",
      "Epoch 26/100, Loss: 0.4183\n",
      "Epoch 27/100, Loss: 0.3898\n",
      "Epoch 28/100, Loss: 0.4088\n",
      "Epoch 29/100, Loss: 0.3184\n",
      "Epoch 30/100, Loss: 0.2830\n",
      "Epoch 31/100, Loss: 0.2823\n",
      "Epoch 32/100, Loss: 0.2726\n",
      "Epoch 33/100, Loss: 0.2418\n",
      "Epoch 34/100, Loss: 0.2575\n",
      "Epoch 35/100, Loss: 0.2479\n",
      "Epoch 36/100, Loss: 0.2155\n",
      "Epoch 37/100, Loss: 0.1842\n",
      "Epoch 38/100, Loss: 0.1848\n",
      "Epoch 39/100, Loss: 0.1918\n",
      "Epoch 40/100, Loss: 0.1655\n",
      "Epoch 41/100, Loss: 0.2257\n",
      "Epoch 42/100, Loss: 0.1580\n",
      "Epoch 43/100, Loss: 0.1285\n",
      "Epoch 44/100, Loss: 0.1376\n",
      "Epoch 45/100, Loss: 0.1570\n",
      "Epoch 46/100, Loss: 0.0946\n",
      "Epoch 47/100, Loss: 0.1258\n",
      "Epoch 48/100, Loss: 0.0961\n",
      "Epoch 49/100, Loss: 0.1358\n",
      "Epoch 50/100, Loss: 0.0946\n",
      "Epoch 51/100, Loss: 0.1118\n",
      "Epoch 52/100, Loss: 0.1210\n",
      "Epoch 53/100, Loss: 0.0970\n",
      "Epoch 54/100, Loss: 0.0788\n",
      "Epoch 55/100, Loss: 0.1166\n",
      "Epoch 56/100, Loss: 0.0763\n",
      "Epoch 57/100, Loss: 0.0898\n",
      "Epoch 58/100, Loss: 0.0907\n",
      "Epoch 59/100, Loss: 0.0921\n",
      "Epoch 60/100, Loss: 0.0909\n",
      "Epoch 61/100, Loss: 0.0569\n",
      "Epoch 62/100, Loss: 0.0551\n",
      "Epoch 63/100, Loss: 0.0477\n",
      "Epoch 64/100, Loss: 0.0399\n",
      "Epoch 65/100, Loss: 0.0237\n",
      "Epoch 66/100, Loss: 0.0387\n",
      "Epoch 67/100, Loss: 0.0381\n",
      "Epoch 68/100, Loss: 0.0561\n",
      "Epoch 69/100, Loss: 0.0498\n",
      "Epoch 70/100, Loss: 0.0535\n",
      "Epoch 71/100, Loss: 0.1279\n",
      "Epoch 72/100, Loss: 0.0684\n",
      "Epoch 73/100, Loss: 0.0629\n",
      "Epoch 74/100, Loss: 0.0486\n",
      "Epoch 75/100, Loss: 0.0289\n",
      "Epoch 76/100, Loss: 0.0154\n",
      "Epoch 77/100, Loss: 0.0142\n",
      "Epoch 78/100, Loss: 0.0249\n",
      "Epoch 79/100, Loss: 0.0212\n",
      "Epoch 80/100, Loss: 0.0298\n",
      "Epoch 81/100, Loss: 0.0366\n",
      "Epoch 82/100, Loss: 0.1302\n",
      "Epoch 83/100, Loss: 0.1048\n",
      "Epoch 84/100, Loss: 0.0841\n",
      "Epoch 85/100, Loss: 0.0400\n",
      "Epoch 86/100, Loss: 0.0628\n",
      "Epoch 87/100, Loss: 0.0623\n",
      "Epoch 88/100, Loss: 0.0278\n",
      "Epoch 89/100, Loss: 0.0097\n",
      "Epoch 90/100, Loss: 0.0097\n",
      "Epoch 91/100, Loss: 0.0316\n",
      "Epoch 92/100, Loss: 0.0301\n",
      "Epoch 93/100, Loss: 0.0486\n",
      "Epoch 94/100, Loss: 0.0379\n",
      "Epoch 95/100, Loss: 0.0318\n",
      "Epoch 96/100, Loss: 0.0271\n",
      "Epoch 97/100, Loss: 0.0485\n",
      "Epoch 98/100, Loss: 0.0866\n",
      "Epoch 99/100, Loss: 0.0211\n",
      "Epoch 100/100, Loss: 0.0380\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Define model parameters\n",
    "input_size = len(word_to_index)  # Vocabulary size for the encoder\n",
    "output_size = len(word_to_index)  # Vocabulary size for the decoder (same as input)\n",
    "hidden_size = 256  # Number of hidden units in encoder and decoder\n",
    "# initialise the encoder and move it to the selected device\n",
    "encoder = Encoder(input_size, hidden_size).to(device)\n",
    "# initialise the decoder and move it to the selected device\n",
    "decoder = Decoder(output_size, hidden_size).to(device)\n",
    "# Define the loss function (CrossEntropyLoss), ignoring padding tokens during training\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN]) \n",
    "# Set up Adam optimisers for both encoder and decoder with a learning rate of 0.0005\n",
    "encoder_optimiser = optim.Adam(encoder.parameters(), lr=0.0005)\n",
    "decoder_optimiser = optim.Adam(decoder.parameters(), lr=0.0005)\n",
    "# Define the number of epochs for training\n",
    "num_epochs = 100  \n",
    "# Start the training process with specified epoch count and model save names\n",
    "train(num_epochs, \"math_problem_encoder\", \"math_problem_decoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the revised model\n",
    "\n",
    "We'll load our previously trained model, restoring the encoder and decoder models using their saved weights. Next we initialise the models with our predefined hyperparameters and load the previously trained model states using **torch.load()**. We then setup a question input (**\"thirty-eight plus twenty-six\"**) to test the model's ability to predict the correct outputted answer (**\"sixty-four\"**). The **test()** function runs inference, generating predictions from the trained model and optionally visualising the attention mechanism, allowing inspection of a heatmap that focuses on different parts of the input when making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, 'sam': 3, 'has': 4, '2': 5, 'marbles': 6, 'and': 7, 'he': 8, 'loses': 9, '14': 10, '-': 11, 'how': 12, 'many': 13, 'does': 14, 'have': 15, 'left?': 16, 'a': 17, 'baker': 18, '9': 19, 'muffins': 20, 'bakes': 21, 'more': 22, 'the': 23, 'now?': 24, 'train': 25, 'travels': 26, '17': 27, 'miles': 28, 'in': 29, 'morning': 30, 'evening': 31, 'what': 32, 'is': 33, 'total': 34, 'distance': 35, 'traveled?': 36, 'there': 37, 'are': 38, '8': 39, 'pencils': 40, 'one': 41, 'box': 42, '13': 43, 'another': 44, 'altogether?': 45, 'if': 46, '11': 47, 'apples': 48, 'added': 49, 'to': 50, '16': 51, 'total?': 52, '4': 53, 'store': 54, 'sells': 55, '19': 56, 'candies': 57, '12': 58, 'left': 59, 'were': 60, 'sold': 61, '3': 62, '20': 63, '6': 64, '15': 65, '7': 66, '18': 67, '10': 68, '5': 69, 'shop': 70, 'toys': 71, 'on': 72, 'monday': 73, 'tuesday': 74, '1': 75, 'sixteen': 76, 'twenty-three': 77, 'twenty-six': 78, 'twenty-one': 79, 'twenty-seven': 80, 'seventeen': 81, 'eight': 82, 'thirty-one': 83, 'twelve': 84, 'nineteen': 85, 'twenty-five': 86, 'thirty-four': 87, 'twenty-two': 88, 'twenty-eight': 89, 'twenty-four': 90, 'thirteen': 91, 'nine': 92, 'eighteen': 93, 'three': 94, 'ten': 95, 'twenty': 96, 'thirty': 97, 'twenty-nine': 98, 'thirty-seven': 99, 'fourteen': 100, 'thirty-three': 101, 'seven': 102, 'thirty-five': 103, 'fifteen': 104, 'six': 105, 'eleven': 106, 'five': 107, 'thirty-two': 108, 'thirty-six': 109, 'forty': 110, 'thirty-nine': 111, 'thirty-eight': 112, 'four': 113, 'two': 114}\n",
      "Input Sentence: ['if', 'a', 'shop', 'sells', '20', 'toys', 'on', 'monday', 'and', '9', 'toys', 'on', 'tuesday', '-', 'how', 'many', 'toys', 'were', 'sold', 'in', 'total?']\n",
      "Generated Sentence: ['<SOS>', 'twenty-nine']\n",
      "Attention Weights Shape: [[8.84172711e-21 1.70710383e-40 3.30616810e-29 2.50459788e-24\n",
      "  2.65757265e-15 5.44139843e-27 3.70189096e-26 1.00047742e-29\n",
      "  1.82319324e-23 4.77355989e-28 2.63010748e-32 1.96442665e-32\n",
      "  3.18533797e-29 1.32904276e-21 1.18423635e-22 1.00000000e+00\n",
      "  4.15441641e-14 8.57432639e-24 6.96633205e-24 2.98417107e-24\n",
      "  1.20811434e-29]\n",
      " [5.66692778e-20 1.12465282e-36 1.87613348e-27 3.62724997e-23\n",
      "  1.90113818e-14 1.05244517e-24 5.04168262e-24 3.13220304e-27\n",
      "  6.78143406e-22 8.51502686e-26 1.85111479e-29 1.34386772e-29\n",
      "  9.44852603e-27 5.75649787e-20 4.96027053e-21 1.00000000e+00\n",
      "  2.06609306e-13 3.44071693e-22 2.82372143e-22 1.28034763e-22\n",
      "  2.99374408e-27]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwIAAAJNCAYAAACCxcK1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAActRJREFUeJzt3Qd8FNX68PETECsiVrBjw9571yuKvSvqVewVe8WKFStYsVf0KvbeRbFiw+61N7CA7YqKIkj2/fyO/8m7CQGyye5kkvl972evZLPZ2Z3ZnTnPeZ5zTlWhUCgESZIkSbnSprlfgCRJkqT0GQhIkiRJOWQgIEmSJOWQgYAkSZKUQwYCkiRJUg4ZCEiSJEk5ZCAgSZIk5ZCBgCRJkpRDBgKSJElSDhkISJIkSTlkICBJkiSV4LnnngtbbLFFmGuuuUJVVVW47777pvg3Q4YMCSussEKYZpppwsILLxxuvPHG0NwMBCRJkqQSjBkzJiy77LJhwIABDXr8F198ETbbbLOw/vrrh7feeiscfvjhYZ999gmPP/54aE5VhUKh0KyvQJIkSWqhqqqqwr333hu23nrrST7muOOOCw8//HB47733au7baaedwi+//BIee+yx0FymarYtS5IkSZMwduzYMG7cuNS2VygUYqO+GGU83Jpq6NChoVu3brXu6969e8wMNCcDAUmSJGUuCFhg/vZh5PcTUttm+/btw++//17rvj59+oRTTz21yc89cuTI0KlTp1r38fOvv/4a/vzzzzDddNOF5mAgIEmSpEwhE0AQ8NWwLqHDjJUf0vrrb9Vh/hW/DCNGjAgdOnSoub8c2YAsMxCQJElSJrWfsSreKq06/LMNgoDiQKBcOnfuHEaNGlXrPn5mW82VDYCzBkmSJEkVtPrqq4fBgwfXuu/JJ5+M9zcnMwKSJEnKpAmF6jChkM52SsFYgk8//bTW9KBMCzrLLLOE+eabLxx//PHhm2++CQMHDoy/P+CAA8Jll10Wjj322LDXXnuFp59+Otxxxx1xJqHmZEZAkiRJKsHrr78ell9++XjDkUceGf99yimnxJ+/++67MHz48JrHL7DAArHRTxaA9Qf69esXrr322jhzUHNyHQFJkiRlCrPpzDTTTGHkR/OlNli486LDw+jRoysyRiCrzAhIkiRJOeQYAUmSJGVSdfxfOtvJIzMCkiRJUg6ZEZAkSVImTSgU4i2N7eSRGQFJkiQphwwEJEmSpByyNEiSJEmZVB0K8ZbGdvLIjIAkSZKUQ2YEJEmSlEn01E8wI1AxZgQkSZKkHDIjIEmSpExyjEBlmRGQJEmScsiMgCRJkjLJBcUqy4yAJEmSlENmBCRJkpRJ1f93S2M7eWRGQJIkScohMwKSJEnKpAkprSMwwVmDJEmSJOWFgYAkSZKUQ5YGSZIkKZMmFP65pbGdPDIjIEmSJOWQGQFJkiRlktOHVpYZAUmSJCmHzAhIkiQpk6pDVZgQqlLZTh6ZEZAkSZJyyIyAJEmSMqm68M8tje3kkRkBSZIkKYfMCEiSJCmTJqQ0RmCCYwQkSZIk5YUZAUmSJGWSGYHKMiMgSZIk5ZCBgCRJkpRDlgZJkiQpk6oLVfGWxnbyyIyAJEmSlENmBCRJkpRJDhauLDMCkiRJUg6ZEZAkSVImTQht4q3y28knMwKSJElSDpkRkCRJUiYVUpo1qOCsQZIkSZLywoyAJEmSMslZgyrLjIAkSZKUQ2YEJEmSlEkTCm3irfLbCblkRkCSJEnKIQMBSZIkKYcsDZIkSVImVYeqUJ1Cv3V1yGdtkBkBSZIkKYfMCEiSJCmTnD60sswISJIkSTlkRkCSJEk5nz60EPLIjIAkSZKUQ2YEJEmSlOFZgypfv1/tGAFJkiRJeWFGQJIkSZnEGgITXEegYswISJIkSTlkRkCSJEmZ5KxBlWVGQJIkScohAwFJkiQphywNkiRJUmYHC3Or/HYKIY/MCEiSJEk5ZEZAkiRJmTShUBVvaWwnj8wISJIkSTlkRkCSJEmZNCGlBcUmOEZAkiRJUl6YEZAkSVImVRfaxFvlt1MIeWRGQJIkScohMwKSJEnKJMcIVJYZAUmSJCmHDAQkSZKkHLI0SJIkSZlUndJiX9Uhn8wISJIkSTlkRkCSJEmZVB3axFsa28mjfL5rSZIkKefMCEiSJCmTJhTaxFsa28mjfL5rSZIkKefMCEiSJCmTqkNVvKWxnTwyIyBJkiTlkBkBSZIkZZJjBCorn+9akiRJyjkzApIkScqkCaFNvKWxnTzK57uWJEmScs5AQJIkScohS4MkSZKUSdWFqnhLYzt5ZEZAkiRJyiEzApIkScqk6pQGC1fntG88n+9akiRJyjkzApIkScqk6kKbeEtjO3mUz3ctSZIk5ZwZAUmSJGXShFAVb2lsJ4/MCEiSJEk5ZEZAkiRJmeQYgcrK57uWJEmScs6MgCRJkjJpQkr1+xNCPpkRkCRJknLIQECSJEnKIUuDJEmSlEkOFq6sfL5rSZIkKefMCEiSJCmTJhTaxFsa28mjfL5rSZIkKefMCEiSJCmTCqEqVKcwfWghhW1kkRkBSZIkqREGDBgQunTpEqaddtqw6qqrhldffXWyj7/ooovCoosuGqabbrow77zzhiOOOCKMHTs2NBczApIkScqkLI8RuP3228ORRx4ZrrzyyhgE0Mjv3r17+Oijj8Icc8wx0eNvvfXW0Lt373D99deHNdZYI3z88cdhjz32CFVVVaF///6hOZgRkCRJkkpE433fffcNe+65Z1hiiSViQDD99NPHhn59XnrppbDmmmuGXXbZJWYRNtpoo7DzzjtPMYtQSQYCkiRJyqTqQlVqN/z666+1bn/99Veoz7hx48KwYcNCt27dau5r06ZN/Hno0KH1/g1ZAP4mafh//vnn4ZFHHgmbbrppaC4GApIkSVIIsW5/pplmqrmdffbZ9T7uxx9/DBMmTAidOnWqdT8/jxw5st6/IRNw+umnh7XWWiu0a9cuLLTQQmG99dYLJ5xwQmgujhGQJElSJk0IbeItje1gxIgRoUOHDiExzTTThHIZMmRI6Nu3b7j88svjmIJPP/00HHbYYeGMM84IJ598cmgOBgKSJElSCDEIKA4EJmW22WYLbdu2DaNGjap1Pz937ty53r+hsb/bbruFffbZJ/689NJLhzFjxoT99tsvnHjiibG0KG2WBkmSJEklmHrqqcOKK64YBg8eXHNfdXV1/Hn11Vev92/++OOPiRr7BBMoFAqhOZgRkCRJUiYVD+St9HZKxdShu+++e1hppZXCKqusEqcPpYefWYTQs2fPMPfcc9eMM9hiiy3iTEPLL798TWkQWQLuTwKCtBkISJIkSSXq0aNH+OGHH8Ipp5wSBwgvt9xy4bHHHqsZQDx8+PBaGYCTTjoprhnAf7/55psw++yzxyDgrLPOCs2lqtBcuQhJkiSpHkzdyaw9B7+wTZimfbuKb++v38eHy9a6N4wePbpBYwRaC8cISJIkSTlkaZAkSZIyaUKhKt7S2E4emRGQJEmScsiMgCRJkjIpy7MGtQZmBCRJkqQcMiMgSZKkTCoU2oTqQptUtpNH+XzXkiRJUs6ZEZAkSVImTQhV8ZbGdvLIjIAkSZKUQwYCkiRJUg5ZGiRJkqRMqi6kM7VndSHkkhkBSZIkKYfMCEiSJCmTqlOaPrTa6UMlSZIk5YUZAUmSJGVSdaiKtzS2k0dmBCRJkqQcMiMgSZKkTJpQqIq3NLaTR2YEJEmSpBwyIyBJkqRMctagysrnu5YkSZJyzkBAkiRJyiFLgyRJkpTd6UNTGMhb7fShkiRJkvLCjIAkSZIyqZDSgmIFMwKSJEmS8sKMgCRJkjKJ8QGpjBEomBGQJEmSlBNmBCRJkpRJLihWWfl815IkSVLOmRGQJElSJjlGoLLMCEiSJEk5ZEZAkiRJ2V1ZOIU5/qtdR0CSJElSXhgISJIkSTlkaZAkSZIyycHClWVGQJIkScohMwKSJEnKJDMClWVGQJIkScohMwKSJEnKJDMClWVGQJIkScohMwKSJEnKJDMClWVGQJIkScohMwKSJEnKpAK99aEqle3kkRkBSZIkKYfMCEiSJCmTHCNQWWYEJEmSpBwyEJAkSZJyyNIgSZIkZZKlQZVlRkCSJEnKITMCkiRJyiQzApVlRkCSJEnKITMCkiRJyiQzApVlRkCSJEnKITMCkiRJyqRCoSre0thOHpkRkCRJknLIjIAkSZIyqTpUxVsa28kjMwKSJElSDpkRkCRJUiY5a1BlmRGQJEmScshAQJIkScohS4MkSZKUSU4fWllmBCRJkqQcMiMgSZKkTHKwcGWZEZAkSZJyyIyAJEmSMskxApVlRkCSJEnKITMCkiRJyiR66tOo3y+YEZAkSZKUF2YEJEmSlEmF2FufznbyyIyAJEmSlENmBCRJkpRJ1aEq/i+N7eSRGQFJkiQphwwEJEmSpByyNEiSJEmZ5IJilWVGQJIkScohMwKSJEnKJBYTq0qht77ajIAkSZKkvDAjIEmSpExiMbFUFhQrhFwyIyBJkiTlkBkBSZIkZZKzBlWWGQFJkiQph8wISJIkKZPMCFSWGQFJkiQph8wISJIkKZNcR6CyzAhIkiRJOWQgIEmSJOWQpUGSJEnKJBcUqywzApIkSVIOmRGQJElShjMCaUwfGnLJjIAkSZKUQ2YEJEmSlEkuKFZZZgQkSZKkHDIjIEmSpEyidD+N8v1CyCczApIkSVIOmRGQJElSJjlGoLLMCEiSJEk5ZCAgSZIk5ZClQZIkScomRwtXlBkBSZIkKYfMCEiSJCmbUhosHBwsLEmSJKmhBgwYELp06RKmnXbasOqqq4ZXX311so//5ZdfQq9evcKcc84ZpplmmtC1a9fwyCOPhOZiRkCSJEmZVCj8c0tjO6W6/fbbw5FHHhmuvPLKGARcdNFFoXv37uGjjz4Kc8wxx0SPHzduXNhwww3j7+66664w99xzh6+++ip07NgxNBcDAUmSJKlE/fv3D/vuu2/Yc889488EBA8//HC4/vrrQ+/evSd6PPf//PPP4aWXXgrt2rWL95FNaE6WBkmSJCnTC4qlccOvv/5a6/bXX3+F+tC7P2zYsNCtW7ea+9q0aRN/Hjp0aL1/88ADD4TVV189lgZ16tQpLLXUUqFv375hwoQJYUr22muv8Ntvv010/5gxY+LvGstAQJIkSQohzDvvvGGmmWaquZ199tn1Pu7HH3+MDXga9MX4eeTIkfX+zeeffx5Lgvg7xgWcfPLJoV+/fuHMM8+c4uu66aabwp9//jnR/dw3cODA0FiWBkmSJCmb6KlPcdagESNGhA4dOtTczYDecqmuro7jA66++urQtm3bsOKKK4ZvvvkmnH/++aFPnz71/g1ZiUKhEG9kBBiUnEgCivrGIzSUgYAkSZIUQgwCigOBSZltttliY37UqFG17ufnzp071/s3zBTE2AD+LrH44ovHDAKlRlNPPfVEf8NA4qqqqnhjhqG6uP+0004LjWUgIEmSpEzK6qxBU089dezRHzx4cNh6661revz5+eCDD673b9Zcc81w6623xscxngAff/xxDBDqCwLwzDPPxGzAv/71r3D33XeHWWaZpdZrmH/++cNcc80VGstAQJIkSSoRU4fuvvvuYaWVVgqrrLJKnD6UwbvJLEI9e/aMU4Qm4wwOPPDAcNlll4XDDjssHHLIIeGTTz6Jg4UPPfTQSW5j3XXXjf/94osv4viFJIAoFwMBSZIkqUQ9evQIP/zwQzjllFNiec9yyy0XHnvssZoBxMOHD6/VcKch//jjj4cjjjgiLLPMMjFIICg47rjjprgtev5ZjIwFy77//vuYVShG0NEYVQXyDZIkSVJGMEiWWXvmv+bk0Gb6/z9AtlKq/xgbvtr3jDB69OgGjRFI24MPPhj+/e9/h99//z2+PsYGJPg36xM0htOHSpIkSRl21FFHxfUCCATIDPzvf/+ruTU2CGhUaRDTKhF5zDPPPPFnUhQMfFhiiSXCfvvt1+gXIkmSJBUrXuyr0tvJMqYZZSzB9NNPX9bnLTkjsMsuu8QRzKAeasMNN4zBwIknnhhOP/30sr44ScorOlxOPfXU5n4ZkqQM6N69e3j99dfL/rwlBwLvvfdeHBmNO+64Iy6P/NJLL4X//Oc/4cYbbyz7C5SkUl1++eWxIb3qqqvW+/v//ve/sZH95Zdf1vu3aZ3LWAgma419Xg/7jlUz69OlS5ew+eabV/Q1kGVm9g1Jigop3DLogQceqLltttlm4ZhjjonnaKYRLf4dt9RKg8aPH1+zytpTTz0Vttxyy/jvxRZbLHz33XeNfiGSVC50TNBgJVv56aefhoUXXniiQIAFWNZbb734uLqBAAvF7LHHHqkEAgMGDKg3GGDZ+KmmyufEbgQCdDodfvjhzf1SJKnZJOsTFKuv+obOG1YZTiUjsOSSS4Yrr7wyPP/88+HJJ58MG2+8cbz/22+/DbPOOmujXoQklQtzLZOl7N+/f5h99tljUNASsYx8XgMBSao7RiCNW9YwRWhDbo0NAhoVCJx77rnhqquuij1pO++8c1h22WXj/aQlkpIhSWouNPxnnnnmmEbdfvvtJwoEKPvZYYcd4r/XX3/9mqXbhwwZErMD77//fnj22Wdr7udcl2CmBnqpmQuazCiZBs6JxfM5U27E311wwQXh6quvDgsttFB87Morrxxee+21mseRcSAbgGRbdaeDq5spePPNN8Mmm2wSp45r37592GCDDcLLL7880fvjb1988cW42A3B0AwzzBC22WabON91JfD+KeWho4gAhjm0999//zibRbH7778/HhdWwWSfsG/OOOOMWhcx9vfDDz8cvvrqq5p9kmRtOEb8TFkqGR3m4J5xxhnjcWbKv7/++isenznmmCPuHxb14b5iN9xwQ1yhk8fwGpjo4oorrphkCdQTTzwR5wbnffHYe+65pyL7UJKaQ8ndTZykqR1lflcutglmDCr3SGZJKhUN/2233TYuvU5nBY08GuA0xLHOOuvEmRcuueSScMIJJ4TFF1883s9/acyy2iONSCZAQLIwzB9//BFXeGTmBhq58803X8w8HH/88bEssm5NO+Utv/32W3wsjdfzzjsvvq7PP/88tGvXLt5PJpXM6s033zzF90WAsvbaa8cg4Nhjj43PkXTKELjUHQ/B++Ac3adPnxic8PpY9v72229v0H6c1HR0dRexAe+FAISGN/uWrAyrZxK4EJDwWsFj2LcEKPz36aefjgvxcD05//zz42PY7zTqv/7663DhhRfG+3hsMVbpnG666ULv3r1j6dell14at8HCPQQfBFAESGxvgQUWiNtI8HkgYKGslYwLc3MfdNBB8X316tWr1nZY9ZMFgw444IC4eihBBEEkCwYxUYakFKRVw18ImcY1qz5cX+iooGOK61vbtm1Lel4XFJPUagwbNiwu9U7julu3boHTGw327bbbrlZD/a677ooNOmZAK+7xBxMgMEaA3udiZ555ZjjnnHNi43aRRRapuZ9AgEZssvw7jW4an5RK0pBMOkzImm611Vax4ZkMtqVhTlagvtMwJ3ca8UlWgB59xhR88MEHYcEFF4z3EYAsuuiiYfnll4/BAJIGOe+f3uwky0DjmwvJTz/9FBfpmRS2R2/75NCr/9BDD8V/v/DCCzFAIQBjVrkEq2dSOlp8P+MeaMAXo5FNIETgkYw/Y/8wRqDuYG6OCVkcjtEbb7xRE2Dw/IMGDYrbYx8l1lhjjRhsFT9Pfa+Bv+NYffbZZ7UyAmQlGJRHAAcCFsbDde7cOW5fUuUXFJv3yj6hzXQpLCj259gw4oDTMrugGNcVsrp0SiXXFTo+6ISns4TVhrk2cF3jWlSx0qBRo0aF3XbbLaZ26U0h8ii+SVJzodFJDz6NRdAIpkeXRmJTaihx5513xgYvJ2CyosmNBjfP/dxzz9V6PNstzprytyAjUCqen0Y9A8eSIABzzjlnbATTGOeiWYwsbXGpEdvneWjcNgQNYAKqurckQ1K8X7hY00NevF9WXHHFeHFKpptGcQOcbAmP43VxYfvwww8bvD969uxZEwSAbAjBFIvtFON+1r75+++/630NXPB5DWR6OC78XIzrHAFYgsYB2yYYZPpsSWmoSvGWXX379o2ZbTot6NDh9vHHH8fz3MUXXxyGDx8eOymOOOKIypYGUdfKxk4++eR4ESq+0EhSc6GRS4OfIIDe+QQnyX79+oXBgweHjTbaqNHPz8n3nXfeiTX39aE3phiZiGLFPTilSnqB6P2vi5Imylpo8FLyUq7tk2ImM1IXKei6+4UGNDX3U9ovlDeddNJJsSSobuBStxE+OXXfW5LhqNsLxv3sG547mcyCUiUyLUOHDo37tO5rKM6WkGqve43r2rVr/C9ZBi66kpQGzp100DC2qvgcxXg0st50ZlCCyr8rGgjQ88SMQQyekqSsoHFJqQzBALf6sgVNCQRoUNLrTX1+fZIGYmJSGdK0qjHT2j77hSBgUrMzJYETA63peadXnenvuJgRVFBic9xxx9U79qDU9zal90zpDwOsKe9hVikCB8aSUE7EeIRSXoMkpYnrW3F2M8F9SYaSLCbZ1ooGApw4HVYgKWtoiNIgTWbiKcZML/fee2+c+pjSkMllMif1Oxquv//+eywFKpeGZlRpTFMH+tFHH030O0pqGCRbSk1oObFfWFNmzTXXnKj2vm59P6lsjgXZhkRx9iZRqUwz4zOYRYjxGsVZheLypWIMROZ6V/x6SMWj7voTkirEwcIR2W4mZrj22mvjuDBQpnjggQfGmdDw7rvvxrEEpSh5jAAD7pipob4VOSWpOTAAlAYmg0yZSrLujUG59JIkqy8ynWbSS10Xv6vv/h133DGWkzAIti4eX19PzZRM7nXU7ekmm8H0m8XnXsZsMTvRWmut1WyD29gvlGUxDWhd7JPkvSW99cUdSePGjYsLuNW3X0opFWqo+l4D22E2oPow0JgAMkE508CBA2NG3LIgSWm67rrrwiyzzBLHXzGxAjcmx+A+fgfGZVEKW9GMAAPgqKukF4gequIBW5Obck6SKoUGPg39ZKXzulZbbbWaxcU4h9GQo1HIGgA0BDmhJnPLc5JliklmCaL+kvv4HUu7sx2CDcZK8bgxY8bEHhhmIaKBXl9N/eTwHGDKze7du8fXtNNOO9X7WF4Pg3Vp9DPdJZM1MH0oPdzUhTYXyn3opWJKz7feeisGLFwXGDvAQGIGsRGMMYMP4xSYhpP3Sy87swXVl2FmvzDNKTMdMTiOi9sWW2zR5NfKa6MUiOfiNZPhueaaa+IxJu1eX7nX3nvvHaefZZD09ddfH4OvSQUOkirAjEBE5wPXALLASWaScWPFY8eSiTIqGgjUnStbkpobDXzqzSc1tzulM0x5yeMoT+GESpkQjVcaevRoUx5Cg5A555lZh8Y1wQUNXQIBOj6YopOZG2jg0jNMLzyNRabbnNyUnJPCtJTM98+YhltuuSU2iicVCDAQmPFZTFfK66aenYHQ/F3dNQTSxr6k8U5gwtoMBCmUzuy6666xZAgM1mXK0aOOOioOeiMo4PfU7BMEFSPQIaigwU3t/vzzz1+WQIALJkEb2z/66KPj54C0OkFi3RmHwDSxrFFAEEhZFil3ApS6r1eS0sIYJ27l4joCkiTVQSDDegXJegmSmmkdgQGnpbeOQK8+mVpHgKwoZZeUS/LvyWEChMYoOSOQzLxATw3/Je1LL9qjjz4aB18VT18nSZIkqXQMBh4/fnzNvyelKRMslBwIkBrfZJNNYrqXBXTOOuusGAi8/fbbcbACaVdJkiSpqahbSaN2pZDB+pjiGc0mNbtZU5U8axAzBiWD1hh0laCG9uWXXy7365MkSZIU/pnWmNnrmC0PTa3wLzkjwAwZTFdXF1kBlmqXJKmlc4psKSOcNShiogumayYzQCkQM7MtuOCCccILJl8oddrQRmcEOnbsWO80a9QuzT333I16EZIkSZLqd8QRR8SpmYcPHx5nsUswJfZjjz0WUssIMLUdy8EzfR4RCVPYvfjii3Eqtp49ezb6hahyOEYsjDPjjDNWbMVOSZLUOlBuwvTJc801V5x+uXlfTNU/tzS2k2FPPPFELAmaZ555JprmmCmvUwsEmEO7V69ecTl75t5eYokl4n932WWXODezsocggOMlSZLUUCNGjJio4anmwQKWxZmA4oV8WRQztUCAAcKsxMiiO4wXYGXG5ZdfPkYkDFyYbrrpGv1iVBlkArBW2DRMFWqvBC1Jatnu/fjdVLazTdelU9mOmt/fYXx4ITxS035Q81t77bXjQpasK4CkKofFLxuzonCjAwGWhr/kkktiD3NxLzORyuabb16x6Y3UeEk5EEHAVFUGApLUmnSYMZ3SDa8fOfJ/A2ezUE5cVfjnlsZ2sowGPyuxv/7662HcuHHh2GOPDe+//37MCFCi31glnz0efvjh0KdPn1r3EQRsvPHG4e+//270C5EkSZI0MVY6/+ijj+I6XltttVVse2+77bZxsp6FFloopJYRYLAC6QmmKjr88MPjYJLu3buHqaaaKq4uLEmSJJVFzqcP3X333WMmYL311gvzzTdf2cfjlhwIEHUwTRH1SIwkv+222+IgBTIFM8wwQ1lfnCRJkpRXX331Vdh///1jOVCXLl1i+5tFfLl17tw5/UAAyyyzTHjooYfChhtuGFZdddX4bwcJS5IkqaxyPn3okCFDwl9//RVeeuml+G9ut9xySxg/fnycqCcJDHbYYYfKBQLMClTfgBEyAUxNSb1S4o033mjUC5EkSZI0cXubBn8yO9DYsWNjYEBJ/tVXXx1vFQ0Ett5660Y9uSRJktRoOR8jUIzyoKFDh8asALN0vvLKK3HRt+222y40VoMCgbqzBOXNs88+G0477bTw1ltvxShs7rnnDmussUZcT4F1FcCiakyrev3114dPPvkklkqtttpqcVBHccaEx51//vnhxhtvjHVfPI7Uzr777hv22WefZnyXkiRJypLnnnuuVsOfAcPrrrtu2G+//WKJUFMXfGvUGAEMGzYsfPDBB/HfSy65ZCwfak3+97//hXbt2oXhw4fHqVEPOeSQ2NCn4U5D/+67746N+mQp7p122ik89dRTsZHP6O5ff/01DBgwII7yvvPOO2uyKgQUV111VbjsssvCSiutFB/HnLBsL0G51RxzzBFnYpIkScqtnGcE1vu/2YKOO+64MGjQoNCpU6eyPn/JLc3vv/8+NnqJTjp27Bjv++WXX2LdEi9w9tlnDy0V6yA8/vjjsbf+wQcfjJEXERijslnIoXjmJIKDxB133BHuuuuu8MADD4Qtttii5n5qtn766afY08/AamZV4jEHHXRQrVquZZddttbrINNwxRVXhF133TVOG7X00q7mKEmSlDfHHntsbHMzZT9tQ7IBBAf8d7bZZmvy85e8oBg946wdkKxmxu29996LPdusOtwSvfvuu+Goo46K6ZWePXvGYIYAgAY6QcB3330XUzOTcuutt4auXbvWCgISPC/BwJNPPhl/5vmefvrp8MMPP0zy+Yj6Lr744phxWWGFFeKNbMTk/qYYo8s5HsU3SZKkFpsRSOOWQeecc054+eWXY1vy3HPPDdNPP33snGZsAIuM9erVK3ZGpxYIsIbA5ZdfHhZffPGa+5ZYYolYBtOSFhRjh9LYppFNic7nn38e3xeNfv67+uqrx8fRc7/zzjvHyGvOOecM22yzTSzrKW5cf/zxx7X2R7Hkfh6D/v37xwY9AQHTsB5wwAET7bdpp5029OjRI67N8M0338TghCwFYxMoMbr33nsnu4rz2WefHWaaaaaa27zzzluWfSZJkqT0tW/fPmyyySYxGKBiZeTIkbFNyDgB2oypBQLV1dWxdr4u7uN3LcWll14a0yzs2E8//TQ2rlmqORn8m2jbtm244YYbwtdffx0jMBrjffv2jeMiCBoSjBNoCIImMihEd3vttVcstSKTMKmBwowV4HUyLev9998fR4vzOnmOSTn++OPD6NGja24jRoxo8H6RJElSttDGJgAgECAgYHEx2qMzzzxz7DCueCDAoFleBIsWHHbYYXFAa4Je6yOOOCIOkm0pGG19xhlnxIiKRv2ee+4ZS3YmFcwQAOy2224xG0BZFLMHXXnllfF3lAUlA6frSu7nMQlWZF555ZVjA/+ee+6Jvf3XXXdd+OKLLyb6e8qwCETY7wQMpIFuuummGFBMbr7ZDh061LpJkiS12AXF0rhlEJ3Qm266aWzwU61CO5SxARdddFH47LPPwpdffhnbiRUPBBZYYIHw448/1pTFEIkwaJYbv+M+etlbCmqrmNqTkh3KncgE0NM+//zzh969e8fG/qRwMCgTGjNmTPyZwdPMJMQA47r69esXZp111jhYeFKSRn3yfMxGRLnQLrvsEkeHUx9GkEX50uDBg2PkVzdzIUmSpNbloosuipPzXHDBBbHNSpXHzTffHKtKaH+nNmtQUvpCvTllKkyV+eGHH9bUwXfr1i20VKwJwI0xA/fdd1/soWeHv/nmm3HlNtYPYGwAQQ+ZgIEDB8ZAIQl8CASYIpQZfupOH8osQfyOGYOw/fbbx3UF2B7jBMgCUMpDxmCxxRaLjyHVQwBBzRf7mcdKkiTlTVXhn1sa28mi4gqcSihp+tCqqqqa/9LDPble7paIQbo06rmx4xk/wKDcF154IQ7qTe6jlIiAgQHEyf5gClGitgsvvDBOD8pzkcJhyqfiBcW6d+8ebrvttjigl/p9ggHKfk499dSadQMoQTrmmGPic0iSJEmVUFVo4ChX6tqpq2faoslhVhxlC9kJZg9aL2wVpqqaeKC3JKnlevzbt1LZTve5lktlO2p+fxfGhyHh/thh2VzjDJO2y3znnhnaTFf5jtHqP8eG4ced1KzvuTlMVep8+5OrTU8yBpIkSZJaUSDAFJtMZylJkiSpZWvwrEH29kuSJEk5DAQaumCWJEmSVA5VRTMHVfQWsm3UqFFxMhmmv2dyGRa8Lb5VvDSIxQoYtCFJkiQpPXvssUdc3Pfkk0+Oa1mVq1KnwYEAc+RLkiRJShdT2T///PNhueWWa77BwpIkSVJqClX/3NLYToaxoG8lyvQbPEZAkiRJUvpYtLZ3797hyy+/LOvzmhGQJElSNtEJnsZ8NYWQaT169Ah//PFHWGihheLivu3a1V4g9ueff04nEFhwwQXDa6+9FmadddZa9//yyy9hhRVWCJ9//nmjXogkSZKk+jMClVByIEBKYsKECRPd/9dff4VvvvmmXK9LkiRJeWdGoKKT9jQ4EHjggQdq/v3444/XmkqUwGDw4MGhS5cu5X+FkiRJUs5NmDAh3HfffeGDDz6IPy+55JJhyy23TGcdga233jr+l3lL60Yl1CkRBPTr16/RL0SSJEkqliz4lcZ2suzTTz8Nm266aay+WXTRReN9Z599dpxN6OGHH45jByoaCFRXV8f/LrDAAnGMwGyzzdaoDUqSJElquEMPPTQ29l9++eUwyyyzxPt++umnsOuuu8bfEQykMkbgiy++aNSGFMKpp54aUzpvvfVWc78USZKk7HOMQPTss8/WCgLAxD3nnHNOWHPNNUNjlRwInH766ZP9/SmnnNLg51pvvfXiCmmVGgnd0CWbmfGIBnqlHX300eGQQw6p+HYkSZLUekwzzTTht99+m+j+33//PUw99dTpBQL33ntvrZ/Hjx8fswRTTTVVTFmUEgjkTfv27eNNkiRJDWBGINp8883DfvvtF6677rqwyiqrxPteeeWVcMABB8QBw6mtLPzmm2/Wur333nvhu+++CxtssEE44ogjSuqJJ81x8cUXxwHI3Bh3cMEFF9QaoMxAZKIdfP311/FxDJhIpiyll33uuecOM8wwQ1h11VXDkCFDav7+xhtvDB07doyzHC2++OKxEb7xxhvH15uU6tx0003h/vvvr3kN/P2//vWvcPDBB9d6vT/88EOMuJgdqT78HX/P71daaaW42MMaa6wRPvroo5rHsD0yIMX7gPfIe55zzjljiqdXr14xuEpM6T3Wh7/59ddfa90kSZLUMl1yySWxw3311VcP0047bbxRErTwwgvHtnRqgUB9OnToEE477bRw8sknN/hveNG8mX333Tc2zLnttttuNY3cQqEQnn/++diQf+GFF+J9BA40iHnToLE+dOjQMGjQoPDOO++EHXbYITb0P/nkk5rtsAobDe2bb745PPfcc2H48OGxYQ3+u+OOO9YEB9xovO+zzz7h1ltvjQ3qxC233BK3TZAwOSeeeGKcPen111+PWZK99tprso9/5plnwmeffRb/S1BC8MIt0ZD3WBejyJneNbkxolySJEktU8eOHWPHNR3Md911V7zxbyp1iqf0b5ZAAKNHj463huJF08NOz3nnzp3jjUY2jX7mSaXRy+///e9/1wQH/HfdddeN/6ZBf8MNN4Q777wzrL322jFKomG/1lprxfsT9K5feeWVsZeelY9pWCe9+mQIpptuulh3lbwGtrntttvG37PDEzTO6cGn139yzjrrrPgal1hiidC7d+/w0ksvhbFjx07y8TPPPHO47LLLwmKLLRbTPptttlnN62voe6zr+OOPrzke3EaMGNHAoyJJkpS96UPTuLUEiyyySNhiiy3iLekYb4qpGpOaKEbPPT3p9LhvsskmTXoxNHYZCEHJEQ1oGtQMKGZEdJIROOaYY+K/33333RgwdO3atdZz0ItPiU2CQKN4blVKcL7//vvJvg7SLWQnrr/++pgxeOONN2IJVLKoGu+TbAXmn3/+8P7779f87TLLLFNrW2B78803X73bYjGI4oUg+BveWynvsS4CG26SJElqmY488shwxhlnxNJw/j05/fv3TycQuPDCC2v93KZNmzD77LPHRcboiW5q2mPZZZeNPf+Uw2y44YZhnXXWCT169Agff/xxLIdJMgKMG6ABPWzYsIlWVCsekMsYg2L06BO8TAnlQdTzMy6B3neyFTT6ce2114Y///yz3ucv/jnJHiRrMNSnvteXPL6h71GSJKlVKlT9c0tjOxlDx3gybpR/V0KzriNAGQ493sVo6FMv/+qrr8YyG+ZLZaAv/6a3POkdX3755ePf0ttOJqGcrwFLL710LCe65ppr4ngByncSjBVIQ7neoyRJklqWZ555pt5/l1OTxghQe96U+vMuXbrEqY++/PLL8OOPP8aecEqBmOWHgbbUzYP7/vOf/9RkA0BAwPiBnj17hnvuuScGKAQPDJQtZXU1XgPjERhwwWsonrGHrABlSWQQttlmm5C2cr1HSZKkFj19aBq3DGPymfrWERgzZswUJ6YpayDw999/x9mBGOxLI5ob/z7ppJNqNaIbgoGvlLwwsJbyIgbH0vNNQFDc6CcQoGec/xajZIdG8lFHHRUWXXTROBXna6+9Nsl6/PowaxF/S+8/r+HFF1+s+d3OO+8cAxL+y7iB5lCO9yhJkqSW66abbqopSy/GfQMHDmz081YVGlIwX+TAAw+MvdOsMMz0n6CenznyaaReccUVobUgU8FAYxrezDjUUrGOAMHaemGrMFVV7TEJkqSW7fFv30plO93n+v/r4Kh1+7swPgwJ98eZB5kivjnbLgv26RvapNAZWz12bPj8tBOa9T1Paj/QVGeWScbK0mmdoJP8wQcfjLNUfvvtt+mMEaBenjnti2cIYqYc5qqn57w1BAJkNn766aeY5VhttdVadBAgSZKklqljx441i97WnUUS3M9aXo1VciDAtJSUA9W1wAILxIG3rQHlQeuvv37c4SzYIEmSpGaQVv1+IWQSg4TJCDB75d133x0n0UnQ7mZGy7nmmiu9QIAFuZjTlNr1ZK565rVnVh9+1xowFqHEiilJkiSprJIxs0wYQ/UN0/aXU8mBAPOYsvLtPPPME+f8x9tvvx3GjRsXNthgg5pVecFYAkmSJKlR0lr1txAyjZ7/X375Jc4eybTyddeoYmKZVAIBapW22267WvcRoSi7kuzG32F85j/okqTS/PrbpBetLPcAUuVDbC8UtR/U/BgUzJTyLDbLYOZk0Vrw79QCAUqC1LIk886+EB5p7pciSSqzmSceP1ghn6e1IWWo/cDMPWp+TCPPegF9+/YN008/fdmet+RAgMEKlPyQGag7vRHThz799NNle3EqDwaRsPDbjDPOWCuCnBSOJVke/qZSU2i5jextx224DbfhNlrKNtLaTl63QSaAIKApg1DLJueDhRPffPNNOPTQQ8saBDQqEBgyZEgcD1DX2LFjw/PPP1+u16UyYmAJYzpKxcmi0nPpuo3sbcdtuA234TZayjbS2k4et2EmIFu6d+8eXn/99bDgggs2TyDwzjvv1Pz7v//9bxg5cmStBQ0ee+yxMPfcc5f1xUmSJCnHzAhEm222WTjmmGNiG3zppZcO7drVXiB2yy23DBUNBJZbbrmaBQ0oD6pruummC5deemmjXoQkSZKk+u27777xv6effvpEv6NtTqd8RQMB5i+lZoyUBFMXFS9xzIIGc8wxR2jbtm2jXoSyhfUh+vTpU7NOhNto3m2ktR234TbchttoKdtIaztuo/lVpTR9aFXGMwJ1pwstl6qCc0NJkiQpQxjgzDiFhU7oG9pOO23Ftzdh7NjwWd8TwujRo1MZ29IUjMudtkz7pOTBwgMHDpzs7xs7j6kkSZKkiVH6w9ShV155ZRg1alT4+OOPY5XOySefHLp06RL23nvvkEogcNhhh9X6efz48eGPP/6I5UFMaWQgIEmSJJXPWWedFW666aZw3nnn1YwXwFJLLRUuuuiiRgcCbUr9g//973+1bqxw9tFHH4W11lor3HbbbY16EZIkSdIkZw1K45ZhVORcffXVcXXh4jG5yy67bPjwww8b/bwlBwL1WWSRRcI555wzUbZAkiRJUtMXFFt44YXrHURMdU6zBgKYaqqpwrfffluup1MFsBicpPol8yY4f4IkZW/WoDRuWbbEEkvUu3DvXXfdFZZffvlGP2/JYwQeeOCBWj9z0fzuu+/CZZddFtZcc81GvxBV1pFHHhlryFgIjqlfmXNW2cH3yGPSvJgWedVVV43HoVLHI5n+jdW+JaWPwZbt27cPhx56aGipvF7k0ymnnBJ23333mBngWnLPPffE0nxKhh566KH0AoGtt9661s98GGlYsshYv379Gv1CVDmvvfZajBhp6LDeAx8iV4FuXgTPI0aMiONsunXrVpE1ODhRNEeDsxIXqbrvpdzv7aWXXorjnM4+++xw3HHHVSQYYDVISii//vrrmN7lnLnTTjuFcmstx73u+2jJjZ/i157W+6j08WiJ++vvv/+O4xpPOumkOLnJPvvsE1qa5Dj8/PPP8Toy55xzhllmmaW5X5ZSsNVWW4UHH3wwLig2wwwzxMBghRVWiPdtuOGGjX7eNo35EBbfmM6IXuZbb701fiCVPZ07d44n0jfeeCNGjZtssklshFYKEerrr78eXnjhhRazoEaxpDSEfcQUXeX2zjvvhNVXXz3stttuoUePHnHEPwPtObFX4qLNSYJtNqWGcEqYd5nZw5A0osspeS833nhj+PPPP8veIGEKNk6u5557bpyRodzvg4FcBBrMrrb55puH4cOHxynfDjnkkFBOrem4J+9jwIAB4YcffqjI52pyDcZy4rUPHjw4HpNKBwFvvvlmzTZb6ueqeIXUch53SphPPPHEcMYZZ4T99tsvXHPNNaElSY7Du+++G9ZYY42wzTbbhMUWWyw899xzrb+sMecDhRNrr712ePLJJ8P3338fz720szbaaKPQFI2+mv7444/xpuy6/fbbw/vvvx8XxmBaqQsuuCBsueWWsTdk3nnnrUij+r777gsbb7xxnEaWDyfbpdei3Bcjxjs8/fTTsSe3nJLep/vvvz+eZJ966qnwyy+/lO35adDQ+GfU/6OPPhp7iRnxz4Xpkksuib8vx3tI9tPxxx8fDjrooHjhThps5ca+Wm+99cIGG2wQdtlll3hfJRo7X331VWysEwyU+6JHsHzEEUfERgJZgcsvv7xsjZC//vornHnmmTHwu/baa2OZHt+TGWecMTZyk33WVK3xuPPdu/TSS2uyzWn0pLPv7r333rI+57hx42InDNP/ffnll6FS7r777tjLzWJD5fqOpP25ImBiasQdd9wxHHDAAXHfleO4J9c7elL5vB577LFh//33j52Y5Zbsdzp3ytlO4jhwHtxss83i7c4774wdDJy30FIzZmp4h9VPP/1U73mS36USCLCxXr16hdlmmy106tQp3vj3wQcfXNbGkpqO8gPGbdDYYGW+eeaZJ3z66afxv6RGk5NKOYOBJ554Iuy5557xYvHWW2/F+rUbbrghNnx4PU2VXIyOOuqosN1228Vtrb/++jHo4MJUDpxIGQfDhYKyDZ6/Y8eOtR7TlIsrDX0u0ttuu2384s4111xh0KBBMUBjf9HIbeoFNrkY0Ohg/99xxx1xf/E5KH795Tj2ZH523XXX2MvdvXv38PLLL4eVV1459laUGxlHBkTRUCjnRS/ZD2+//Xb47bffYv0w5zQCs2Q7TTnm00wzTcyaJun7ZEVIUrl8DsigEaQ3VWs87py/tthiizBs2LCaXvpK9npybiH453vfVMWvk0wQmdh27dqFZ555Jt5XiY4YygToXGCu8XJ9R9L8XBEgU/4w88wzxzGHjz32WFhllVXK0kGSXD8I8vjeffHFF/G7yeeYAL2c2Gdsh8b6SiutFI455piaTE1TkIUhI0NGmeCYTiSuVZUoLc0Upw+N6EQozpYVdzZR8l3xQIDIloF0nGA4CfAh5MYXisYLH0zqnZUNNPhpmM8333zxwvDss8/Gk/j2228fGzj0RJYzGGApcHqj6FUl5cqHksYUnxVO5vybcoimXlDJcNAT+cgjj8T3RFaACytlFp9//nnJz/3BBx/UbIMbpUCnnnpq7Hk++uijw6yzzhqDXE6+L774YpMvrpzIadAkjX3KXEDtOI2PK664IgZsdd93qbPeULLBvmG/8N3k+8v+SjJDHIumltfQcOa4n3DCCTGjwX7jM0dDd9NNN21ST1h9n0kaU7wfAgF6wsqF/cBnioY5F1R6CbmA877OP//8JgUD/A3Hml7Nzz77LB57ggC+H2Ts2A4zQfB5bqzWctzrO+YcD6alZnwT82dXsteT90BjjWNBB1dT8TrJXCYBJRnSddddN/Tu3Tsel3Kde5Pjz7llgQUWiOethx9+OB6HpgRNaX+uaOyTOePc279//7DDDjvERs9qq60WxyHWfV2NQQcV2dgDDzwwnmv5XNFRxTWrqcFA8esiUOY8wjmF5+Z8xXclCQJLRXA6ZsyYGEgut9xytaaQZL8TgHNuqWS5lpoPnZPJRD2PP/54zc/cCDj5bLGycKMVGuiwww4rLLXUUoWRI0dO9LvvvvuusPTSSxcOP/zwhj6dUvLLL78UVl111cJuu+1W+PHHH+PxO+iggwrLLrts4bLLLqt53IQJE5q0nb/++qtwxx13FD799NPCTz/9VFh++eULe++9d/zdbbfdVqiqqipsuummha+//rrR2+jXr19hn332KRx44IHx5+rq6vjf999/vzDzzDMXjj766JKeb/DgwfF18foSvPY111yzcOONN8bP9SmnnFJYd9114/MvvPDChWuuuabQVCuvvHJh/fXXr/l57NixNf9eaaWVCjvttFOTnv/777+Px5Pv68knn1wYNGhQYccddyysvfbacdtsg331999/1+zDUv3vf/8rzDnnnHH/HXnkkbV+98knn8Rt87kbNWpUk97L448/Ho9D4tdffy3ssssuhV69epXlc4sxY8bEz2bx52fEiBGFU089tTD99NMXLr744pr7G7u/XnjhhUKbNm0K66yzTvwuzjDDDPGzjHfffbcw44wzFj788MNGP39rOu733ntv4eWXX6513zHHHFPYaqut4vmsKftocn777bfCH3/8UZbn4jiwrzp06BD31R577FF48cUX42dt2223LWyxxRbxOJTDF198Uevn+++/vzDLLLPU7MOm7q80Plf44YcfCgsttFA8Dt9++21h7rnnLuy///613lcpeJ289mIPPPBAYckllyyMHj265j6O0xFHHDHRtaCU7XzwwQc1P3MNPP/88wtnnHFGzX2vvfZaYcUVVyxsvfXWhWeeeaak53/++ecLiy66aOHggw8u/P777/G+4v38yCOPxOsT221tOE40Uxc5tm9hsZP7V/y2yLF94/aKPx9ZwGdzUrepp5660LVr18KDDz7Y6OdvU0rKjsifcqD66msZYFfuuko1Helbamzp+WY2FHomGWlO2pWBUvSKoKm9OfTWksJfaKGFYo8avZ70FCboCaM3v760VkPQI0Vmg2mykp5/emBIidGjSl03Paz0jDS0x4hSBnqDmI6LdDfoceF21VVXxdKd9957L9b0MxaBhfM+/vjjkl43vTiUm9CDmuC52RdJXTXp6aTsYZ111ol/01i33HJL7BHkeFIyRQaPnil6kOg1oAdsySWXjNk7elsb27tKuRS1tfROMVipuGyDbXG+YLA15VWN7fX85JNP4ngT9j9lYJTXUCpCKv+6666Lx6Icg4bZB9TdFn82yajttddecUDe4YcfHgcRJ49tDL5vlM+QoeN4c75MBiryeWZ7yaD+PB93jjE17gyipjc1KdfimNMjzXmsUoOGKQmbbrrpyvJcHAf2FeWZlG9Q/kVGgykr+SywHTKoTcVzLLPMMvE8lswvTpkhmRlKNNl3TcmgpPG5YuwEU1uzT2hf3HzzzTHrQNkZ1y6QcaDHnp7xhiAbwnmdc2xxTTXb4DOUZKf5HHGcdt555/j6eTyZ81JLcBl3APYD1zquscVlcpQHMe6I7ZKNJ/vUUPwt3yeyVWThyDCyn5Pef8bpcN3lO5PcV+7B7mpeyeQ8888/f/xcFU/YQxuI8wvfl8Zq8FWUAZ984SeFmU84iSt7aPByEWLWoNNOOy2eLPr06RMbnZSklKs+ksY/qL3kApScHLmYUyJEw46GUEPUvdAT0FBnyUA1TqI03Lk40agC/+WEzmuY0sWImnwuFDQqCVZo6HEhoIHDfTw3qeMrr7wyXggZsMbMDFxESrnQEbhQOseFYfHFFw//+c9/4v38++KLL44j/0l/czySBi1fcvYbJ/LGNHZo0FKKRTDB/iKNyHGnvpfXkXyXk/reUjEWg+cnJUkAxvugTIASFySNNC5MNBT5bDW0sV73/RJ40Ugmlc+JjtJEShEI1Ki35gJcjlQ4x5WGEzP78BlNMKB+xRVXjCdfgjcaFE1pgPI9JJAlAOBznKABRwOoKQ22lnrc6+5PgiGCb85LNMz4XtLBQClVcr5KGkJZxXgGFtfkO8znimliGfjKOZBgg0GqNGj5LDS1wda1a9d4bqfkhPENTEX8yiuvxIY02+JcjMYG45X+XFFCs8cee8QxAQTifH7YPwSanH/5roNGNJ8BPnsNQWkXz01nAUFkUq7GGCNKMCn547uefI4Yq8W5mM8XpUilluByrmAmHzBVN6VMNNwpRUow1oHzCJ8Pjn1DxoJx3LimEdRRWsaxZWAwf8u+4fdJUMl3krIq6siZHalVcYxARPuNNkpddPDymWq0hqYO5pprrpiimpTnnnsupoyVXW+88UZNyQ6lD5RckNL97LPPyr6daaaZJpbYbLDBBjE9/vbbbzf474vLPX7++eeYJk5Q2rTvvvsWpppqqsLAgQNjqRH3bbTRRoUNN9xwiqlp0s+8Nl4XZUAgFX3sscfG0o1bbrkl3lf8PKQJe/fuXZh11llj+UZDUK7E40k5/+c//4llFO3atYv7BpQIkKaeZ555CosttlhMGZNup2SEUpGGKH6NxWUGK6ywQizlK0ZJBaUJm222WUzxjx8/vlCqO++8M76n5ZZbLqYk11prrcJFF10Uv/uk9Lt3717vayv1mPO5ZJ+Tsi/+3bXXXhvL2tq2bRv30+KLL16Twm3o9pLHUTJQXOZ43333xec77rjjCh999FHN/YceemjhvPPOi/uv3N555534fvh+vPXWWw3+u9Zy3IuP+eeffx6/58VlYHjsscdiOcpMM81U6Ny5c2H22WevOT7lKAsrN8r85ptvvlgGQinjn3/+WbjrrrtiGU3yeaPskO88x72xJVRfffVV3FZSVshzDxkypLDJJpsUVltttViexbHi/JPVz9XHH39cOPfcc+N3LkGJDWWr//rXvwoXXHBB4e677y4ccMAB8fiX8h1JvPfee7HtsvHGG9ec76+++urC6quvHksMKdn55ptvCscff3xhlVVWaXRJCH9HefTOO+8ctzN06NDCvPPOG0vC+J4XGzZsWPy8N1RyLDjWp512Wjy2HA+uIcW4pvMaFllkkXg8KlVC1yylQcf0LSx2Uv+K3xY5JpulQQnaKPWdMyj75neN1eBAYM8994z1rdSC18UHlDpqHqNsoyFKXSeNTk6AlbqYvvTSS4Vdd9011nJzMm6o4pMXJz1eKydyPnvUDvP5ozFP7SgXutlmmy1ug98nF8UpvSca2lysacDwBUJxMFBcJ0ojfssttywssMACNY34KeFCQGBCI7LYeuutVzjkkENq3UfNO9ulXpwaUAKIpuAC0KdPn0K3bt1qXShoJNBAoBZ+3Lhx8b5SapR57+xrGuMEZzTYevbsGcc6XHrppbFROP/888fgr1TFx6tv376xobnEEksUdthhh8Kbb7450eNfeeWVwgknnBAbhlzAS3XPPffEmkrqbnn9X375ZU0Dge1yH8EyDQVqb2mwlBufVV4H40FKCZJby3EvPuann356DDI4JnRU0MCsi+8Fr4HvIY2rLKORzHeaayKNS2r4aaBvvvnmtd5PcQdHKRi3ROObG5//umMEnnrqqTiuhXMcjUIanln6XHGO5xxJQ5lOmbrtBhr8fMYYk0VQwL6r25guBdcfOinpKOJ8i+uuuy4GB1xDCMoYU1HfuaYUBBWMl9hrr73id4VxQUkw0NDOnSkhsGTs0qSCAb6vdT8PrSEQ6Hp038LiJ/av+K3r0dkOBPi81h33knxnuFZVPBAg2uzUqVM8uRDFM3CHXrSzzz47ftjnmGOOwvDhwxv9QpSeV199NTacG3shKuVi39heCRoH9EIyOJcAgEY7jQUGOHPB4fNINoNeNRpwifoC1cn1FE0qGGAAGLhg9e/fv6SsCb1zNABoJBU3erjg/fvf/47/Zr/UDVgaE5SxP2iEcWFOehzpbWLwafEg1+QikWyj1B48AiIayZwgk2NKo5DGMgEOF6Snn346XlRLOQ8Ufz5OPPHE2ON76623Fm6//fbYkKKRyQUWHPfk9bM9zj1czJMBdA3ZDidMzlVnnnlm4frrr48XbhqXSWOJwck0fAhG6N1rTC9kKcFAQ157azvuxZ9zevs5HlxLCPBorDFAm+OQPLb4M0KQzjHLyrUmeW28Hnrp//vf/9a8bhqW22+/fQxY+e5z7Uwyjo3F++c7wnFh0gS+Iww8rq+Hmc8ujemrrroqM5+r4mPJZA1klGjs03FUjPM452Ma7qUM4C5+/uJ/0xDnOBDQJMEA3z0a688++2y8npQD+4TrVHEwsOCCCxa22267kjp5iifC4LvBa0yOMfuDc1QSDJRrgHsWGQj8g88UnSS0Tcj68O/ktswyy8TvJx1nFQ8EwAeRCy8vJhmxzL9pTDFjhFoOehayiBMgqS/S0aTQi+23336xUZL03NBTSw87wQAny4Y+f92eovqCAXqq6DWq+zcNVdyLnPSYnXTSSXHGmGLFJ5xStsNjaUgyUwC9s+wvbjQUCF64aJPJoHSqHAEHz8tFOyndSC789D5xHqAxiIZelOoGoTT8ONklDYKHH344ntw4ydFQTBrqxSlvShX5XUN7wF5//fX4OaHxWXxsKNsg4CjuOeX+5LhlSUs+7nUzH/T8U6JBoxO8p44dO8aeZsroioOBBA0jvrPl6mFtiuRzSPkK2QwafJSw0EAv7ji44oor4jmG/UWDsHiWsFJQJsW5iZLIBIEFnwNKdJKGIsco6Z1ntiWChSmd79P6XBE4sq3k9fD56dKlSwwsi3vkG/NZTY7Hk08+GUsxCZC4hiTPmwQDBJtJmVAlFAcDlDcySxDZGzLwpX6uKB3lmsff0/ZKyrOTYIAOC7LJrTUYqAkEjupbWPyE/hW/dT0qm4EAWSBunEPoAE1+5kYWnc6zhnaCNjkQSBDp0nvDrZJfKOUTnykurMkFr/jCSR03NaMJGiT0ivAFmdz0WcnJlRMmF8nkOZOLQ91ggHIj0sXFPaGNUXxBo8e7uJ6aLzB1xA3tTZvcxZGLDeMYknpYLuLsw2QKwaaWgDE1HcERwUwxymq4SFET21Ck/enVLQ6W6PWnkZNMh0ev5+WXXx4vqvSkknWs22tI44RAoL4pjevieLM/+JxQslYsCQb4PdvIWm1tazjuXLzoyaJspTgQT6ZYfOKJJ2LGmWNORwANKQLBulNGUh5ExqBcPbhNRRAz3XTTxcY+x4JyL0qpaPAXjzXhs857K55mshTsZ4JisqR1p7gkO0BNPQ315DuVfIZ79OgR76+vkZD25+rRRx8tbLPNNvG1ch5M9gXbIhggW9rUDBz7f9ppp43nGBr87DOy38nnLikLJQBNzveVwHmLcxwluJSKldpQJ1vCsR4wYED8mYZe+/bt43eIQAc8J4Ee77Mh58CWyECgNgLbSQX1TekcaVQgIJVLfRcUGuoMtiOtnkguZJzgk/nXE/S+cUKc1EDe5KJILzPlHow7YJDmQw89VPMFSsqEksCWtHFT58Cvu30CAepdQa80jdKGXviK9xOZCoIhAqC6aX8uQEnJC8/PoOhy9RbR+8icxTQQyACyf3hPNNIb2tsFGkg0ltgXxY0lLswce+qOk4Yn75t0Pr3A3J/cx8mQ+uhSGg6UblD2QLlEMud2cmwIxpKUa5ayZa3luBOw89p4XTT6E0ljjO86A1s5HtxIc9M4JEAD9xGw0eDOQjYgwViV5HOZoAeaToS66yw0FSWKZGdo+NWtEyY44PObrIXB94hzGZ0clII29+eKgI6AiXFflF2SdaBhm1QSkBlgPANBS2PHA5C14vtbXMJEAJCUsSVj1chM0aHE+SArJbjJeYj/kjVhXEEyiJrvGN8F9g1BGesgJJkBzlWMmWutDAQmjzI3vrO0aVIZLCyVW3FPOL1DnMiThji9RFwokgG3SQOBusjkBFncczulXnUuRPQUURtOY4KeMr44SU0vFwl6iihLIONVTslFl1Qu5U0sNkMva2MG8RHw0FtOQEMDhBIK6o/rvn96wJnZhbEKje2FrIv9TYODnlr2FT2EpK5LeR9JI5uGIT1eLBJVXDvLRY/nvPnmm+PPfB5oFFImUnyxnFKvZPIYgkOyDcl4DXqS6cnmxJnUmRcHA8nA4axpyce9eGYbGpQEgJS5FAcDPCeDj5MgnB51etvrHvMsZWt4LRwDJgZIPo9JhwWfX7JVfN7K+ZovvPDCeA6kM6RuA5AAKykJSrY5pTKkSn+ueB00VsgCnHPOOfE+9gnjcjgXFr9WMoGMFyilU6EYDXs6dJilqRg96DT8KbVJNKWMohST61Sob3xFcu0hAKOxTzaB7Bgz5SWfK2ZN430WZ9dafSBwZN/C4sf3r/it65GNCwQYW0N5Kdd1viNUyzQE51W2x3WwFIwZoVOU2fMIoGkT1RfwN5SBgFLHIM9kACjoaaSXll5f6iqTcgPSYHzQSePSY0g9JCf0Ugc8UqdJrzIXUdCbxok0WZ02Qe9yJXuKCELoVaOOuPj9T05xY5eBZ/TwcRIA+4GLP3XVZDgSSWOAHlR6kghAyonGMg05MiyllGgUvxd6dZkekP1B+rx4SkimUqVBwEBxeg457snfNqQkIWlYMMic988xpTeSXjZ65wgA6FUjGEhef5YamK3puNd9H8x4w8WSnv5kfAFofPJ++I6sscYaMWBI3k8Wpgnl85G8HoLTZLYWSlF4P0m5RvJa+ezxuWtK6SzjWWg8cx5kfEuCjgT2EcFAfTOIFM/eU/dznfbnigY3z0sASdkSwQsrBidBAG666aaaqYLrzoIzOcl7IwPDdzpZ0Z5GWd33SvCUxVkN+U4xMUYyTS/7tzjA4zPAsWZsBgiOyW7QQVbuab+zqCUEAoMGDYoZUzJndGoRtPEdmlJFAZ1hfBc4FzYkEKCjlLYTbSU6GZhlkGnUmzrTIJq+LKdUghdffDHcdtttcUEaFnBiIRwW7WIFSRb2YlE6Fq5hJV9WhkxWrpx11lnjKq8sbMRiKaUsxMOiUyyywoJELPTDojIshMSCVGAxFhaXYQVQFoBp6KJnperevXv8L++N1SKnhEA9WZSJ98uqlSwewyJXYD+w+iqLN7FPeV6wEBOL87DgDNthsZHGLihUHxbY4r2wWBIL6jRU8l5Y4Zq/Z7VlFox69NFH47HnGPAY/r3AAguECy+8MC6uxoqy3M97aMgiVSwSxCI/rEbMQjwcU44xq6IeccQR8fdsk88FiwuxUFGWFqhqTcc9eR+9e/eOC2qxOBKr67LQ00knnVSzUiwL+7GgE6uSszgT7yl5P+VYPbqxeD0siMjng9dz7733xpV7WfCKRRk5Liw4yKquLBCYvFYWfpp++ukb/bniO3LwwQfHzymraLPPHnzwwfg7VvploUIW2WPhw19++aXW3/I6E8XbT/tzxcJZrHjM49kGCzbyXxaIS869P/zwQ1yNmveJhq7szHvhvfG3nMtZVG2WWWaJiw6y6NLQoUNr3iuPZYExFivLGvYzC01uv/32ceFEvge81sTvv/8eV6FnP4HF3RZddNG4cBgr3+dFVSG9G7g2Fd9YvXdS+vfvH/bdd994vWHBOxbC47t//fXXT/Jv+D5xvPmsNuQ48p3huNP+YRVu2jHJqtvl0MqWn1PWrbnmmnGZdFZX5ULKRZ+VJDfeeON440LBRYJg4Mwzz4wNNR5b90s0uZUTk4sEDUACCFZtZTXfZEVMGjJXXHFFzRLxDz/8cHy+hRdeOEw99dQVe+9cRItXXJ4cAiS+7JwsaGhwYunZs2dceZhgilUmE8sss0xcWbm4QcDFm4bJPffcExsyzdmYKsZqn5wgWb2ZYwuOE6uv0thhBVFWLOXGe2WFTo4lDZeGrpbJiZuGP43+/fbbL66uSkONhigX0j///DNccsklsRFBIEJDJSta43Fnhe2bb7453HDDDXFFaNDI5b1wLuA7R5BOY473wgrhKOWYV8KoUaPi61xvvfXiaq4EjnvttVdcwZeVajlvENCwYizviwbpCiusEBvMrI5MAMuKuaXiAn/77bfH7wgrBBMQE0gRAIwdOzYGTLwGGoms6tuhQ4dMfq5YYXrIkCHhgw8+iI2Y8847Lx7n5NybNKLYh2uttVb8uaGBE49j/++yyy7xu8y1Azw3K4BvvfXWMQjh/EFDmtW7L7jggpA1HFe+H3SGbb755nEFaiSdHjQsWel4m222iZ0jdIwR5DR2JWc1zLzzzlvrZ9oqBGl1ce0g4KXDKcFxY4VvjtOkEMjNMcccYe+9946fzSkhUOb6SOC/yCKLhHIzEFBquJBykezRo0e8sLDcOhdLGv2JDTbYIJ7kCQb48hGJJyf5+nq8JtdT1KtXr3jhJuqml59GIRcIIvbkQjZgwIDYe8dj0ugVbkgQQLBw9tlnx5MMDYJnn302PPfcc/FCwP5hv7Vv3z5mSEBPGL1IdbMk9O6RCal7UmtOHB8aGUmPMq+Z3tWHHnooBgMccwLDpZZaKp4ok4tiKQ1Cep05EdMo+/nnn2MAQGMu6emkMcRnkQCT/dqcjc3WeNyT72CC480+Tnp7eX98H7m4kYU755xzYqOTnvYkCOA5mvu40IFw1113hf333z/2wvEzDXAyGaCHnkYoF/xdd901bLXVVvE9cVwIdhtzweYzQLBMo58ggG1wbmK7NGhpOLIfaTTyXUn29eSyZWl/rgi0eY2HHXZY3H+8fs7HNPg/++yzeH/Xrl1jA4rAgmCh1M8qAVGS4dtnn33CH3/8ETOKDzzwQGxc8V4ff/zxMGLEiDD33HPH97zYYouFLOHaxmd8xhlnjBkB9u0ZZ5wRG3ydO3eOjyF7zfWRaxRBGx0lWXsfqYgFOyltJ4T4uSkOsDmH1YcOATomOTcU42cqHiYVIJPlowOsoZK/oaOUTs3ddtst7LTTTqFcsnEFVKvHhYogADT6aJjRE0hkTGqUHikahKBByMWNCPz++++fKBAotaeICySBBT1opPrPPffceN/nn38eG4ZE5FlqLHNhGDRoULwwc8HmIk6DCVzkaJTQk0p5QJcuXeJ7Y1/SI5ng5ETA1Jzvq26DMDlB0jjnwkxDidfI40jb81rpGSMAKO69K7VXm31BKpWAgOfjv0lvDq+HbAO9cFPKLKWtNRz34gZp0iAk+CXw4nvG957zAI1MGmg0COlppnFDIJDISqkWwSQNZXriyBAUX3z5jIEeexqlJ598cgxqmvoZINDgGPIZpdHMOZLG4Y033hjPXzQCaNBzzmQ/FZf8NPfnisY33zleI6+PczvfN3rquZ+GLhkIyo449jRwCPpLxXsm00eDmfMJQRFlEwQDvHb2F6UafM5oxDUka5L2eTFpXNIZBbJOBJL8nvLIpHHJ/iFgq+98qsro0KFDRT4zBOV8N+iEKi4BmxKyQtz4rvLdp6OBjBfnW86ffC/5njdWdq6CarWKL1RccCgR4KJ50EEHxZ4bToT0GtFYSy5QlI2Q/kyCg6b0FNETRU8RFzp6HrkYclHnBMsFqTEXokpjf9E45mIwePDgMNdcc8UTCI0lLnQEO6ecckq8mFL+9PLLL9fU8vLfyWVN0m4Q0mvCa+Q+eoI5NjRu6P2lJwyUKlBewbiQUo95fWj8g8ZCcTkWZQ1kCOjlTQLTLGnJx734mBOE08NJrTuNNY43jWneD41PAjC++xxrgr611147ZBXBABdusok0XOmZZ9xSEgywz2nE8T4obyL4KbXBRsna6NGjw9JLLx3LQcA2yZyxv0CZEdujUcgt0ZBtpfG54jxPSR6NfwIWyvH4PlM/TeaX8SV9+/aNj+PY85yNDcTZxzw/ZU6c79kf7CfKnQgCeD/8N0uBPpLGPFkk9gnnIM6JBJiUrbJPeO08ju8LDT46q+g9buj4iVYp5YxAQ9GY55jRnijGz0lWpxgZMc6LSScCkvE2fFYZ9zO5sSxcx6hy4MZjyRLQ+UCmkIwd7ZzGqGLEcKP+UioRvUE0ELhIcMFJ6hxJG9NjRGOQgIAa1WINHSSa9EJSh0pancAi6Sn69NNP40mXiwMlQjwfX7xKjgkoBwZPc4HjfdHDxcU7QX0wFwf2Y6l19GlhnAeNHBoUBGY0wkmJc1yogaaRQM8wmR8GL1IuUM738uabb8bPAuMzCA5ee+212DNd9zOWNS35uFPKQO9vMsaHwXAcW+rBaexQRjPnnHPGcgd6c2nk8H1MGp1ZxXmEhi1jAjiPJMEAGJxOHTwDqktFfTHjAmicE7iSYSBQpUFNxwk9gAyCJWim04J9yHFvzP6q9OeKGnZ6LXmdDKzme8fzkAXgu04nQDl7tsmaMNifRlBynWBcB8E/QdmkSjqaE+VQfI7o4eV7wXgSxirRQQauW1wTyWDTkcV+5PjnEeO9+Dwueljf0Haafzp3KmnCX2PDRxefEIPyhmYEODacE5LBu0mHF59DGuh1OyppixQjA8jn9eKLL47tolLbJJwHKCEkaDQQUKZxwWdswB577BFrtDl501PPTBLUc3MBpIHGF4ITYlNmeBg4cGDsKaLhT08RPXn0FJFmf/fdd+NFO0sNpylJBrvSi0bPERcRyqcYeE0PW6nBUiUVX+S5ENPIoTeYYIDBigRpBAG8VgaQEhjSq0JdMhdIjlm5U+D0vlFbywWFXrbiBlyWtaTjniAAYIwHgd3KK68c7+N4Jhd0ygJpKHKcyQjxXeXfWXsfkwssCWjJEtCoTXrvG4P98tVXX8XGOANpCSQ49xFIUb6TNC4oo+P7QQOBjBbnrqZ8R8r9uaJnmzJL9gt/x3mWBizHmc8DkzTQc5l8FyvVqKUmm3MKGebGlhxVGvuec2AyKxTXO14r10QGBCf7iWsh10w6LBjLkVfJeWOxQ9MLBD68pLRAgECd7xAlhHxn+dzTycXnkQwcbQ86u/hO14c2EZUKBH/NxUBAqaDngxMzvVDUjtIw46TIhYOZe2gs0ntDjxKNw6Y2ClpiT9HksK8YQMgMHPSokz2h9zyrGQ2OI40AAjEGUYIUJr1b9IRRSsFYAHq9ksGklezd5jPA87e0GtuWdtw5rrxWgjpKaJithgskjRp6OukV5n0Uf/+yltFoSDBARwOZDoLaxg7eZJ8www09eWRPkt59egap/6VBQakQjQR68mlos5/KkTkp1+eK10IQw3En00Mmg7EIZALogKEElMYcv+e7z8BgZmcrN157v379YnaJUpqkxLS51BdI0dQiq0SJKnXdxdNGEiDTUcZYjuKykbxrCYEAGFdz/vnnx+8p5Y60YZKAl3FRjL1hjE99DASUK/R20NvFxYMLKY10sgH0hpCSZlaXRDl7CFtCT1FDkLrngkf9IT0QyXoKWWtEMWCQHscxY8bEHs7iqQkJBjjhcZJkJpTiNRscDNeyjnt9x4vvMFkgejfp/aXHnF5NSlEYpE/PcfGA05Z6zCkx41xGo5Myp1LRMKYxSFaUciJ6EMkIJAgC6DVOplFOlLN8qpyfKxq4vFYCezJBTNLA7Gxkh5LZiIqnhi03Pl/MtkSDKysTPzDzDOVvlHTRwE9myOP7wOeGQCnBOgFMocrvCfhUJxA4JMVA4NLSA4GWLlstCLVq1KbS+KcHKplajwY/FyJSasXKFQQU9xSRYm+pQQBocDBtYCJrs94kGBTIIER6QOgZptGfjAehZpKGDI0ELtqUVyRaYoMwr8e9OFCnQZlkdZiti55uSoOYDIDvOz3ANNLoDa7biG2px5zGLuVuyaD0UtAoJEAmYCIwIkNJAEXGMhljwKwxBNKMpyoOlso5hqKcnysat5R5UXbJWBDeDxk/Xn8SCFQqCAAdSVkacM4sWQRBw4cPjxNSENhxzqPklQwpnWLsn2TfMECbWykLZUrlYkZAzYKeIxrn9BJSJ0sdaSUaN1nsKWpNJpe5obyB0hAWQKKRU9zDwkBI6pOzPDhU9StumNJbTeOPOc6ZJYNSEMrBGBRHIzmZIYZyCBqaTI/YUhv/5UBnBL3/BMfUDoMySeqHyYwyhqV4wHGyr1tS5oRGMNkMyiWY6YjBkU2Z2rClIgNCZoQySTLgHOekDIhgiTIsPgOUlpEhIDjksXlaMXhKzAikI3vdiWr1uKjROKennosGvfblqn3Nek9Raw0CKL1iICP7m/pc0uFc7Bg4SFYAxcEAjR5kfaYYTSxpkDLdJI0bGjD0ZjL7BVPAUn/O4DgGjDIVIo+hUUQjZ0qLX7VmyYw9ZECZHSRB5oRzImVzfBd4TNIYbGlBAK+Vwd9891nBncxvHoOAZKpHbpwPCYY4RzIwnCwMx5OAkGNNJxXqjhvQ/1dV+OeWxnbyKH9nYzU7ToJM6cgAYVLHyQJDNghblqQxR10rAw+pd2YMxo477hjr/5OBj6S/KRVhRhLKHYp5zFsmGrOU+pDZYXAj5UAMouV4EwTQ2CcbR+kLA16pqU++53kMAkDGhKCYaUIJkJjBLMEq6EwVSoaUDEuxlhIEFAcuYPxXY6ZTbS34vHN9IwtG7z8BM8EAWKyN9Q8YrM3vOW+yirDUHCwNUrPLaw9ha8AAOOrCaeDQ4KckhEY/MyEQHLB2BJgtimNM7XBLatjoH3V7pWngMzMMg0SZB53yL2bNoASCAIAsAL2fZICSOnqzP/8gc8b3gf1Hxqx4Olu+R9SQu59aF74vrD3B+ZFyIG6MESFTxho6mnxp0OK90isN+mBA/kqDbH2p2RkEtFzUhlMWQqMGNPqYBo/p05ilibIv0BvGSqnFPYZqOeoGb6z9wGJhlICQAaLMjyAAjPmhFp4xQEkQwDG3cfsPSkUYLMp3gwwKUx0nGEuRrOir1oOSH9aEoHFJwMzkGATLdJhIzc0WmKQGSZZCr9sgpCacRh+SRj6DIUmHMyi8OOBL5vNXy0MNO72aoEFDqQuzodC7yWrdYJ0OysLo+aQ0JOExr40yELJjfG9Yi4DvUDGDptaHhcEIBpgpjXECjJnJ64rBJSukeMshBwtLKql8i2kTSZ1S+02DhgVTyABQCsRiKph99tljpoAZY4qZ/WmZqO1nIDgzwTAAklluyPiwGCClYCyMRVBIqRCLB9LbnQR+HvP68d1hfzKtZJ5r6fOE8TN77bVXc78MqRbHCEhqMOpa6dViwOOXX34Z50Cn95cZMMgAMBsQ85OT/mYA6csvv2zvZguUNOCLxwYw0PvWW2+Ns9wwzz3HGAQFLNrH52DxxRePswdlZdGzliDZxwZNUv1jBJY4KL0xAv+9PH9jBDxLS5qk4nnMqf1mdgumuWMVVOqc991331jnTHkIv+NnVpMlG8BCOkm9s8FAy1HcIGW2n2SxPzIBBHr8nqlgeQyz3BAc1m3EZmHRs5Yi+X4ZBEhqDp6pJdWruHFHuQdrPqy11lqxYUjDnmlDaexRN37BBRfEgOCss86Kj2fVTBo49gq3LMUNUhb5W2211cKFF14YZ7fB9NNPH4MBesxYJZqAjxVU6zZiDfxK4xgKaTLSqt8vhFzyCi2pXknj7sQTT4xZANYJoJaZmS7ICICBbzRiCAqYW56VZWksJoGEQUDLwSrQ3377bc0Kt0wDyvE87bTT4mfhkEMOiY9r37592HDDDWPQRzDAz5QLSZJaHq/SkiaZCRg0aFCc+pPyD+bCZkwAs51QGpIMcKRePFlFlgGlCUsdWk4WgNmdqPVncPftt98ennvuuTirCT3+HEeCQSTBwMwzzxynid1mm21iUCBJapkMBCTVkjTgmQv++eefj9NGMgYAiyyySGwwUvpBr3ESDLAqKsFCUu9sqUPLwbGaccYZY9DHonAEAawC3bVr1/h7xn3guOOOi7MEMUsUA8YJ+jbeeGNLwCRVVFXhn1sa28kjz9ySJjJy5Miw9957x3KfpEEISkBo6BMcEAzwGBbLgUFAyw8AF1poodCpU6e4YvQ888wTdt111zhD1P777x//y3gAMj8dO3aM04Ymx9wgQJJaJnP3kibCYlH33HNPbPzR8Hv33XdrfterV6+YAWDGmCeeeKLW3xkEtFw07jnWlAa1a9cuXH/99eGWW26p+TyQGXj//ffDI488ErMGPIZMgMdcUkW5oFhFGQhIqtcyyywT1wf48ccfYykIjcAEZUH8LikbUetBo5+Frhj0PXDgwDhGhOlA11133biI2HzzzVezWJiZAElq2VxQTNJkvfnmm2GfffYJK664YhwYzDoBxVwnoHX64osv4mrRH3zwQfjrr79iYMCKwSwcJ0lpLSi25P59Q9upU1hQbNzY8P5V+VtQzEBAUoOCAerEGRx83nnnhQUWWKC5X5JS8N1338XGP2NFdt99d1cMlpQaA4F0eDaXNEXLL798LBe58sora2YKUus355xzhs0337zmZ1cMlpQ2Zw2qLM/okhqEFYVXXnnlODi0eK0B5YclYJLUuhgISGqwZLpIgwBJUirSmtGnEHLJq7mkkjhdpCRJrYMZAUmSJGWSYwQqy4yAJEmSlEMGApIkSVIOWRokSZKkbHKwcEWZEZAkSZJyyEBAklSSLl26hIsuuqi5X4akHA0WTuOWRwYCklQBe+yxR9h6661T3+6NN94YOnbsONnHrLfeenEa2End+L0kqfVzjIAk5cw999wTxo0bF/89YsSIuGr0U089FZZccsl439RTT93Mr1CS/o9jBCrKjIAkpYBe9kMPPTQce+yxYZZZZgmdO3cOp556aq3H0Bt/xRVXhE022SRMN910YcEFFwx33XVXze+HDBkSH/PLL7/U3PfWW2/F+7788sv4+z333DOMHj26pne/7jaQbJ/b7LPPHu+bddZZa+575plnYlAwzTTTxDKgfv36Tfa9XXvttTELMXjw4Pjze++9F99D+/btQ6dOncJuu+0WfvzxxwbvC1av5uf55psvvoa55porPl6SVF4GApKUkptuuinMMMMM4ZVXXgnnnXdeOP3008OTTz5Z6zEnn3xy2G677cLbb78d/v3vf4eddtopfPDBBw16/jXWWCPW7nfo0CF899138Xb00UeX9BqHDRsWdtxxx7jdd999NzbIeU2UHNWH99G7d+/wxBNPhA022CAGKf/617/C8ssvH15//fXw2GOPhVGjRsXnbOi+uPvuu8OFF14YrrrqqvDJJ5+E++67Lyy99NIlvQ9JrSwjkMYthywNkqSULLPMMqFPnz7x34ssski47LLLYi/6hhtuWPOYHXbYIeyzzz7x32eccUZsHF966aXh8ssvn+LzU9Iz00wzxUwAveyN0b9//9igp/GPrl27hv/+97/h/PPPj+Meih133HHh5ptvDs8++2xNWRHviSCgb9++NY+7/vrrw7zzzhs+/vjj+HxT2hfDhw+Pr79bt26hXbt2MTNA+ZIkqbzMCEhSSmj8FptzzjnD999/X+u+1VdffaKfG5oRKAe2teaaa9a6j5/pmZ8wYULNfZQLXXPNNeGFF16oCQJAJoPSIsqCkttiiy0Wf/fZZ581aF8QDP3555+xNGrfffcN9957b/j7778r9p4lZZezBlWWgYAkpYTe7WL03FdXVzf479u0aVNTQ58YP358aA5rr712DAzuuOOOWvf//vvvYYsttohjF4pvBBLrrLNOg/YF2YOPPvooZkEYK3HQQQfFv22u9ypJrZWBgCRlyMsvvzzRz4svvnj8dzKwl9r/BI3suuVBxT33pWJbL774Yq37+JmSnrZt29bcR6nOo48+GkuALrjggpr7V1hhhfD+++/HQcYLL7xwrRtjAhqKAICA4pJLLomDoIcOHRrHLEjKGccIVJSBgCRlyJ133hlr6qmnp4b+1VdfDQcffHD8HY1pessZwEsP+8MPPzzRjD40wOmVp96emXr++OOPkrZ/1FFHxb9lfAKvgUG91O/XN+iYwcmPPPJIOO2002oWGOvVq1f4+eefw8477xxee+21WA70+OOPx9mMGhqgMDD5uuuui7MPff755+GWW26JgcH8889f0nuRJE2egYAkZQiN6kGDBsUa+oEDB4bbbrstLLHEEjXlNPz84Ycfxt+fe+654cwzz5yocX7AAQeEHj16xAwCM/KUgh59yn14DUsttVQ45ZRT4ow+dQcKJ9Zaa60YkJx00klxUDNTfZJBoNG/0UYbxdl+Dj/88Di9aFLaNCU8lvEHjE3gfbLGwYMPPhinOJUklU9VobjYVJLUbKiTZ2Bsc6xILElZ8uuvv8ZZ0Jbb7azQduppK769CePGhrduPjGuw8IUzHlhRkCSJEnKIdcRkCRJUjalNZC3EHLJQECSMsJKTUlSmgwEJEmSlElpLfZVldN+GMcISJIkSTlkRkCSJEnZ5BiBijIjIEmSJOWQGQFJkiRlkmMEKsuMgCRJkpRDZgQkSZKUTY4RqCgzApIkSVIOGQhIkiRJOWRpkCRJkjLJwcKVZUZAkiRJyiEzApIkScomBwtXlBkBSZIkKYfMCEiSJCmz8lq/nwYzApIkSVIOmRGQJElSNhUK/9zS2E4OmRGQJEmScsiMgCRJkjLJdQQqy4yAJEmSlENmBCRJkpRNriNQUWYEJEmSpBwyEJAkSZJyyNIgSZIkZVJV9T+3NLaTR2YEJEmSpBwyIyBJkqRscrBwRZkRkCRJknLIjIAkSZIyyQXFKsuMgCRJkpRDZgQkSZKUTYXCP7c0tpNDZgQkSZKkHDIjIEmSpExyjEBlmRGQJEmScshAQJIkScohS4MkSZKUTS4oVlFmBCRJkqQcMiMgSZKkTHKwcGWZEZAkSZJyyIyAJEmSsskFxSrKjIAkSZKUQ2YEJEmSlEmOEagsMwKSJElSDpkRkCRJUja5jkBFmRGQJEmScsiMgCRJkjLJMQKVZUZAkiRJyiEDAUmSJCmHLA2SJElSNlUX/rmlsZ0cMiMgSZIk5ZAZAUmSJGWT04dWlBkBSZIkKYfMCEiSJCmTqlKa2rMq5JMZAUmSJCmHzAhIkiQpmwqFf25pbCeHzAhIkiRJOWRGQJIkSZnE+IBUxggUQi6ZEZAkSZJyyIyAJEmSssl1BCrKjIAkSZLUCAMGDAhdunQJ0047bVh11VXDq6++OsnHXnPNNWHttdcOM888c7x169Ztso9Pg4GAJEmSVKLbb789HHnkkaFPnz7hjTfeCMsuu2zo3r17+P777+t9/JAhQ8LOO+8cnnnmmTB06NAw77zzho022ih88803oblUFQo5nS9JkiRJmfTrr7+GmWaaKay9Xp8w1VTTVnx7f/89Njw/5LQwYsSI0KFDh5r7p5lmmnirDxmAlVdeOVx22WXx5+rq6ti4P+SQQ0Lv3r2nuM0JEybEzAB/37Nnz9AczAhIkiRJIcSGPAFIcjv77LPrfdy4cePCsGHDYnlPok2bNvFnevsb4o8//gjjx48Ps8wyS2guDhaWJElSNlX/3y2N7YRQb0agPj/++GPs0e/UqVOt+/n5ww8/bNAmjzvuuDDXXHPVCibSZiAgSZIkhRCDgOJAoFLOOeecMGjQoDhugIHGzcVAQJIkSZlUVSjEWxrbKcVss80W2rZtG0aNGlXrfn7u3LnzZP/2ggsuiIHAU089FZZZZpnQnBwjIEmSJJVg6qmnDiuuuGIYPHhwzX0MFubn1VdffZJ/d95554UzzjgjPPbYY2GllVYKzc2MgCRJkrIpwwuKHXnkkWH33XePDfpVVlklXHTRRWHMmDFhzz33jL9nJqC55567ZsDxueeeG0455ZRw6623xrUHRo4cGe9v3759vDUHAwFJkiSpRD169Ag//PBDbNzTqF9uueViT38ygHj48OFxJqHEFVdcEWcb2n777Ws9D+sQnHrqqaE5uI6AJEmSMrmOwDprnpzaOgLPvXhGGD16dCqDhbPCMQKSJElSDlkaJEmSpEyqKvxzS2M7eWRGQJIkScohAwFJkiQphywNkiRJUjYxp00a89oU8lkbZEZAkiRJyiEzApIkScqkqup/bmlsJ4/MCEiSJEk5ZEZAkiRJ2eQYgYoyIyBJkiTlkBkBSZIkZRMd9Wl01hdCLpkRkCRJknLIjIAkSZIyqapQiLc0tpNHZgQkSZKkHDIjIEmSpGxy1qCKMiMgSZIk5ZCBgCRJkpRDlgZJkiQpm6jYqU5pOzlkRkCSJEnKITMCkiRJyiSnD60sMwKSJElSDpkRkCRJUjbRUZ/K9KEhl8wISJIkSTlkRkCSJEnZ5IJiFWVGQJIkScohMwKSJEnKJtYQqEppOzlkRkCSJEnKITMCkiRJyiTXEagsMwKSJElSDhkISJIkSTlkaZAkSZKyyelDK8qMgCRJkpRDZgQkSZKUTWYEKsqMgCRJkpRDZgQkSZKUTWYEKsqMgCRJkpRDZgQkSZKUTdWs9pXSdnLIjIAkSZKUQ2YEJEmSlElVhUK8pbGdPDIjIEmSJOWQgYAkSZKUQ5YGSZIkKZucPrSizAhIkiRJOWRGQJIkSdlUXWAkbzrbySEzApIkSVIOmRGQJElSNjlGoKLMCEiSJEk5ZEZAkiRJGZVSRiCYEZAkSZKUE2YEJEmSlE2OEagoMwKSJElSDpkRkCRJUjbF+f1dR6BSzAhIkiRJOWQgIEmSJOWQpUGSJEnKpkL1P7c0tpNDZgQkSZKkHDIjIEmSpGxy+tCKMiMgSZIk5ZAZAUmSJGWT04dWlBkBSZIkKYfMCEiSJCmbHCNQUWYEJEmSpBwyIyBJkqRsikME0sgIhFwyIyBJkiTlkBkBSZIkZZNjBCrKjIAkSZKUQwYCkiRJUg5ZGiRJkqRsqq7m/1LaTv6YEZAkSZJyyIyAJEmSssnBwhVlRkCSJEnKITMCkiRJyiYzAhVlRkCSJEnKITMCkiRJyqZqeuoLKW0nf8wISJIkSTlkRkCSJEmZVChUx1sa28kjMwKSJElSDpkRkCRJUjYxm08a9fsFxwhIkiRJygkDAUmSJCmHLA2SJElSNsWSHUuDKsWMgCRJkpRDZgQkSZKUTdXVIVSlMLVnwelDJUmSJOWEGQFJkiRlk2MEKsqMgCRJkpRDZgQkSZKUSYXq6lBIYYxAwTECkiRJkvLCjIAkSZKyyTECFWVGQJIkScohMwKSJEnKpupCCFVmBCrFjIAkSZKUQwYCkiRJUg5ZGiRJkqRsiiU7KUztWbA0SJIkSVJOmBGQJElSJhWqC6GQwmDhghkBSZIkSXlhRkCSJEnZVKhOaYxAdcgjMwKSJElSDpkRkCRJUiY5RqCyzAhIkiRJjTBgwIDQpUuXMO2004ZVV101vPrqq5N9/J133hkWW2yx+Pill146PPLII6E5GQhIkiQpm6jdT+tWottvvz0ceeSRoU+fPuGNN94Iyy67bOjevXv4/vvv6338Sy+9FHbeeeew9957hzfffDNsvfXW8fbee++F5lJVyGsuRJIkSZn066+/hplmmimsF7YKU1W1q/j2/i6MD0PC/WH06NGhQ4cODfobMgArr7xyuOyyy+LP1dXVYd555w2HHHJI6N2790SP79GjRxgzZkx46KGHau5bbbXVwnLLLReuvPLK0BzMCEiSJCmT/g7jYyO94rcwviYAKb799ddf9b6ucePGhWHDhoVu3brV3NemTZv489ChQ+v9G+4vfjzIIEzq8WlwsLAkSZIyZeqppw6dO3cOL4xMr4a+ffv2sUe/GGU/p5566kSP/fHHH8OECRNCp06dat3Pzx9++GG9zz9y5Mh6H8/9zcVAQJIkSZnCYNovvvgi9rynpVAohKqqqlr3TTPNNKE1MxCQJElSJoMBblk022yzhbZt24ZRo0bVup+fyWTUh/tLeXwaHCMgSZIklVi6tOKKK4bBgwfX3MdgYX5effXV6/0b7i9+PJ588slJPj4NZgQkSZKkEjF16O677x5WWmmlsMoqq4SLLroozgq05557xt/37NkzzD333OHss8+OPx922GFh3XXXDf369QubbbZZGDRoUHj99dfD1VdfHZqLgYAkSZJUIqYD/eGHH8Ipp5wSB/wyDehjjz1WMyB4+PDhcSahxBprrBFuvfXWcNJJJ4UTTjghLLLIIuG+++4LSy21VGguriMgSZIk5ZBjBCRJkqQcMhCQJEmScshAQJIkScohAwFJkiQphwwEJEmSpBwyEJAkSZJyyEBAkiRJyiEDAUmSJCmHDAQkSZKkHDIQkCRJknLIQECSJEkK+fP/AKR3J0YK3hiDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(word_to_index)\n",
    "# load the trained model\n",
    "input_size = len(word_to_index)  # Same as during training\n",
    "output_size = len(word_to_index)\n",
    "hidden_size = 256  # Same as during training\n",
    "# Initialise the encoder and decoder\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(output_size, hidden_size)\n",
    "# Load the trained weights\n",
    "encoder.load_state_dict(torch.load(\"math_problem_encoder.pth\"))\n",
    "decoder.load_state_dict(torch.load(\"math_problem_decoder.pth\"))\n",
    "#create our test question\n",
    "input_sentence = \"if a shop sells 20 toys on monday and 9 toys on tuesday - how many toys were sold in total?\" #sixty five\n",
    "# Test and visualise\n",
    "test_token_indices_to_words = test(encoder, decoder, input_sentence, word_to_index, index_to_word, with_attention_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the test predictions for evaluation\n",
    "\n",
    "We look to generate predictions using our trained sequence-to-sequence model and evaluate its performance using 'edit distance'. Edit distance is a measure of how different our sequences are from each other, calculated by the minimum number of operations required to transform one sequence into another. A lower edit distance indicates that the two sequences are highly similar, while a higher edit distance suggests greater differences. The **generate_predictions** function processes an input question, tokenises it into indices, and runs it through the encoder to obtain hidden states, which are then passed to the decoder. The decoder iteratively predicts the answer tokens, stopping when the **<EOS>** token is encountered. The predicted answer tokens are converted back into words for interpretation. The **evaluate_with_edit_distance** function allows us to compare generated outputs with ground truth sentences by computing the edit distance, which quantifies the differences between predictions and expected answers. The final output is the average edit distance, helping us to assess model accuracy when solving math word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edit_distance\n",
    "\n",
    "def generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word, max_target_length=10):\n",
    "    \"\"\"\n",
    "    Generates a predicted output sequence for a given input sentence using a trained encoder-decoder model.\n",
    "    Args:\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        input_sentence (str): The input sentence to be processed.\n",
    "        word_to_index (dict): Mapping of words to numerical token indices.\n",
    "        index_to_word (dict): Reverse mapping of indices to words.\n",
    "        max_target_length (int): Maximum length of the output sequence.\n",
    "    Returns:\n",
    "        list: A list of words representing the predicted output sequence.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "\n",
    "    # tokenise the input sentence by converting words to numerical token indices\n",
    "    input_tokens = [word_to_index[word] for word in input_sentence.split()]\n",
    "    input_seq = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "    # Forward pass through the encoder to obtain hidden and cell states\n",
    "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "    # initialise decoder with <SOS> token as the first input\n",
    "    decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]], dtype=torch.long)\n",
    "    decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs to initialise decoder states\n",
    "    \n",
    "    # Generate output sequence iteratively\n",
    "    output_sequence = []\n",
    "    for _ in range(max_target_length):\n",
    "        # Pass current decoder input through the decoder\n",
    "        output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "            decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "        )\n",
    "        # Get the token with the highest probability from the decoder output\n",
    "        predicted_token = output.argmax(1).item()\n",
    "        # Stop generation if <EOS> token is predicted\n",
    "        if predicted_token == word_to_index[END_OF_SEQUENCE_TOKEN]:\n",
    "            break\n",
    "        # Append the predicted token to the output sequence\n",
    "        output_sequence.append(predicted_token)\n",
    "        # Use predicted token as the next decoder input\n",
    "        decoder_input = torch.tensor([predicted_token], dtype=torch.long)\n",
    "\n",
    "    # Convert token indices back into words for interpretation\n",
    "    return [index_to_word[token] for token in output_sequence]\n",
    "\n",
    "\n",
    "def evaluate_with_edit_distance(test_data, encoder, decoder, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Evaluates the sequence-to-sequence model using edit distance.\n",
    "    Args:\n",
    "        test_data (list): List of (input_sentence, ground_truth_sentence) pairs.\n",
    "        encoder: Trained encoder model.\n",
    "        decoder: Trained decoder model.\n",
    "        word_to_index (dict): Word-to-index mapping.\n",
    "        index_to_word (dict): Index-to-word mapping.\n",
    "    Returns:\n",
    "        float: Average edit distance across the test dataset.\n",
    "    \"\"\"\n",
    "    total_distance = 0  # initialise total edit distance accumulator\n",
    "    for input_sentence, ground_truth_sentence in test_data:\n",
    "        # Generate predicted sequence for the input sentence\n",
    "        predicted_sentence = generate_predictions(encoder, decoder, input_sentence, word_to_index, index_to_word)\n",
    "        # Compute edit distance between ground truth and predicted sentence\n",
    "        distance = edit_distance.SequenceMatcher(a=ground_truth_sentence, b=predicted_sentence).distance()\n",
    "        total_distance += distance  # Accumulate total distance\n",
    "        # Debugging: Print example results for verification\n",
    "        print(f\"Input: {input_sentence}\")\n",
    "        print(f\"Ground Truth: {ground_truth_sentence}\")\n",
    "        print(f\"Predicted: {predicted_sentence}\")\n",
    "        print(f\"Edit Distance: {distance}\\n\")\n",
    "\n",
    "    # Compute average edit distance across the dataset\n",
    "    average_distance = total_distance / len(test_data)\n",
    "    print(f\"Average Edit Distance: {average_distance}\")\n",
    "\n",
    "    return average_distance  # Return evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using edit distance\n",
    "\n",
    "We'll setup a small test dataset consisting of math word problems and their expected answers, formatted as tokenised sequences with a start token (**<SOS>**). We'll evaluate the trained model using 'edit distance' (as described above). Each test case follows a structured pattern where a simple math question is presented in text, and associated with the corresponding answer, again in text format. The **evaluate_with_edit_distance** is called to generate answer predictions and compare them against the ground truth answers. Finally, we calculate the average edit distance across all test cases, with the aim of assessing model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: there are 12 pencils in one box and 19 pencils in another box - how many pencils are there altogether?\n",
      "Ground Truth: ['<SOS>', 'thirty-one']\n",
      "Predicted: ['<SOS>', 'thirty-one']\n",
      "Edit Distance: 0\n",
      "\n",
      "Input: there are 1 pencils in one box and 1 pencils in another box - how many pencils are there altogether?\n",
      "Ground Truth: ['<SOS>', 'two']\n",
      "Predicted: ['<SOS>', 'two']\n",
      "Edit Distance: 0\n",
      "\n",
      "Input: a baker has 5 muffins and bakes 14 more muffins - how many muffins does the baker have now?\n",
      "Ground Truth: ['<SOS>', 'nineteen']\n",
      "Predicted: ['<SOS>', 'nineteen']\n",
      "Edit Distance: 0\n",
      "\n",
      "Input: a baker has 14 muffins and bakes 7 more muffins - how many muffins does the baker have now?\n",
      "Ground Truth: ['<SOS>', 'twenty-one']\n",
      "Predicted: ['<SOS>', 'twenty-one']\n",
      "Edit Distance: 0\n",
      "\n",
      "Input: a baker has 2 muffins and bakes 4 more muffins - how many muffins does the baker have now?\n",
      "Ground Truth: ['<SOS>', 'six']\n",
      "Predicted: ['<SOS>', 'six']\n",
      "Edit Distance: 0\n",
      "\n",
      "Input: if 7 apples are added to 3 apples - how many apples are there in total?\n",
      "Ground Truth: ['<SOS>', 'ten']\n",
      "Predicted: ['<SOS>', 'ten']\n",
      "Edit Distance: 0\n",
      "\n",
      "Input: a train travels 3 miles in the morning and 20 miles in the evening - what is the total distance traveled?\n",
      "Ground Truth: ['<SOS>', 'twenty-three']\n",
      "Predicted: ['<SOS>', 'twenty-three']\n",
      "Edit Distance: 0\n",
      "\n",
      "Input: if a shop sells 2 toys on monday and 18 toys on tuesday - how many toys were sold in total?\n",
      "Ground Truth: ['<SOS>', 'twenty']\n",
      "Predicted: ['<SOS>', 'twenty']\n",
      "Edit Distance: 0\n",
      "\n",
      "Input: sam has 2 marbles and he loses 4 marbles - how many marbles does sam have left?\n",
      "Ground Truth: ['<SOS>', 'six']\n",
      "Predicted: ['<SOS>', 'six']\n",
      "Edit Distance: 0\n",
      "\n",
      "Average Edit Distance: 0.0\n",
      "Final Average Edit Distance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Define an example test dataset with math addition problems and expected solutions\n",
    "test_data = [\n",
    "    (\"there are 12 pencils in one box and 19 pencils in another box - how many pencils are there altogether?\", [START_OF_SEQUENCE_TOKEN, \"thirty-one\"]),  # Expected output: \"thirty-one\"\n",
    "    (\"there are 1 pencils in one box and 1 pencils in another box - how many pencils are there altogether?\", [START_OF_SEQUENCE_TOKEN, \"two\"]),  # Expected output: \"two\"\n",
    "    (\"a baker has 5 muffins and bakes 14 more muffins - how many muffins does the baker have now?\", [START_OF_SEQUENCE_TOKEN, \"nineteen\"]), # Expected output: \"nineteen\"\n",
    "    (\"a baker has 14 muffins and bakes 7 more muffins - how many muffins does the baker have now?\", [START_OF_SEQUENCE_TOKEN, \"twenty-one\"]),  # Expected output: \"twenty-one\"\n",
    "    (\"a baker has 2 muffins and bakes 4 more muffins - how many muffins does the baker have now?\", [START_OF_SEQUENCE_TOKEN, \"six\"]),  # Expected output: \"six\"\n",
    "    (\"if 7 apples are added to 3 apples - how many apples are there in total?\", [START_OF_SEQUENCE_TOKEN, \"ten\"]),  # Expected output: \"ten\"\n",
    "    (\"a train travels 3 miles in the morning and 20 miles in the evening - what is the total distance traveled?\", [START_OF_SEQUENCE_TOKEN, \"twenty-three\"]),  # Expected output: \"twenty-three\"\n",
    "    (\"if a shop sells 2 toys on monday and 18 toys on tuesday - how many toys were sold in total?\", [START_OF_SEQUENCE_TOKEN, \"twenty\"]), # Expected output: \"twenty\"\n",
    "    (\"sam has 2 marbles and he loses 4 marbles - how many marbles does sam have left?\", [START_OF_SEQUENCE_TOKEN, \"six\"])  # Expected output: \"six\"\n",
    "]\n",
    "\n",
    "# Evaluate the trained sequence-to-sequence model using edit distance\n",
    "average_distance = evaluate_with_edit_distance(\n",
    "    test_data, encoder, decoder, word_to_index, index_to_word\n",
    ")\n",
    "\n",
    "# Print the final average edit distance to assess model performance\n",
    "print(f\"Final Average Edit Distance: {average_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add positional encodings to our encode model\n",
    "\n",
    "We look to further extend our original encoder model with the addition of a positional encoding layer to inject sequential information. Positional encodings allow us to enforce built-in order awareness, ensuring that questions and answers maintain their relative positioning within a given sequence.\n",
    "Thes encodings are implemented using learned embeddings, which generate continuous patterns that help the model differentiate token positions. The intention here is to ensure the model can effectively understand and retain the order of words, improving context understanding, translation accuracy, and sentence structure in an attempt to improve training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An LSTM-based encoder with positional encodings for sequence-to-sequence tasks.\n",
    "    This model encodes input sequences using embeddings, positional encodings, \n",
    "    and a stacked LSTM, returning hidden and cell states for use by a decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1, max_seq_len=100):\n",
    "        \"\"\"\n",
    "        initialises the encoder model.\n",
    "        Args:\n",
    "            input_size (int): Vocabulary size (number of unique tokens).\n",
    "            hidden_size (int): Dimensionality of hidden states in the LSTM.\n",
    "            dropout (float): Dropout rate for regularisation.\n",
    "            max_seq_len (int): Maximum sequence length for positional encodings.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # Embedding layer to convert token indices into dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # Positional encodings to retain word order information\n",
    "        self.positional_encodings = nn.Embedding(max_seq_len, hidden_size)\n",
    "        # LSTM layer for sequential processing with batch support\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Dropout for regularisation to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        Args:\n",
    "            input_seq (Tensor): Tensor containing input token indices with shape (batch_size, seq_length).\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tuple[Tensor, Tensor]]: LSTM outputs and final hidden & cell states.\n",
    "        \"\"\"\n",
    "        # Generate positional indices for the input sequence\n",
    "        positions = torch.arange(0, input_seq.size(1), device=input_seq.device).unsqueeze(0)\n",
    "        # Retrieve positional encodings for each token position\n",
    "        positional_enc = self.positional_encodings(positions)\n",
    "        # Compute embeddings, add positional encodings, and apply dropout\n",
    "        embedded = self.dropout(self.embedding(input_seq) + positional_enc)\n",
    "        # Process embedded input sequence through the LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        return outputs, (hidden, cell)  # Return LSTM outputs and final hidden/cell states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters using Opuna\n",
    "\n",
    "This code utilises Optuna, a powerful hyperparameter optimisation framework, to fine-tune an encoder-decoder model for sequence-to-sequence tasks. The **objective** function defines the hyperparameters to be optimised, including hidden size, learning rate, and dropout rate, which are sampled across a predefined range. It then initialises the encoder and decoder models with these parameters, applies dropout regularisation, and sets up cross-entropy loss while ignoring padding tokens. The models are trained for one epoch with teacher forcing, where the actual target sequence is used as input to the decoder, helping accelerate learning. The script runs 30 trials, minimising loss to find the best-performing hyperparameter combination, which is then printed for further experimentation. We aslo provide visualisations of the hyperparameter tuning process, which should help to identify trends in model performance based on different parameter values, with plots including an optimisation history graph that tracks how loss decreases across trials, a hyperparameter importance chart highlighting which parameters most affect performance, and a loss vs. hyperparameter relationships plot to visualise individual parameter effects. The final visualisation offers insights into how different hidden sizes, learning rates, and dropout rates impact model accuracy, enabling informed decisions on optimal configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 09:32:42,766] A new study created in memory with name: no-name-406f550e-4d46-4978-b6b7-6720a680a731\n",
      "[I 2025-04-11 09:32:50,729] Trial 0 finished with value: 4.675533332521954 and parameters: {'hidden_size': 491, 'learning_rate': 0.0005683477813594467, 'dropout_rate': 0.145209675764585}. Best is trial 0 with value: 4.675533332521954.\n",
      "[I 2025-04-11 09:32:55,155] Trial 1 finished with value: 12.187020755949474 and parameters: {'hidden_size': 358, 'learning_rate': 2.66192951110919e-05, 'dropout_rate': 0.10797147348343628}. Best is trial 0 with value: 4.675533332521954.\n",
      "[I 2025-04-11 09:32:58,378] Trial 2 finished with value: 5.3940709507654585 and parameters: {'hidden_size': 283, 'learning_rate': 0.0005435091829606456, 'dropout_rate': 0.4372843759383984}. Best is trial 0 with value: 4.675533332521954.\n",
      "[I 2025-04-11 09:32:59,816] Trial 3 finished with value: 13.621236801147461 and parameters: {'hidden_size': 155, 'learning_rate': 2.9057865880767667e-05, 'dropout_rate': 0.24500835663424778}. Best is trial 0 with value: 4.675533332521954.\n",
      "[I 2025-04-11 09:33:01,161] Trial 4 finished with value: 8.558354582105364 and parameters: {'hidden_size': 135, 'learning_rate': 0.00037215585067060225, 'dropout_rate': 0.3886602865831613}. Best is trial 0 with value: 4.675533332521954.\n",
      "[I 2025-04-11 09:33:02,607] Trial 5 finished with value: 6.940251615312365 and parameters: {'hidden_size': 160, 'learning_rate': 0.0004520901536251934, 'dropout_rate': 0.23664413744370824}. Best is trial 0 with value: 4.675533332521954.\n",
      "[I 2025-04-11 09:33:09,361] Trial 6 finished with value: 4.128727171156141 and parameters: {'hidden_size': 468, 'learning_rate': 0.0018527293475168007, 'dropout_rate': 0.2623156309567794}. Best is trial 6 with value: 4.128727171156141.\n",
      "[I 2025-04-11 09:33:12,071] Trial 7 finished with value: 10.470939886002313 and parameters: {'hidden_size': 252, 'learning_rate': 7.81102383144101e-05, 'dropout_rate': 0.4568438084307116}. Best is trial 6 with value: 4.128727171156141.\n",
      "[I 2025-04-11 09:33:17,080] Trial 8 finished with value: 5.696848195696634 and parameters: {'hidden_size': 380, 'learning_rate': 0.0003221040535257828, 'dropout_rate': 0.40207205485838027}. Best is trial 6 with value: 4.128727171156141.\n",
      "[I 2025-04-11 09:33:26,060] Trial 9 finished with value: 4.141282562225584 and parameters: {'hidden_size': 505, 'learning_rate': 0.004564916762293555, 'dropout_rate': 0.2550026873625575}. Best is trial 6 with value: 4.128727171156141.\n",
      "[I 2025-04-11 09:33:33,251] Trial 10 finished with value: 4.248559800405351 and parameters: {'hidden_size': 436, 'learning_rate': 0.007457730380082476, 'dropout_rate': 0.33589786149586953}. Best is trial 6 with value: 4.128727171156141.\n",
      "[I 2025-04-11 09:33:41,816] Trial 11 finished with value: 4.091367975113884 and parameters: {'hidden_size': 505, 'learning_rate': 0.0040763674555153034, 'dropout_rate': 0.2480370489800546}. Best is trial 11 with value: 4.091367975113884.\n",
      "[I 2025-04-11 09:33:50,672] Trial 12 finished with value: 4.197306769234793 and parameters: {'hidden_size': 437, 'learning_rate': 0.0017514321519612995, 'dropout_rate': 0.18401442351733904}. Best is trial 11 with value: 4.091367975113884.\n",
      "[I 2025-04-11 09:34:01,036] Trial 13 finished with value: 4.141789542304145 and parameters: {'hidden_size': 448, 'learning_rate': 0.0018931140554773618, 'dropout_rate': 0.3149509272363742}. Best is trial 11 with value: 4.091367975113884.\n",
      "[I 2025-04-11 09:34:14,551] Trial 14 finished with value: 4.058168937289525 and parameters: {'hidden_size': 510, 'learning_rate': 0.0020464606135086366, 'dropout_rate': 0.20358022632744072}. Best is trial 14 with value: 4.058168937289525.\n",
      "[I 2025-04-11 09:34:22,128] Trial 15 finished with value: 4.291578977827042 and parameters: {'hidden_size': 387, 'learning_rate': 0.009921184394857665, 'dropout_rate': 0.19144756735993512}. Best is trial 14 with value: 4.058168937289525.\n",
      "[I 2025-04-11 09:34:27,417] Trial 16 finished with value: 4.15147764720614 and parameters: {'hidden_size': 326, 'learning_rate': 0.003440750288760239, 'dropout_rate': 0.18786153763273153}. Best is trial 14 with value: 4.058168937289525.\n",
      "[I 2025-04-11 09:34:35,980] Trial 17 finished with value: 4.316403018103705 and parameters: {'hidden_size': 508, 'learning_rate': 0.0009997852437458386, 'dropout_rate': 0.33658026424293364}. Best is trial 14 with value: 4.058168937289525.\n",
      "[I 2025-04-11 09:34:42,688] Trial 18 finished with value: 8.994541554223924 and parameters: {'hidden_size': 410, 'learning_rate': 6.818946888812865e-05, 'dropout_rate': 0.29697947565167315}. Best is trial 14 with value: 4.058168937289525.\n",
      "[W 2025-04-11 09:34:44,797] Trial 19 failed with parameters: {'hidden_size': 229, 'learning_rate': 0.0001045384996068907, 'dropout_rate': 0.4991143060728579} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Local\\Temp\\ipykernel_21404\\1757033414.py\", line 39, in objective\n",
      "    encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
      "                                      ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Local\\Temp\\ipykernel_21404\\3984380246.py\", line 42, in forward\n",
      "    outputs, (hidden, cell) = self.lstm(embedded)\n",
      "                              ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Spencer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\rnn.py\", line 1124, in forward\n",
      "    result = _VF.lstm(\n",
      "             ^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-11 09:34:44,804] Trial 19 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Create Optuna study for hyperparameter tuning (minimising loss)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m)  \u001b[38;5;66;03m# Run 30 trials to find the best parameters\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Extract best-performing hyperparameters from optimisation results\u001b[39;00m\n\u001b[0;32m     70\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[1;32mc:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     _optimize(\n\u001b[0;32m    476\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    477\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m    478\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[0;32m    479\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    480\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    481\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[0;32m    482\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    483\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[0;32m    484\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[0;32m    485\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m         _optimize_sequential(\n\u001b[0;32m     64\u001b[0m             study,\n\u001b[0;32m     65\u001b[0m             func,\n\u001b[0;32m     66\u001b[0m             n_trials,\n\u001b[0;32m     67\u001b[0m             timeout,\n\u001b[0;32m     68\u001b[0m             catch,\n\u001b[0;32m     69\u001b[0m             callbacks,\n\u001b[0;32m     70\u001b[0m             gc_after_trial,\n\u001b[0;32m     71\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     72\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     73\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[0;32m     74\u001b[0m         )\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Spencer\\anaconda3\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[23], line 39\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     37\u001b[0m decoder_optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Forward pass through encoder to obtain initial hidden states\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m encoder_outputs, (hidden, cell) \u001b[38;5;241m=\u001b[39m encoder(input_seq)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# initialise decoder input with <SOS> token for each sequence in batch\u001b[39;00m\n\u001b[0;32m     41\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([word_to_index[START_OF_SEQUENCE_TOKEN]] \u001b[38;5;241m*\u001b[39m input_seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[22], line 42\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, input_seq)\u001b[0m\n\u001b[0;32m     40\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(input_seq) \u001b[38;5;241m+\u001b[39m positional_enc)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Process embedded input sequence through the LSTM\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m outputs, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embedded)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, (hidden, cell)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1121\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1125\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1126\u001b[0m         hx,\n\u001b[0;32m   1127\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m   1130\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m   1134\u001b[0m     )\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1138\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1146\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "\n",
    "# Objective function for hyperparameter tuning\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defines the optimisation objective for hyperparameter tuning using Optuna.\n",
    "    Args:\n",
    "        trial (optuna.Trial): A single optimisation trial where hyperparameters are sampled.\n",
    "    Returns:\n",
    "        float: The averaged loss computed over one epoch, which Optuna minimises.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters for tuning\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 128, 512)  # Hidden layer size\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)  # Log-scaled learning rate\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)  # Dropout rate for regularisation\n",
    "    # initialise encoder-decoder models with suggested hyperparameters\n",
    "    encoder = Encoder(input_size, hidden_size).to(device)\n",
    "    decoder = Decoder(output_size, hidden_size).to(device)\n",
    "    # Apply dropout in LSTM layers for both models\n",
    "    encoder.lstm.dropout = dropout_rate\n",
    "    decoder.lstm.dropout = dropout_rate\n",
    "    # Define loss function, ignoring padding tokens in label sequences\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "    # Set up Adam optimisers with the suggested learning rate\n",
    "    encoder_optimiser = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimiser = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    # initialise total loss for tracking performance\n",
    "    total_loss = 0\n",
    "\n",
    "    # Run one epoch of training to evaluate hyperparameter performance\n",
    "    for input_seq, target_seq in dataloader:\n",
    "        # Move data to the selected device (CPU/GPU)\n",
    "        input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "        # Reset gradients before forward pass\n",
    "        encoder_optimiser.zero_grad()\n",
    "        decoder_optimiser.zero_grad()\n",
    "        # Forward pass through encoder to obtain initial hidden states\n",
    "        encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "        # initialise decoder input with <SOS> token for each sequence in batch\n",
    "        decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "        decoder_hidden, decoder_cell = hidden, cell  # Set decoder states from encoder outputs\n",
    "\n",
    "        loss = 0  # Track cumulative loss across target sequence\n",
    "\n",
    "        for t in range(target_seq.size(1)):  # Iterate over target sequence length\n",
    "            # Forward pass through decoder\n",
    "            output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "            )\n",
    "\n",
    "            # Compute loss for current timestep\n",
    "            loss += criterion(output, target_seq[:, t])\n",
    "            # Apply teacher forcing: use ground-truth target as next input\n",
    "            decoder_input = target_seq[:, t]\n",
    "\n",
    "        # Backpropagate loss and update optimiser\n",
    "        loss.backward()\n",
    "        encoder_optimiser.step()\n",
    "        decoder_optimiser.step()\n",
    "\n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "\n",
    "    return total_loss / len(dataloader)  # Return average loss per batch for Optuna optimisation\n",
    "\n",
    "# Create Optuna study for hyperparameter tuning (minimising loss)\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=30)  # Run 30 trials to find the best parameters\n",
    "# Extract best-performing hyperparameters from optimisation results\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "# Store best hyperparameter values for model training\n",
    "best_hidden_size = best_params[\"hidden_size\"]\n",
    "best_learning_rate = best_params[\"learning_rate\"]\n",
    "best_dropout_rate = best_params[\"dropout_rate\"]\n",
    "# Plot optimization history (loss over trials)\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "# Plot relationship between hyperparameters\n",
    "optuna.visualization.plot_param_importances(study)\n",
    "# Additional plots (Loss vs. specific hyperparameters)\n",
    "fig = optuna.visualization.matplotlib.plot_slice(study)\n",
    "plt.show()  # Display plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with k-fold\n",
    "\n",
    "We look to further improvements by implementing **k-fold cross-validation** for training our model.\n",
    "\n",
    "K-fold cross-validation is a technique used to evaluate a model’s generalisation ability by splitting the dataset into multiple equal-sized folds (typically 5, as in our case). The model is trained on (k-1) folds while the remaining 1 fold is used for validation, ensuring every data point is tested at least once. This process repeats (k) times, each time using a different fold as the validation set. The final performance metric is the average over all iterations, making this method highly effective in reducing bias and variance compared to simple train-test splits. It helps us assess how well our model performs on unseen data, ensuring reliable real-world predictions.\n",
    "\n",
    "To achieve this, we split our dataset into multiple folds (default: 5), ensuring each portion is used for both training and validation, which helps to assess model generalisation. During each fold, the encoder and decoder weights are reset, trained for multiple epochs using teacher forcing, and optimised with Adam optimisers. After training, the validation step computes loss while ignoring padded tokens, ensuring evaluation accuracy. The trained models are then saved per fold for further analysis. Finally, the **evaluate_model** function calculates validation loss, ensuring we have some means of performance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "def train_with_k_fold(encoder: Encoder, decoder: Decoder, num_epochs, batch_size, weight_init, learning_rate, k_folds=5):\n",
    "    \"\"\"\n",
    "    Implements k-fold cross-validation for training a sequence-to-sequence model \n",
    "    and returns the best encoder and decoder instead of saving each fold.\n",
    "    Args:\n",
    "        encoder (Encoder): Initialized encoder model.\n",
    "        decoder (Decoder): Initialized decoder model.\n",
    "        num_epochs (int): Number of epochs per fold.\n",
    "        batch_size (int): Training batch size.\n",
    "        weight_init (function): Function to initialize model weights.\n",
    "        learning_rate (float): Learning rate for optimization.\n",
    "        k_folds (int): Number of folds for cross-validation.\n",
    "    Returns:\n",
    "        best_encoder (Encoder): The best encoder model based on validation loss.\n",
    "        best_decoder (Decoder): The best decoder model based on validation loss.\n",
    "    \"\"\"\n",
    "    # Create KFold object for splitting dataset into `k_folds` subsets\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    validation_results = []  # Stores validation loss for each fold\n",
    "    best_encoder = None  # Placeholder for best encoder model\n",
    "    best_decoder = None  # Placeholder for best decoder model\n",
    "    best_loss = float(\"inf\")  # Initialize best validation loss as infinity\n",
    "    # Track current fold number\n",
    "    fold_index = 1  \n",
    "    # Iterate through each fold\n",
    "    for train_indices, val_indices in kfold.split(dataset):\n",
    "        # Create subsets for training and validation\n",
    "        train_data = Subset(dataset, train_indices)\n",
    "        val_data = Subset(dataset, val_indices)\n",
    "        # Create DataLoaders for batch processing\n",
    "        train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        # Print fold information\n",
    "        print(f\"Training fold {fold_index}/{k_folds}...\")\n",
    "        # Reset model weights before training on a new fold\n",
    "        encoder.apply(weight_init)\n",
    "        decoder.apply(weight_init)\n",
    "        # Initialize optimizers with the specified learning rate\n",
    "        encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "        decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_loss = 0  # Track loss for current epoch\n",
    "            for input_seq, target_seq in train_loader:\n",
    "                # Move tensors to device (GPU or CPU)\n",
    "                input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "                # Reset gradients before forward pass\n",
    "                encoder_optimizer.zero_grad()\n",
    "                decoder_optimizer.zero_grad()\n",
    "                # Forward pass through encoder\n",
    "                encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "                # Initialize decoder input with <SOS> token for batch processing\n",
    "                decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "                decoder_hidden, decoder_cell = hidden, cell  # Encoder outputs initialize decoder states\n",
    "                # Compute sequence lengths for each target batch (ignoring padding)\n",
    "                target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "                max_target_length = target_lengths.max().item()  # Find longest sequence in batch\n",
    "                 # Track cumulative batch loss\n",
    "                loss = 0 \n",
    "                # Decoding loop (teacher forcing)\n",
    "                for t in range(max_target_length):\n",
    "                    still_active = t < target_lengths  # Ensure valid sequence length\n",
    "                    if not still_active.any():\n",
    "                        break\n",
    "                    # Forward pass through decoder\n",
    "                    output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                        decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                    )\n",
    "                    # Compute loss while ignoring padded sequences\n",
    "                    loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum() / still_active.sum()\n",
    "                    # Apply teacher forcing (use actual target token as next input)\n",
    "                    decoder_input = target_seq[:, t]\n",
    "\n",
    "                # Backpropagate gradients and update model parameters\n",
    "                loss.backward()\n",
    "                encoder_optimizer.step()\n",
    "                decoder_optimizer.step()\n",
    "                epoch_loss += loss.item()  # Accumulate epoch loss\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "        # Evaluate performance on validation set\n",
    "        val_loss = evaluate_model(val_loader)\n",
    "        print(f\"Validation Loss for fold {fold_index}: {val_loss:.4f}\")\n",
    "        # Store validation loss for current fold\n",
    "        validation_results.append((fold_index, val_loss))\n",
    "         # Vocabulary size for the encoder\n",
    "        input_size = len(word_to_index) \n",
    "        # If current fold has better validation loss, update best model\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_encoder = Encoder(input_size, best_hidden_size)  # Reinitialize encoder\n",
    "            best_decoder = Decoder(input_size, best_hidden_size)  # Reinitialize decoder\n",
    "            best_encoder.load_state_dict(encoder.state_dict())  # Copy trained weights\n",
    "            best_decoder.load_state_dict(decoder.state_dict())\n",
    "        # Move to next fold\n",
    "        fold_index += 1  \n",
    "    # Find and print the best fold\n",
    "    best_fold = min(validation_results, key=lambda x: x[1])[0]\n",
    "    print(f\"Best model found in fold {best_fold} with Validation Loss: {best_loss:.4f}\")\n",
    "    # Return best-performing models\n",
    "    return best_encoder, best_decoder  \n",
    "\n",
    "\n",
    "def evaluate_model(dataloader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the validation or test dataset and computes loss.\n",
    "    Args:\n",
    "        dataloader (DataLoader): DataLoader for validation or test samples.\n",
    "    Returns:\n",
    "        float: Average loss over all batches.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    # Initialize validation loss accumulator\n",
    "    val_loss = 0  \n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.no_grad():  \n",
    "        for input_seq, target_seq in dataloader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            # Generate encoder outputs\n",
    "            encoder_outputs, (hidden, cell) = encoder(input_seq)\n",
    "            # Initialize decoder input with <SOS> token\n",
    "            decoder_input = torch.tensor([word_to_index[START_OF_SEQUENCE_TOKEN]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs for decoder initialization\n",
    "            # Compute sequence lengths for target batch\n",
    "            target_lengths = (target_seq != word_to_index[PADDING_SEQUENCE_TOKEN]).sum(dim=1)\n",
    "            max_target_length = target_lengths.max().item()\n",
    "            # Iterate over target sequence length\n",
    "            for t in range(max_target_length):\n",
    "                still_active = t < target_lengths  # Ensure valid sequence length\n",
    "                if not still_active.any():\n",
    "                    break\n",
    "                # Forward pass through decoder\n",
    "                output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "                # Compute validation loss while ignoring padded sequences\n",
    "                val_loss += (criterion(output, target_seq[:, t]) * still_active.float()).sum().item() / still_active.sum().item()\n",
    "                # Teacher forcing: Use actual target token as next input\n",
    "                decoder_input = target_seq[:, t]\n",
    "    # Return average validation loss\n",
    "    return val_loss / len(dataloader)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement hyperparameter changes and re-test\n",
    "\n",
    "Finally, we initialise and train our revised model using hyperparameter-tuned values and k-fold cross-validation to enhance generalisation. It first sets-up the encoder and decoder models using the best hyperparameters obtained from tuning (**hidden_size**, **dropout_rate**, and **learning_rate**), and applies cross-entropy loss with ignored padding tokens to prevent irrelevant data from influencing training. The Adam optimiser refines both models, ensuring efficient weight adjustments. We then invoke **k-fold cross-validation** (5 folds), systematically splitting the dataset into training and validation subsets, enabling performance tracking across different data partitions. Each fold resets model weights, trains for 100 epochs, evaluates validation performance, and saves the trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    \"\"\"\n",
    "    Initializes the weights of a neural network layer using Xavier Uniform initialization.\n",
    "    This ensures that the weights are properly scaled to maintain stable gradients during training.\n",
    "    Args:\n",
    "        m: A module (e.g., a layer in the neural network) whose weights will be initialized.\n",
    "    \"\"\"\n",
    "    for param in m.parameters():  # Iterate over all parameters in the module\n",
    "        if param.dim() > 1:  # Check if the parameter has more than one dimension (e.g., weight matrices)\n",
    "            nn.init.xavier_uniform_(param)  # Apply Xavier Uniform initialization to multi-dimensional parameters\n",
    "\n",
    "# Initialize the encoder and decoder with the tuned hyperparameters\n",
    "tuned_encoder = Encoder(input_size, hidden_size=best_hidden_size, dropout=best_dropout_rate).to(device)\n",
    "tuned_decoder = Decoder(output_size, hidden_size=best_hidden_size, dropout=best_dropout_rate).to(device)\n",
    "# Select the device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Train using k-fold cross-validation (ensures better generalization)\n",
    "best_encoder, best_decoder = train_with_k_fold(\n",
    "    tuned_encoder,\n",
    "    tuned_decoder,\n",
    "    num_epochs=100,  # Number of epochs for training\n",
    "    batch_size=32,   # Mini-batch size for training\n",
    "    weight_init=weight_init,  # Weight initialization parameter\n",
    "    learning_rate=best_learning_rate,  # Optimized learning rate from tuning\n",
    "    k_folds=min(5, len(dataset))  # Ensure the number of folds does not exceed the number of samples\n",
    ")\n",
    "\n",
    "print('best encoder and decoder loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate using BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "\n",
    "def evaluate_BLEU_score_for_model(encoder, decoder, dataloader, max_target_len, device, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of a trained sequence-to-sequence model using BLEU score and plots its trends.\n",
    "    Args:\n",
    "        encoder (Encoder): Trained encoder model.\n",
    "        decoder (Decoder): Trained decoder model.\n",
    "        dataloader (DataLoader): DataLoader containing test data.\n",
    "        max_target_len (int): Maximum length of the output sequence.\n",
    "        device: Torch device (CPU or GPU).\n",
    "        word_to_index (dict): Dictionary mapping words to indices.\n",
    "        index_to_word (dict): Dictionary mapping indices to words.\n",
    "    Returns:\n",
    "        float: BLEU score representing translation accuracy.\n",
    "    \"\"\"\n",
    "    encoder.eval()  # Set encoder to evaluation mode\n",
    "    decoder.eval()  # Set decoder to evaluation mode\n",
    "    bleu_scores = []  # Stores BLEU scores per batch\n",
    "    batch_indices = []  # Track batch numbers\n",
    "    references = []  # Stores ground truth sequences\n",
    "    candidates = []  # Stores model-generated sequences\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations during evaluation\n",
    "        for batch_idx, (input_seq, target_seq) in enumerate(dataloader):\n",
    "            input_seq = input_seq.to(device)  # Move input tensor to the correct device\n",
    "            # Forward pass through the encoder to obtain output representations\n",
    "            encoder_output = encoder(input_seq)\n",
    "            encoder_outputs, (hidden, cell) = encoder_output  # Unpack hidden states\n",
    "            # Initialize decoder input with <SOS> token for batch processing\n",
    "            decoder_input = torch.tensor([word_to_index[\"<SOS>\"]] * input_seq.size(0), device=device)\n",
    "            decoder_hidden, decoder_cell = hidden, cell  # Use encoder outputs to initialize decoder states\n",
    "            # Stores predicted sequences per instance\n",
    "            predictions = [[] for _ in range(input_seq.size(0))]  \n",
    "            # Iterate through the target sequence length\n",
    "            for _ in range(max_target_len):\n",
    "                # Forward pass through decoder\n",
    "                output, decoder_hidden, decoder_cell, _ = decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs\n",
    "                )\n",
    "                # Retrieve the most probable token from the decoder output\n",
    "                top_token = output.argmax(1)\n",
    "                # Append predicted tokens to individual sequence lists\n",
    "                for i, token in enumerate(top_token.tolist()):\n",
    "                    predictions[i].append(token)\n",
    "                # Stop decoding if all sequences generate <EOS>\n",
    "                if all(token == word_to_index[\"<EOS>\"] for token in top_token.tolist()):\n",
    "                    break\n",
    "                # Update decoder input with the predicted token\n",
    "                decoder_input = top_token\n",
    "            # Convert predicted and target sequences to readable word format\n",
    "            predicted_seqs = [\" \".join([index_to_word[token] for token in seq]) for seq in predictions]\n",
    "            target_seqs = [\" \".join([index_to_word[token.item()] for token in target_seq[i]]) for i in range(target_seq.size(0))]\n",
    "            # Wrap references in lists for BLEU score calculation\n",
    "            references.extend([[ref] for ref in target_seqs])\n",
    "            # Store model-predicted sequences\n",
    "            candidates.extend(predicted_seqs)  \n",
    "            # Compute BLEU score for this batch\n",
    "            batch_bleu_score = sacrebleu.corpus_bleu(predicted_seqs, [[ref] for ref in target_seqs]).score\n",
    "            bleu_scores.append(batch_bleu_score)\n",
    "            batch_indices.append(batch_idx + 1)\n",
    "            # Print batch BLEU score for monitoring\n",
    "            print(f\"Batch {batch_idx + 1} BLEU Score: {batch_bleu_score:.4f}\")\n",
    "\n",
    "    # Plot BLEU scores across batches\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(batch_indices, bleu_scores, marker='o', linestyle='-', color='b', label=\"BLEU Score\")\n",
    "    plt.xlabel(\"Batch Number\")\n",
    "    plt.ylabel(\"BLEU Score\")\n",
    "    plt.title(\"BLEU Score Across Batches\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    # Compute overall BLEU score for the entire dataset\n",
    "    overall_bleu_score = sacrebleu.corpus_bleu(candidates, references).score\n",
    "    print(f\"Overall BLEU Score: {overall_bleu_score:.4f}\")\n",
    "    # return the overall BLEU score for the dataset\n",
    "    return overall_bleu_score\n",
    "\n",
    "\n",
    "# Load test dataset and prepare sequences\n",
    "csv_file = \"test_math_word_problems.csv\"\n",
    "# Load question and answer sequences from CSV file\n",
    "question_sequences, answer_sequences = load_question_and_answer_sequences_from_csv(csv_file)\n",
    "# Tokenize sentences and create input-target pairs\n",
    "question_data, answer_data = create_question_and_answer_sequences(question_sequences, answer_sequences)\n",
    "# Create a reverse mapping dictionary to convert token indices back into words\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}\n",
    "# Convert tokenized sentences into PyTorch tensors for model compatibility\n",
    "question_tensors = [torch.tensor(seq) for seq in question_data]  # Convert question sequences to tensors\n",
    "answer_tensors = [torch.tensor(seq) for seq in answer_data]  # Convert answer sequences to tensors\n",
    "# Apply dynamic padding to ensure uniform sequence lengths\n",
    "questions_padded = pad_sequence(question_tensors, batch_first=True, padding_value=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "answers_padded = pad_sequence(answer_tensors, batch_first=True, padding_value=word_to_index[PADDING_SEQUENCE_TOKEN])\n",
    "# Create a dataset instance using the padded sequences\n",
    "dataset = MathWordProblemDataset(questions_padded, answers_padded)\n",
    "# Initialize a DataLoader for batch processing\n",
    "dataloader = DataLoader(dataset, batch_size=40, shuffle=True)  # Process in batches of 40 for efficiency\n",
    "# Evaluate BLEU score using the best model obtained from k-fold training\n",
    "evaluate_BLEU_score_for_model(best_encoder, best_decoder, dataloader, max_target_len=10, \n",
    "                              device=device, word_to_index=word_to_index, index_to_word=index_to_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
